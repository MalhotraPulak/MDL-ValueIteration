iteration=0
(W,0,0,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 0, 0, 25)
0.8 ( 4 0 0 0 25 ) 0.0 0.0
0.2 ( 4 0 0 1 25 ) 0.0 0.0
0.0 ( 0 0 0 0 25 ) 0.0 0.0
0.0 ( 0 0 0 1 25 ) 0.0 0.0
(W,0,0,D,25):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 0, 50)
0.8 ( 4 0 0 0 50 ) 0.0 0.0
0.2 ( 4 0 0 1 50 ) 0.0 0.0
0.0 ( 0 0 0 0 50 ) 0.0 0.0
0.0 ( 0 0 0 1 50 ) 0.0 0.0
(W,0,0,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 0, 75)
0.8 ( 4 0 0 0 75 ) 0.0 0.0
0.2 ( 4 0 0 1 75 ) 0.0 0.0
0.0 ( 0 0 0 0 75 ) 0.0 0.0
0.0 ( 0 0 0 1 75 ) 0.0 0.0
(W,0,0,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 0, 100)
0.8 ( 4 0 0 0 100 ) 0.0 0.0
0.2 ( 4 0 0 1 100 ) 0.0 0.0
0.0 ( 0 0 0 0 100 ) 0.0 0.0
0.0 ( 0 0 0 1 100 ) 0.0 0.0
(W,0,0,D,100):RIGHT=[-10.0]
(W,0,0,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 0, 1, 25)
0.5 ( 4 0 0 1 25 ) 0.0 0.0
0.5 ( 4 0 0 0 25 ) 0.0 0.0
0.0 ( 0 0 0 1 25 ) 0.0 0.0
0.0 ( 0 0 0 0 25 ) 0.0 0.0
(W,0,0,R,25):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 1, 50)
0.5 ( 4 0 0 1 50 ) 0.0 0.0
0.5 ( 4 0 0 0 50 ) 0.0 0.0
0.0 ( 0 0 0 1 50 ) 0.0 0.0
0.0 ( 0 0 0 0 50 ) 0.0 0.0
(W,0,0,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 1, 75)
0.5 ( 4 0 0 1 75 ) 0.0 0.0
0.5 ( 4 0 0 0 75 ) 0.0 0.0
0.0 ( 0 0 0 1 75 ) 0.0 0.0
0.0 ( 0 0 0 0 75 ) 0.0 0.0
(W,0,0,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 1, 100)
0.5 ( 4 0 0 1 100 ) 0.0 0.0
0.5 ( 4 0 0 0 100 ) 0.0 0.0
0.0 ( 0 0 0 1 100 ) 0.0 0.0
0.0 ( 0 0 0 0 100 ) 0.0 0.0
(W,0,0,R,100):RIGHT=[-10.0]
(W,0,1,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 1, 0, 25)
0.6000000000000001 ( 0 0 0 0 25 ) 0.0 0.0
0.15000000000000002 ( 0 0 0 1 25 ) 0.0 0.0
0.2 ( 0 0 0 0 0 ) 0.0 50.0
0.05 ( 0 0 0 1 0 ) 0.0 50.0
(W,0,1,D,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 0, 1, 0, 50)
0.8 ( 4 0 1 0 50 ) 0.0 0.0
0.2 ( 4 0 1 1 50 ) 0.0 0.0
0.0 ( 0 0 1 0 50 ) 0.0 0.0
0.0 ( 0 0 1 1 50 ) 0.0 0.0
(W,0,1,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 1, 0, 75)
0.8 ( 4 0 1 0 75 ) 0.0 0.0
0.2 ( 4 0 1 1 75 ) 0.0 0.0
0.0 ( 0 0 1 0 75 ) 0.0 0.0
0.0 ( 0 0 1 1 75 ) 0.0 0.0
(W,0,1,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 1, 0, 100)
0.8 ( 4 0 1 0 100 ) 0.0 0.0
0.2 ( 4 0 1 1 100 ) 0.0 0.0
0.0 ( 0 0 1 0 100 ) 0.0 0.0
0.0 ( 0 0 1 1 100 ) 0.0 0.0
(W,0,1,D,100):RIGHT=[-10.0]
(W,0,1,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 1, 1, 25)
0.375 ( 0 0 0 1 25 ) 0.0 0.0
0.375 ( 0 0 0 0 25 ) 0.0 0.0
0.125 ( 0 0 0 1 0 ) 0.0 50.0
0.125 ( 0 0 0 0 0 ) 0.0 50.0
(W,0,1,R,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 0, 1, 1, 50)
0.5 ( 4 0 1 1 50 ) 0.0 0.0
0.5 ( 4 0 1 0 50 ) 0.0 0.0
0.0 ( 0 0 1 1 50 ) 0.0 0.0
0.0 ( 0 0 1 0 50 ) 0.0 0.0
(W,0,1,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 1, 1, 75)
0.5 ( 4 0 1 1 75 ) 0.0 0.0
0.5 ( 4 0 1 0 75 ) 0.0 0.0
0.0 ( 0 0 1 1 75 ) 0.0 0.0
0.0 ( 0 0 1 0 75 ) 0.0 0.0
(W,0,1,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 1, 1, 100)
0.5 ( 4 0 1 1 100 ) 0.0 0.0
0.5 ( 4 0 1 0 100 ) 0.0 0.0
0.0 ( 0 0 1 1 100 ) 0.0 0.0
0.0 ( 0 0 1 0 100 ) 0.0 0.0
(W,0,1,R,100):RIGHT=[-10.0]
(W,0,2,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 2, 0, 25)
0.6000000000000001 ( 0 0 1 0 25 ) 0.0 0.0
0.15000000000000002 ( 0 0 1 1 25 ) 0.0 0.0
0.2 ( 0 0 1 0 0 ) 0.0 50.0
0.05 ( 0 0 1 1 0 ) 0.0 50.0
(W,0,2,D,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 0, 2, 0, 50)
0.8 ( 4 0 2 0 50 ) 0.0 0.0
0.2 ( 4 0 2 1 50 ) 0.0 0.0
0.0 ( 0 0 2 0 50 ) 0.0 0.0
0.0 ( 0 0 2 1 50 ) 0.0 0.0
(W,0,2,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 2, 0, 75)
0.8 ( 4 0 2 0 75 ) 0.0 0.0
0.2 ( 4 0 2 1 75 ) 0.0 0.0
0.0 ( 0 0 2 0 75 ) 0.0 0.0
0.0 ( 0 0 2 1 75 ) 0.0 0.0
(W,0,2,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 2, 0, 100)
0.8 ( 4 0 2 0 100 ) 0.0 0.0
0.2 ( 4 0 2 1 100 ) 0.0 0.0
0.0 ( 0 0 2 0 100 ) 0.0 0.0
0.0 ( 0 0 2 1 100 ) 0.0 0.0
(W,0,2,D,100):RIGHT=[-10.0]
(W,0,2,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 2, 1, 25)
0.375 ( 0 0 1 1 25 ) 0.0 0.0
0.375 ( 0 0 1 0 25 ) 0.0 0.0
0.125 ( 0 0 1 1 0 ) 0.0 50.0
0.125 ( 0 0 1 0 0 ) 0.0 50.0
(W,0,2,R,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 0, 2, 1, 50)
0.5 ( 4 0 2 1 50 ) 0.0 0.0
0.5 ( 4 0 2 0 50 ) 0.0 0.0
0.0 ( 0 0 2 1 50 ) 0.0 0.0
0.0 ( 0 0 2 0 50 ) 0.0 0.0
(W,0,2,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 2, 1, 75)
0.5 ( 4 0 2 1 75 ) 0.0 0.0
0.5 ( 4 0 2 0 75 ) 0.0 0.0
0.0 ( 0 0 2 1 75 ) 0.0 0.0
0.0 ( 0 0 2 0 75 ) 0.0 0.0
(W,0,2,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 2, 1, 100)
0.5 ( 4 0 2 1 100 ) 0.0 0.0
0.5 ( 4 0 2 0 100 ) 0.0 0.0
0.0 ( 0 0 2 1 100 ) 0.0 0.0
0.0 ( 0 0 2 0 100 ) 0.0 0.0
(W,0,2,R,100):RIGHT=[-10.0]
(W,0,3,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 3, 0, 25)
0.6000000000000001 ( 0 0 2 0 25 ) 0.0 0.0
0.15000000000000002 ( 0 0 2 1 25 ) 0.0 0.0
0.2 ( 0 0 2 0 0 ) 0.0 50.0
0.05 ( 0 0 2 1 0 ) 0.0 50.0
(W,0,3,D,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 0, 3, 0, 50)
0.8 ( 4 0 3 0 50 ) 0.0 0.0
0.2 ( 4 0 3 1 50 ) 0.0 0.0
0.0 ( 0 0 3 0 50 ) 0.0 0.0
0.0 ( 0 0 3 1 50 ) 0.0 0.0
(W,0,3,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 3, 0, 75)
0.8 ( 4 0 3 0 75 ) 0.0 0.0
0.2 ( 4 0 3 1 75 ) 0.0 0.0
0.0 ( 0 0 3 0 75 ) 0.0 0.0
0.0 ( 0 0 3 1 75 ) 0.0 0.0
(W,0,3,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 3, 0, 100)
0.8 ( 4 0 3 0 100 ) 0.0 0.0
0.2 ( 4 0 3 1 100 ) 0.0 0.0
0.0 ( 0 0 3 0 100 ) 0.0 0.0
0.0 ( 0 0 3 1 100 ) 0.0 0.0
(W,0,3,D,100):RIGHT=[-10.0]
(W,0,3,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 3, 1, 25)
0.375 ( 0 0 2 1 25 ) 0.0 0.0
0.375 ( 0 0 2 0 25 ) 0.0 0.0
0.125 ( 0 0 2 1 0 ) 0.0 50.0
0.125 ( 0 0 2 0 0 ) 0.0 50.0
(W,0,3,R,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 0, 3, 1, 50)
0.5 ( 4 0 3 1 50 ) 0.0 0.0
0.5 ( 4 0 3 0 50 ) 0.0 0.0
0.0 ( 0 0 3 1 50 ) 0.0 0.0
0.0 ( 0 0 3 0 50 ) 0.0 0.0
(W,0,3,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 3, 1, 75)
0.5 ( 4 0 3 1 75 ) 0.0 0.0
0.5 ( 4 0 3 0 75 ) 0.0 0.0
0.0 ( 0 0 3 1 75 ) 0.0 0.0
0.0 ( 0 0 3 0 75 ) 0.0 0.0
(W,0,3,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 0, 3, 1, 100)
0.5 ( 4 0 3 1 100 ) 0.0 0.0
0.5 ( 4 0 3 0 100 ) 0.0 0.0
0.0 ( 0 0 3 1 100 ) 0.0 0.0
0.0 ( 0 0 3 0 100 ) 0.0 0.0
(W,0,3,R,100):RIGHT=[-10.0]
(W,1,0,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 0, 0, 25)
0.8 ( 4 1 0 0 25 ) 0.0 0.0
0.2 ( 4 1 0 1 25 ) 0.0 0.0
0.0 ( 0 1 0 0 25 ) 0.0 0.0
0.0 ( 0 1 0 1 25 ) 0.0 0.0
(W,1,0,D,25):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 0, 50)
0.8 ( 4 1 0 0 50 ) 0.0 0.0
0.2 ( 4 1 0 1 50 ) 0.0 0.0
0.0 ( 0 1 0 0 50 ) 0.0 0.0
0.0 ( 0 1 0 1 50 ) 0.0 0.0
(W,1,0,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 0, 75)
0.8 ( 4 1 0 0 75 ) 0.0 0.0
0.2 ( 4 1 0 1 75 ) 0.0 0.0
0.0 ( 0 1 0 0 75 ) 0.0 0.0
0.0 ( 0 1 0 1 75 ) 0.0 0.0
(W,1,0,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 0, 100)
0.8 ( 4 1 0 0 100 ) 0.0 0.0
0.2 ( 4 1 0 1 100 ) 0.0 0.0
0.0 ( 0 1 0 0 100 ) 0.0 0.0
0.0 ( 0 1 0 1 100 ) 0.0 0.0
(W,1,0,D,100):RIGHT=[-10.0]
(W,1,0,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 0, 1, 25)
0.5 ( 4 1 0 1 25 ) 0.0 0.0
0.5 ( 4 1 0 0 25 ) 0.0 0.0
0.0 ( 0 1 0 1 25 ) 0.0 0.0
0.0 ( 0 1 0 0 25 ) 0.0 0.0
(W,1,0,R,25):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 1, 50)
0.5 ( 4 1 0 1 50 ) 0.0 0.0
0.5 ( 4 1 0 0 50 ) 0.0 0.0
0.0 ( 0 1 0 1 50 ) 0.0 0.0
0.0 ( 0 1 0 0 50 ) 0.0 0.0
(W,1,0,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 1, 75)
0.5 ( 4 1 0 1 75 ) 0.0 0.0
0.5 ( 4 1 0 0 75 ) 0.0 0.0
0.0 ( 0 1 0 1 75 ) 0.0 0.0
0.0 ( 0 1 0 0 75 ) 0.0 0.0
(W,1,0,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 1, 100)
0.5 ( 4 1 0 1 100 ) 0.0 0.0
0.5 ( 4 1 0 0 100 ) 0.0 0.0
0.0 ( 0 1 0 1 100 ) 0.0 0.0
0.0 ( 0 1 0 0 100 ) 0.0 0.0
(W,1,0,R,100):RIGHT=[-10.0]
(W,1,1,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 1, 0, 25)
0.6000000000000001 ( 0 1 0 0 25 ) 0.0 0.0
0.15000000000000002 ( 0 1 0 1 25 ) 0.0 0.0
0.2 ( 0 1 0 0 0 ) 0.0 50.0
0.05 ( 0 1 0 1 0 ) 0.0 50.0
(W,1,1,D,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 1, 1, 0, 50)
0.8 ( 4 1 1 0 50 ) 0.0 0.0
0.2 ( 4 1 1 1 50 ) 0.0 0.0
0.0 ( 0 1 1 0 50 ) 0.0 0.0
0.0 ( 0 1 1 1 50 ) 0.0 0.0
(W,1,1,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 1, 0, 75)
0.8 ( 4 1 1 0 75 ) 0.0 0.0
0.2 ( 4 1 1 1 75 ) 0.0 0.0
0.0 ( 0 1 1 0 75 ) 0.0 0.0
0.0 ( 0 1 1 1 75 ) 0.0 0.0
(W,1,1,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 1, 0, 100)
0.8 ( 4 1 1 0 100 ) 0.0 0.0
0.2 ( 4 1 1 1 100 ) 0.0 0.0
0.0 ( 0 1 1 0 100 ) 0.0 0.0
0.0 ( 0 1 1 1 100 ) 0.0 0.0
(W,1,1,D,100):RIGHT=[-10.0]
(W,1,1,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 1, 1, 25)
0.375 ( 0 1 0 1 25 ) 0.0 0.0
0.375 ( 0 1 0 0 25 ) 0.0 0.0
0.125 ( 0 1 0 1 0 ) 0.0 50.0
0.125 ( 0 1 0 0 0 ) 0.0 50.0
(W,1,1,R,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 1, 1, 1, 50)
0.5 ( 4 1 1 1 50 ) 0.0 0.0
0.5 ( 4 1 1 0 50 ) 0.0 0.0
0.0 ( 0 1 1 1 50 ) 0.0 0.0
0.0 ( 0 1 1 0 50 ) 0.0 0.0
(W,1,1,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 1, 1, 75)
0.5 ( 4 1 1 1 75 ) 0.0 0.0
0.5 ( 4 1 1 0 75 ) 0.0 0.0
0.0 ( 0 1 1 1 75 ) 0.0 0.0
0.0 ( 0 1 1 0 75 ) 0.0 0.0
(W,1,1,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 1, 1, 100)
0.5 ( 4 1 1 1 100 ) 0.0 0.0
0.5 ( 4 1 1 0 100 ) 0.0 0.0
0.0 ( 0 1 1 1 100 ) 0.0 0.0
0.0 ( 0 1 1 0 100 ) 0.0 0.0
(W,1,1,R,100):RIGHT=[-10.0]
(W,1,2,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 2, 0, 25)
0.6000000000000001 ( 0 1 1 0 25 ) 0.0 0.0
0.15000000000000002 ( 0 1 1 1 25 ) 0.0 0.0
0.2 ( 0 1 1 0 0 ) 0.0 50.0
0.05 ( 0 1 1 1 0 ) 0.0 50.0
(W,1,2,D,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 1, 2, 0, 50)
0.8 ( 4 1 2 0 50 ) 0.0 0.0
0.2 ( 4 1 2 1 50 ) 0.0 0.0
0.0 ( 0 1 2 0 50 ) 0.0 0.0
0.0 ( 0 1 2 1 50 ) 0.0 0.0
(W,1,2,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 2, 0, 75)
0.8 ( 4 1 2 0 75 ) 0.0 0.0
0.2 ( 4 1 2 1 75 ) 0.0 0.0
0.0 ( 0 1 2 0 75 ) 0.0 0.0
0.0 ( 0 1 2 1 75 ) 0.0 0.0
(W,1,2,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 2, 0, 100)
0.8 ( 4 1 2 0 100 ) 0.0 0.0
0.2 ( 4 1 2 1 100 ) 0.0 0.0
0.0 ( 0 1 2 0 100 ) 0.0 0.0
0.0 ( 0 1 2 1 100 ) 0.0 0.0
(W,1,2,D,100):RIGHT=[-10.0]
(W,1,2,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 2, 1, 25)
0.375 ( 0 1 1 1 25 ) 0.0 0.0
0.375 ( 0 1 1 0 25 ) 0.0 0.0
0.125 ( 0 1 1 1 0 ) 0.0 50.0
0.125 ( 0 1 1 0 0 ) 0.0 50.0
(W,1,2,R,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 1, 2, 1, 50)
0.5 ( 4 1 2 1 50 ) 0.0 0.0
0.5 ( 4 1 2 0 50 ) 0.0 0.0
0.0 ( 0 1 2 1 50 ) 0.0 0.0
0.0 ( 0 1 2 0 50 ) 0.0 0.0
(W,1,2,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 2, 1, 75)
0.5 ( 4 1 2 1 75 ) 0.0 0.0
0.5 ( 4 1 2 0 75 ) 0.0 0.0
0.0 ( 0 1 2 1 75 ) 0.0 0.0
0.0 ( 0 1 2 0 75 ) 0.0 0.0
(W,1,2,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 2, 1, 100)
0.5 ( 4 1 2 1 100 ) 0.0 0.0
0.5 ( 4 1 2 0 100 ) 0.0 0.0
0.0 ( 0 1 2 1 100 ) 0.0 0.0
0.0 ( 0 1 2 0 100 ) 0.0 0.0
(W,1,2,R,100):RIGHT=[-10.0]
(W,1,3,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 3, 0, 25)
0.6000000000000001 ( 0 1 2 0 25 ) 0.0 0.0
0.15000000000000002 ( 0 1 2 1 25 ) 0.0 0.0
0.2 ( 0 1 2 0 0 ) 0.0 50.0
0.05 ( 0 1 2 1 0 ) 0.0 50.0
(W,1,3,D,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 1, 3, 0, 50)
0.8 ( 4 1 3 0 50 ) 0.0 0.0
0.2 ( 4 1 3 1 50 ) 0.0 0.0
0.0 ( 0 1 3 0 50 ) 0.0 0.0
0.0 ( 0 1 3 1 50 ) 0.0 0.0
(W,1,3,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 3, 0, 75)
0.8 ( 4 1 3 0 75 ) 0.0 0.0
0.2 ( 4 1 3 1 75 ) 0.0 0.0
0.0 ( 0 1 3 0 75 ) 0.0 0.0
0.0 ( 0 1 3 1 75 ) 0.0 0.0
(W,1,3,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 3, 0, 100)
0.8 ( 4 1 3 0 100 ) 0.0 0.0
0.2 ( 4 1 3 1 100 ) 0.0 0.0
0.0 ( 0 1 3 0 100 ) 0.0 0.0
0.0 ( 0 1 3 1 100 ) 0.0 0.0
(W,1,3,D,100):RIGHT=[-10.0]
(W,1,3,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 3, 1, 25)
0.375 ( 0 1 2 1 25 ) 0.0 0.0
0.375 ( 0 1 2 0 25 ) 0.0 0.0
0.125 ( 0 1 2 1 0 ) 0.0 50.0
0.125 ( 0 1 2 0 0 ) 0.0 50.0
(W,1,3,R,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 1, 3, 1, 50)
0.5 ( 4 1 3 1 50 ) 0.0 0.0
0.5 ( 4 1 3 0 50 ) 0.0 0.0
0.0 ( 0 1 3 1 50 ) 0.0 0.0
0.0 ( 0 1 3 0 50 ) 0.0 0.0
(W,1,3,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 3, 1, 75)
0.5 ( 4 1 3 1 75 ) 0.0 0.0
0.5 ( 4 1 3 0 75 ) 0.0 0.0
0.0 ( 0 1 3 1 75 ) 0.0 0.0
0.0 ( 0 1 3 0 75 ) 0.0 0.0
(W,1,3,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 1, 3, 1, 100)
0.5 ( 4 1 3 1 100 ) 0.0 0.0
0.5 ( 4 1 3 0 100 ) 0.0 0.0
0.0 ( 0 1 3 1 100 ) 0.0 0.0
0.0 ( 0 1 3 0 100 ) 0.0 0.0
(W,1,3,R,100):RIGHT=[-10.0]
(W,2,0,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 0, 0, 25)
0.8 ( 4 2 0 0 25 ) 0.0 0.0
0.2 ( 4 2 0 1 25 ) 0.0 0.0
0.0 ( 0 2 0 0 25 ) 0.0 0.0
0.0 ( 0 2 0 1 25 ) 0.0 0.0
(W,2,0,D,25):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 0, 50)
0.8 ( 4 2 0 0 50 ) 0.0 0.0
0.2 ( 4 2 0 1 50 ) 0.0 0.0
0.0 ( 0 2 0 0 50 ) 0.0 0.0
0.0 ( 0 2 0 1 50 ) 0.0 0.0
(W,2,0,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 0, 75)
0.8 ( 4 2 0 0 75 ) 0.0 0.0
0.2 ( 4 2 0 1 75 ) 0.0 0.0
0.0 ( 0 2 0 0 75 ) 0.0 0.0
0.0 ( 0 2 0 1 75 ) 0.0 0.0
(W,2,0,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 0, 100)
0.8 ( 4 2 0 0 100 ) 0.0 0.0
0.2 ( 4 2 0 1 100 ) 0.0 0.0
0.0 ( 0 2 0 0 100 ) 0.0 0.0
0.0 ( 0 2 0 1 100 ) 0.0 0.0
(W,2,0,D,100):RIGHT=[-10.0]
(W,2,0,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 0, 1, 25)
0.5 ( 4 2 0 1 25 ) 0.0 0.0
0.5 ( 4 2 0 0 25 ) 0.0 0.0
0.0 ( 0 2 0 1 25 ) 0.0 0.0
0.0 ( 0 2 0 0 25 ) 0.0 0.0
(W,2,0,R,25):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 1, 50)
0.5 ( 4 2 0 1 50 ) 0.0 0.0
0.5 ( 4 2 0 0 50 ) 0.0 0.0
0.0 ( 0 2 0 1 50 ) 0.0 0.0
0.0 ( 0 2 0 0 50 ) 0.0 0.0
(W,2,0,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 1, 75)
0.5 ( 4 2 0 1 75 ) 0.0 0.0
0.5 ( 4 2 0 0 75 ) 0.0 0.0
0.0 ( 0 2 0 1 75 ) 0.0 0.0
0.0 ( 0 2 0 0 75 ) 0.0 0.0
(W,2,0,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 1, 100)
0.5 ( 4 2 0 1 100 ) 0.0 0.0
0.5 ( 4 2 0 0 100 ) 0.0 0.0
0.0 ( 0 2 0 1 100 ) 0.0 0.0
0.0 ( 0 2 0 0 100 ) 0.0 0.0
(W,2,0,R,100):RIGHT=[-10.0]
(W,2,1,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 1, 0, 25)
0.6000000000000001 ( 0 2 0 0 25 ) 0.0 0.0
0.15000000000000002 ( 0 2 0 1 25 ) 0.0 0.0
0.2 ( 0 2 0 0 0 ) 0.0 50.0
0.05 ( 0 2 0 1 0 ) 0.0 50.0
(W,2,1,D,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 2, 1, 0, 50)
0.8 ( 4 2 1 0 50 ) 0.0 0.0
0.2 ( 4 2 1 1 50 ) 0.0 0.0
0.0 ( 0 2 1 0 50 ) 0.0 0.0
0.0 ( 0 2 1 1 50 ) 0.0 0.0
(W,2,1,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 1, 0, 75)
0.8 ( 4 2 1 0 75 ) 0.0 0.0
0.2 ( 4 2 1 1 75 ) 0.0 0.0
0.0 ( 0 2 1 0 75 ) 0.0 0.0
0.0 ( 0 2 1 1 75 ) 0.0 0.0
(W,2,1,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 1, 0, 100)
0.8 ( 4 2 1 0 100 ) 0.0 0.0
0.2 ( 4 2 1 1 100 ) 0.0 0.0
0.0 ( 0 2 1 0 100 ) 0.0 0.0
0.0 ( 0 2 1 1 100 ) 0.0 0.0
(W,2,1,D,100):RIGHT=[-10.0]
(W,2,1,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 1, 1, 25)
0.375 ( 0 2 0 1 25 ) 0.0 0.0
0.375 ( 0 2 0 0 25 ) 0.0 0.0
0.125 ( 0 2 0 1 0 ) 0.0 50.0
0.125 ( 0 2 0 0 0 ) 0.0 50.0
(W,2,1,R,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 2, 1, 1, 50)
0.5 ( 4 2 1 1 50 ) 0.0 0.0
0.5 ( 4 2 1 0 50 ) 0.0 0.0
0.0 ( 0 2 1 1 50 ) 0.0 0.0
0.0 ( 0 2 1 0 50 ) 0.0 0.0
(W,2,1,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 1, 1, 75)
0.5 ( 4 2 1 1 75 ) 0.0 0.0
0.5 ( 4 2 1 0 75 ) 0.0 0.0
0.0 ( 0 2 1 1 75 ) 0.0 0.0
0.0 ( 0 2 1 0 75 ) 0.0 0.0
(W,2,1,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 1, 1, 100)
0.5 ( 4 2 1 1 100 ) 0.0 0.0
0.5 ( 4 2 1 0 100 ) 0.0 0.0
0.0 ( 0 2 1 1 100 ) 0.0 0.0
0.0 ( 0 2 1 0 100 ) 0.0 0.0
(W,2,1,R,100):RIGHT=[-10.0]
(W,2,2,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 2, 0, 25)
0.6000000000000001 ( 0 2 1 0 25 ) 0.0 0.0
0.15000000000000002 ( 0 2 1 1 25 ) 0.0 0.0
0.2 ( 0 2 1 0 0 ) 0.0 50.0
0.05 ( 0 2 1 1 0 ) 0.0 50.0
(W,2,2,D,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 2, 2, 0, 50)
0.8 ( 4 2 2 0 50 ) 0.0 0.0
0.2 ( 4 2 2 1 50 ) 0.0 0.0
0.0 ( 0 2 2 0 50 ) 0.0 0.0
0.0 ( 0 2 2 1 50 ) 0.0 0.0
(W,2,2,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 2, 0, 75)
0.8 ( 4 2 2 0 75 ) 0.0 0.0
0.2 ( 4 2 2 1 75 ) 0.0 0.0
0.0 ( 0 2 2 0 75 ) 0.0 0.0
0.0 ( 0 2 2 1 75 ) 0.0 0.0
(W,2,2,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 2, 0, 100)
0.8 ( 4 2 2 0 100 ) 0.0 0.0
0.2 ( 4 2 2 1 100 ) 0.0 0.0
0.0 ( 0 2 2 0 100 ) 0.0 0.0
0.0 ( 0 2 2 1 100 ) 0.0 0.0
(W,2,2,D,100):RIGHT=[-10.0]
(W,2,2,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 2, 1, 25)
0.375 ( 0 2 1 1 25 ) 0.0 0.0
0.375 ( 0 2 1 0 25 ) 0.0 0.0
0.125 ( 0 2 1 1 0 ) 0.0 50.0
0.125 ( 0 2 1 0 0 ) 0.0 50.0
(W,2,2,R,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 2, 2, 1, 50)
0.5 ( 4 2 2 1 50 ) 0.0 0.0
0.5 ( 4 2 2 0 50 ) 0.0 0.0
0.0 ( 0 2 2 1 50 ) 0.0 0.0
0.0 ( 0 2 2 0 50 ) 0.0 0.0
(W,2,2,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 2, 1, 75)
0.5 ( 4 2 2 1 75 ) 0.0 0.0
0.5 ( 4 2 2 0 75 ) 0.0 0.0
0.0 ( 0 2 2 1 75 ) 0.0 0.0
0.0 ( 0 2 2 0 75 ) 0.0 0.0
(W,2,2,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 2, 1, 100)
0.5 ( 4 2 2 1 100 ) 0.0 0.0
0.5 ( 4 2 2 0 100 ) 0.0 0.0
0.0 ( 0 2 2 1 100 ) 0.0 0.0
0.0 ( 0 2 2 0 100 ) 0.0 0.0
(W,2,2,R,100):RIGHT=[-10.0]
(W,2,3,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 3, 0, 25)
0.6000000000000001 ( 0 2 2 0 25 ) 0.0 0.0
0.15000000000000002 ( 0 2 2 1 25 ) 0.0 0.0
0.2 ( 0 2 2 0 0 ) 0.0 50.0
0.05 ( 0 2 2 1 0 ) 0.0 50.0
(W,2,3,D,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 2, 3, 0, 50)
0.8 ( 4 2 3 0 50 ) 0.0 0.0
0.2 ( 4 2 3 1 50 ) 0.0 0.0
0.0 ( 0 2 3 0 50 ) 0.0 0.0
0.0 ( 0 2 3 1 50 ) 0.0 0.0
(W,2,3,D,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 3, 0, 75)
0.8 ( 4 2 3 0 75 ) 0.0 0.0
0.2 ( 4 2 3 1 75 ) 0.0 0.0
0.0 ( 0 2 3 0 75 ) 0.0 0.0
0.0 ( 0 2 3 1 75 ) 0.0 0.0
(W,2,3,D,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 3, 0, 100)
0.8 ( 4 2 3 0 100 ) 0.0 0.0
0.2 ( 4 2 3 1 100 ) 0.0 0.0
0.0 ( 0 2 3 0 100 ) 0.0 0.0
0.0 ( 0 2 3 1 100 ) 0.0 0.0
(W,2,3,D,100):RIGHT=[-10.0]
(W,2,3,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 3, 1, 25)
0.375 ( 0 2 2 1 25 ) 0.0 0.0
0.375 ( 0 2 2 0 25 ) 0.0 0.0
0.125 ( 0 2 2 1 0 ) 0.0 50.0
0.125 ( 0 2 2 0 0 ) 0.0 50.0
(W,2,3,R,25):SHOOT=[2.488]
Deciding optimal action for state: (0, 2, 3, 1, 50)
0.5 ( 4 2 3 1 50 ) 0.0 0.0
0.5 ( 4 2 3 0 50 ) 0.0 0.0
0.0 ( 0 2 3 1 50 ) 0.0 0.0
0.0 ( 0 2 3 0 50 ) 0.0 0.0
(W,2,3,R,50):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 3, 1, 75)
0.5 ( 4 2 3 1 75 ) 0.0 0.0
0.5 ( 4 2 3 0 75 ) 0.0 0.0
0.0 ( 0 2 3 1 75 ) 0.0 0.0
0.0 ( 0 2 3 0 75 ) 0.0 0.0
(W,2,3,R,75):RIGHT=[-10.0]
Deciding optimal action for state: (0, 2, 3, 1, 100)
0.5 ( 4 2 3 1 100 ) 0.0 0.0
0.5 ( 4 2 3 0 100 ) 0.0 0.0
0.0 ( 0 2 3 1 100 ) 0.0 0.0
0.0 ( 0 2 3 0 100 ) 0.0 0.0
(W,2,3,R,100):RIGHT=[-10.0]
(N,0,0,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 0, 0, 25)
0.68 ( 3 0 0 0 25 ) 0.0 0.0
0.17 ( 3 0 0 1 25 ) 0.0 0.0
0.12 ( 2 0 0 0 25 ) 0.0 0.0
0.03 ( 2 0 0 1 25 ) 0.0 0.0
(N,0,0,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 0, 0, 50)
0.68 ( 3 0 0 0 50 ) 0.0 0.0
0.17 ( 3 0 0 1 50 ) 0.0 0.0
0.12 ( 2 0 0 0 50 ) 0.0 0.0
0.03 ( 2 0 0 1 50 ) 0.0 0.0
(N,0,0,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 0, 0, 75)
0.68 ( 3 0 0 0 75 ) 0.0 0.0
0.17 ( 3 0 0 1 75 ) 0.0 0.0
0.12 ( 2 0 0 0 75 ) 0.0 0.0
0.03 ( 2 0 0 1 75 ) 0.0 0.0
(N,0,0,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 0, 0, 100)
0.68 ( 3 0 0 0 100 ) 0.0 0.0
0.17 ( 3 0 0 1 100 ) 0.0 0.0
0.12 ( 2 0 0 0 100 ) 0.0 0.0
0.03 ( 2 0 0 1 100 ) 0.0 0.0
(N,0,0,D,100):DOWN=[-10.0]
(N,0,0,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 0, 1, 25)
0.425 ( 3 0 0 1 25 ) 0.0 0.0
0.425 ( 3 0 0 0 25 ) 0.0 0.0
0.075 ( 2 0 0 1 25 ) 0.0 0.0
0.075 ( 2 0 0 0 25 ) 0.0 0.0
(N,0,0,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 0, 1, 50)
0.425 ( 3 0 0 1 50 ) 0.0 0.0
0.425 ( 3 0 0 0 50 ) 0.0 0.0
0.075 ( 2 0 0 1 50 ) 0.0 0.0
0.075 ( 2 0 0 0 50 ) 0.0 0.0
(N,0,0,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 0, 1, 75)
0.425 ( 3 0 0 1 75 ) 0.0 0.0
0.425 ( 3 0 0 0 75 ) 0.0 0.0
0.075 ( 2 0 0 1 75 ) 0.0 0.0
0.075 ( 2 0 0 0 75 ) 0.0 0.0
(N,0,0,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 0, 1, 100)
0.425 ( 3 0 0 1 100 ) 0.0 0.0
0.425 ( 3 0 0 0 100 ) 0.0 0.0
0.075 ( 2 0 0 1 100 ) 0.0 0.0
0.075 ( 2 0 0 0 100 ) 0.0 0.0
(N,0,0,R,100):DOWN=[-10.0]
(N,0,1,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 1, 0, 25)
0.68 ( 3 0 1 0 25 ) 0.0 0.0
0.17 ( 3 0 1 1 25 ) 0.0 0.0
0.12 ( 2 0 1 0 25 ) 0.0 0.0
0.03 ( 2 0 1 1 25 ) 0.0 0.0
(N,0,1,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 1, 0, 50)
0.68 ( 3 0 1 0 50 ) 0.0 0.0
0.17 ( 3 0 1 1 50 ) 0.0 0.0
0.12 ( 2 0 1 0 50 ) 0.0 0.0
0.03 ( 2 0 1 1 50 ) 0.0 0.0
(N,0,1,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 1, 0, 75)
0.68 ( 3 0 1 0 75 ) 0.0 0.0
0.17 ( 3 0 1 1 75 ) 0.0 0.0
0.12 ( 2 0 1 0 75 ) 0.0 0.0
0.03 ( 2 0 1 1 75 ) 0.0 0.0
(N,0,1,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 1, 0, 100)
0.68 ( 3 0 1 0 100 ) 0.0 0.0
0.17 ( 3 0 1 1 100 ) 0.0 0.0
0.12 ( 2 0 1 0 100 ) 0.0 0.0
0.03 ( 2 0 1 1 100 ) 0.0 0.0
(N,0,1,D,100):DOWN=[-10.0]
(N,0,1,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 1, 1, 25)
0.425 ( 3 0 1 1 25 ) 0.0 0.0
0.425 ( 3 0 1 0 25 ) 0.0 0.0
0.075 ( 2 0 1 1 25 ) 0.0 0.0
0.075 ( 2 0 1 0 25 ) 0.0 0.0
(N,0,1,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 1, 1, 50)
0.425 ( 3 0 1 1 50 ) 0.0 0.0
0.425 ( 3 0 1 0 50 ) 0.0 0.0
0.075 ( 2 0 1 1 50 ) 0.0 0.0
0.075 ( 2 0 1 0 50 ) 0.0 0.0
(N,0,1,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 1, 1, 75)
0.425 ( 3 0 1 1 75 ) 0.0 0.0
0.425 ( 3 0 1 0 75 ) 0.0 0.0
0.075 ( 2 0 1 1 75 ) 0.0 0.0
0.075 ( 2 0 1 0 75 ) 0.0 0.0
(N,0,1,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 1, 1, 100)
0.425 ( 3 0 1 1 100 ) 0.0 0.0
0.425 ( 3 0 1 0 100 ) 0.0 0.0
0.075 ( 2 0 1 1 100 ) 0.0 0.0
0.075 ( 2 0 1 0 100 ) 0.0 0.0
(N,0,1,R,100):DOWN=[-10.0]
(N,0,2,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 2, 0, 25)
0.68 ( 3 0 2 0 25 ) 0.0 0.0
0.17 ( 3 0 2 1 25 ) 0.0 0.0
0.12 ( 2 0 2 0 25 ) 0.0 0.0
0.03 ( 2 0 2 1 25 ) 0.0 0.0
(N,0,2,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 2, 0, 50)
0.68 ( 3 0 2 0 50 ) 0.0 0.0
0.17 ( 3 0 2 1 50 ) 0.0 0.0
0.12 ( 2 0 2 0 50 ) 0.0 0.0
0.03 ( 2 0 2 1 50 ) 0.0 0.0
(N,0,2,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 2, 0, 75)
0.68 ( 3 0 2 0 75 ) 0.0 0.0
0.17 ( 3 0 2 1 75 ) 0.0 0.0
0.12 ( 2 0 2 0 75 ) 0.0 0.0
0.03 ( 2 0 2 1 75 ) 0.0 0.0
(N,0,2,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 2, 0, 100)
0.68 ( 3 0 2 0 100 ) 0.0 0.0
0.17 ( 3 0 2 1 100 ) 0.0 0.0
0.12 ( 2 0 2 0 100 ) 0.0 0.0
0.03 ( 2 0 2 1 100 ) 0.0 0.0
(N,0,2,D,100):DOWN=[-10.0]
(N,0,2,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 2, 1, 25)
0.425 ( 3 0 2 1 25 ) 0.0 0.0
0.425 ( 3 0 2 0 25 ) 0.0 0.0
0.075 ( 2 0 2 1 25 ) 0.0 0.0
0.075 ( 2 0 2 0 25 ) 0.0 0.0
(N,0,2,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 2, 1, 50)
0.425 ( 3 0 2 1 50 ) 0.0 0.0
0.425 ( 3 0 2 0 50 ) 0.0 0.0
0.075 ( 2 0 2 1 50 ) 0.0 0.0
0.075 ( 2 0 2 0 50 ) 0.0 0.0
(N,0,2,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 2, 1, 75)
0.425 ( 3 0 2 1 75 ) 0.0 0.0
0.425 ( 3 0 2 0 75 ) 0.0 0.0
0.075 ( 2 0 2 1 75 ) 0.0 0.0
0.075 ( 2 0 2 0 75 ) 0.0 0.0
(N,0,2,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 2, 1, 100)
0.425 ( 3 0 2 1 100 ) 0.0 0.0
0.425 ( 3 0 2 0 100 ) 0.0 0.0
0.075 ( 2 0 2 1 100 ) 0.0 0.0
0.075 ( 2 0 2 0 100 ) 0.0 0.0
(N,0,2,R,100):DOWN=[-10.0]
(N,0,3,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 3, 0, 25)
0.68 ( 3 0 3 0 25 ) 0.0 0.0
0.17 ( 3 0 3 1 25 ) 0.0 0.0
0.12 ( 2 0 3 0 25 ) 0.0 0.0
0.03 ( 2 0 3 1 25 ) 0.0 0.0
(N,0,3,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 3, 0, 50)
0.68 ( 3 0 3 0 50 ) 0.0 0.0
0.17 ( 3 0 3 1 50 ) 0.0 0.0
0.12 ( 2 0 3 0 50 ) 0.0 0.0
0.03 ( 2 0 3 1 50 ) 0.0 0.0
(N,0,3,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 3, 0, 75)
0.68 ( 3 0 3 0 75 ) 0.0 0.0
0.17 ( 3 0 3 1 75 ) 0.0 0.0
0.12 ( 2 0 3 0 75 ) 0.0 0.0
0.03 ( 2 0 3 1 75 ) 0.0 0.0
(N,0,3,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 3, 0, 100)
0.68 ( 3 0 3 0 100 ) 0.0 0.0
0.17 ( 3 0 3 1 100 ) 0.0 0.0
0.12 ( 2 0 3 0 100 ) 0.0 0.0
0.03 ( 2 0 3 1 100 ) 0.0 0.0
(N,0,3,D,100):DOWN=[-10.0]
(N,0,3,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 3, 1, 25)
0.425 ( 3 0 3 1 25 ) 0.0 0.0
0.425 ( 3 0 3 0 25 ) 0.0 0.0
0.075 ( 2 0 3 1 25 ) 0.0 0.0
0.075 ( 2 0 3 0 25 ) 0.0 0.0
(N,0,3,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 3, 1, 50)
0.425 ( 3 0 3 1 50 ) 0.0 0.0
0.425 ( 3 0 3 0 50 ) 0.0 0.0
0.075 ( 2 0 3 1 50 ) 0.0 0.0
0.075 ( 2 0 3 0 50 ) 0.0 0.0
(N,0,3,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 3, 1, 75)
0.425 ( 3 0 3 1 75 ) 0.0 0.0
0.425 ( 3 0 3 0 75 ) 0.0 0.0
0.075 ( 2 0 3 1 75 ) 0.0 0.0
0.075 ( 2 0 3 0 75 ) 0.0 0.0
(N,0,3,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 0, 3, 1, 100)
0.425 ( 3 0 3 1 100 ) 0.0 0.0
0.425 ( 3 0 3 0 100 ) 0.0 0.0
0.075 ( 2 0 3 1 100 ) 0.0 0.0
0.075 ( 2 0 3 0 100 ) 0.0 0.0
(N,0,3,R,100):DOWN=[-10.0]
(N,1,0,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 0, 0, 25)
0.68 ( 3 1 0 0 25 ) 0.0 0.0
0.17 ( 3 1 0 1 25 ) 0.0 0.0
0.12 ( 2 1 0 0 25 ) 0.0 0.0
0.03 ( 2 1 0 1 25 ) 0.0 0.0
(N,1,0,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 0, 0, 50)
0.68 ( 3 1 0 0 50 ) 0.0 0.0
0.17 ( 3 1 0 1 50 ) 0.0 0.0
0.12 ( 2 1 0 0 50 ) 0.0 0.0
0.03 ( 2 1 0 1 50 ) 0.0 0.0
(N,1,0,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 0, 0, 75)
0.68 ( 3 1 0 0 75 ) 0.0 0.0
0.17 ( 3 1 0 1 75 ) 0.0 0.0
0.12 ( 2 1 0 0 75 ) 0.0 0.0
0.03 ( 2 1 0 1 75 ) 0.0 0.0
(N,1,0,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 0, 0, 100)
0.68 ( 3 1 0 0 100 ) 0.0 0.0
0.17 ( 3 1 0 1 100 ) 0.0 0.0
0.12 ( 2 1 0 0 100 ) 0.0 0.0
0.03 ( 2 1 0 1 100 ) 0.0 0.0
(N,1,0,D,100):DOWN=[-10.0]
(N,1,0,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 0, 1, 25)
0.425 ( 3 1 0 1 25 ) 0.0 0.0
0.425 ( 3 1 0 0 25 ) 0.0 0.0
0.075 ( 2 1 0 1 25 ) 0.0 0.0
0.075 ( 2 1 0 0 25 ) 0.0 0.0
(N,1,0,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 0, 1, 50)
0.425 ( 3 1 0 1 50 ) 0.0 0.0
0.425 ( 3 1 0 0 50 ) 0.0 0.0
0.075 ( 2 1 0 1 50 ) 0.0 0.0
0.075 ( 2 1 0 0 50 ) 0.0 0.0
(N,1,0,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 0, 1, 75)
0.425 ( 3 1 0 1 75 ) 0.0 0.0
0.425 ( 3 1 0 0 75 ) 0.0 0.0
0.075 ( 2 1 0 1 75 ) 0.0 0.0
0.075 ( 2 1 0 0 75 ) 0.0 0.0
(N,1,0,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 0, 1, 100)
0.425 ( 3 1 0 1 100 ) 0.0 0.0
0.425 ( 3 1 0 0 100 ) 0.0 0.0
0.075 ( 2 1 0 1 100 ) 0.0 0.0
0.075 ( 2 1 0 0 100 ) 0.0 0.0
(N,1,0,R,100):DOWN=[-10.0]
(N,1,1,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 1, 0, 25)
0.68 ( 3 1 1 0 25 ) 0.0 0.0
0.17 ( 3 1 1 1 25 ) 0.0 0.0
0.12 ( 2 1 1 0 25 ) 0.0 0.0
0.03 ( 2 1 1 1 25 ) 0.0 0.0
(N,1,1,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 1, 0, 50)
0.68 ( 3 1 1 0 50 ) 0.0 0.0
0.17 ( 3 1 1 1 50 ) 0.0 0.0
0.12 ( 2 1 1 0 50 ) 0.0 0.0
0.03 ( 2 1 1 1 50 ) 0.0 0.0
(N,1,1,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 1, 0, 75)
0.68 ( 3 1 1 0 75 ) 0.0 0.0
0.17 ( 3 1 1 1 75 ) 0.0 0.0
0.12 ( 2 1 1 0 75 ) 0.0 0.0
0.03 ( 2 1 1 1 75 ) 0.0 0.0
(N,1,1,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 1, 0, 100)
0.68 ( 3 1 1 0 100 ) 0.0 0.0
0.17 ( 3 1 1 1 100 ) 0.0 0.0
0.12 ( 2 1 1 0 100 ) 0.0 0.0
0.03 ( 2 1 1 1 100 ) 0.0 0.0
(N,1,1,D,100):DOWN=[-10.0]
(N,1,1,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 1, 1, 25)
0.425 ( 3 1 1 1 25 ) 0.0 0.0
0.425 ( 3 1 1 0 25 ) 0.0 0.0
0.075 ( 2 1 1 1 25 ) 0.0 0.0
0.075 ( 2 1 1 0 25 ) 0.0 0.0
(N,1,1,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 1, 1, 50)
0.425 ( 3 1 1 1 50 ) 0.0 0.0
0.425 ( 3 1 1 0 50 ) 0.0 0.0
0.075 ( 2 1 1 1 50 ) 0.0 0.0
0.075 ( 2 1 1 0 50 ) 0.0 0.0
(N,1,1,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 1, 1, 75)
0.425 ( 3 1 1 1 75 ) 0.0 0.0
0.425 ( 3 1 1 0 75 ) 0.0 0.0
0.075 ( 2 1 1 1 75 ) 0.0 0.0
0.075 ( 2 1 1 0 75 ) 0.0 0.0
(N,1,1,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 1, 1, 100)
0.425 ( 3 1 1 1 100 ) 0.0 0.0
0.425 ( 3 1 1 0 100 ) 0.0 0.0
0.075 ( 2 1 1 1 100 ) 0.0 0.0
0.075 ( 2 1 1 0 100 ) 0.0 0.0
(N,1,1,R,100):DOWN=[-10.0]
(N,1,2,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 2, 0, 25)
0.68 ( 3 1 2 0 25 ) 0.0 0.0
0.17 ( 3 1 2 1 25 ) 0.0 0.0
0.12 ( 2 1 2 0 25 ) 0.0 0.0
0.03 ( 2 1 2 1 25 ) 0.0 0.0
(N,1,2,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 2, 0, 50)
0.68 ( 3 1 2 0 50 ) 0.0 0.0
0.17 ( 3 1 2 1 50 ) 0.0 0.0
0.12 ( 2 1 2 0 50 ) 0.0 0.0
0.03 ( 2 1 2 1 50 ) 0.0 0.0
(N,1,2,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 2, 0, 75)
0.68 ( 3 1 2 0 75 ) 0.0 0.0
0.17 ( 3 1 2 1 75 ) 0.0 0.0
0.12 ( 2 1 2 0 75 ) 0.0 0.0
0.03 ( 2 1 2 1 75 ) 0.0 0.0
(N,1,2,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 2, 0, 100)
0.68 ( 3 1 2 0 100 ) 0.0 0.0
0.17 ( 3 1 2 1 100 ) 0.0 0.0
0.12 ( 2 1 2 0 100 ) 0.0 0.0
0.03 ( 2 1 2 1 100 ) 0.0 0.0
(N,1,2,D,100):DOWN=[-10.0]
(N,1,2,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 2, 1, 25)
0.425 ( 3 1 2 1 25 ) 0.0 0.0
0.425 ( 3 1 2 0 25 ) 0.0 0.0
0.075 ( 2 1 2 1 25 ) 0.0 0.0
0.075 ( 2 1 2 0 25 ) 0.0 0.0
(N,1,2,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 2, 1, 50)
0.425 ( 3 1 2 1 50 ) 0.0 0.0
0.425 ( 3 1 2 0 50 ) 0.0 0.0
0.075 ( 2 1 2 1 50 ) 0.0 0.0
0.075 ( 2 1 2 0 50 ) 0.0 0.0
(N,1,2,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 2, 1, 75)
0.425 ( 3 1 2 1 75 ) 0.0 0.0
0.425 ( 3 1 2 0 75 ) 0.0 0.0
0.075 ( 2 1 2 1 75 ) 0.0 0.0
0.075 ( 2 1 2 0 75 ) 0.0 0.0
(N,1,2,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 2, 1, 100)
0.425 ( 3 1 2 1 100 ) 0.0 0.0
0.425 ( 3 1 2 0 100 ) 0.0 0.0
0.075 ( 2 1 2 1 100 ) 0.0 0.0
0.075 ( 2 1 2 0 100 ) 0.0 0.0
(N,1,2,R,100):DOWN=[-10.0]
(N,1,3,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 3, 0, 25)
0.68 ( 3 1 3 0 25 ) 0.0 0.0
0.17 ( 3 1 3 1 25 ) 0.0 0.0
0.12 ( 2 1 3 0 25 ) 0.0 0.0
0.03 ( 2 1 3 1 25 ) 0.0 0.0
(N,1,3,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 3, 0, 50)
0.68 ( 3 1 3 0 50 ) 0.0 0.0
0.17 ( 3 1 3 1 50 ) 0.0 0.0
0.12 ( 2 1 3 0 50 ) 0.0 0.0
0.03 ( 2 1 3 1 50 ) 0.0 0.0
(N,1,3,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 3, 0, 75)
0.68 ( 3 1 3 0 75 ) 0.0 0.0
0.17 ( 3 1 3 1 75 ) 0.0 0.0
0.12 ( 2 1 3 0 75 ) 0.0 0.0
0.03 ( 2 1 3 1 75 ) 0.0 0.0
(N,1,3,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 3, 0, 100)
0.68 ( 3 1 3 0 100 ) 0.0 0.0
0.17 ( 3 1 3 1 100 ) 0.0 0.0
0.12 ( 2 1 3 0 100 ) 0.0 0.0
0.03 ( 2 1 3 1 100 ) 0.0 0.0
(N,1,3,D,100):DOWN=[-10.0]
(N,1,3,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 3, 1, 25)
0.425 ( 3 1 3 1 25 ) 0.0 0.0
0.425 ( 3 1 3 0 25 ) 0.0 0.0
0.075 ( 2 1 3 1 25 ) 0.0 0.0
0.075 ( 2 1 3 0 25 ) 0.0 0.0
(N,1,3,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 3, 1, 50)
0.425 ( 3 1 3 1 50 ) 0.0 0.0
0.425 ( 3 1 3 0 50 ) 0.0 0.0
0.075 ( 2 1 3 1 50 ) 0.0 0.0
0.075 ( 2 1 3 0 50 ) 0.0 0.0
(N,1,3,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 3, 1, 75)
0.425 ( 3 1 3 1 75 ) 0.0 0.0
0.425 ( 3 1 3 0 75 ) 0.0 0.0
0.075 ( 2 1 3 1 75 ) 0.0 0.0
0.075 ( 2 1 3 0 75 ) 0.0 0.0
(N,1,3,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 1, 3, 1, 100)
0.425 ( 3 1 3 1 100 ) 0.0 0.0
0.425 ( 3 1 3 0 100 ) 0.0 0.0
0.075 ( 2 1 3 1 100 ) 0.0 0.0
0.075 ( 2 1 3 0 100 ) 0.0 0.0
(N,1,3,R,100):DOWN=[-10.0]
(N,2,0,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 0, 0, 25)
0.68 ( 3 2 0 0 25 ) 0.0 0.0
0.17 ( 3 2 0 1 25 ) 0.0 0.0
0.12 ( 2 2 0 0 25 ) 0.0 0.0
0.03 ( 2 2 0 1 25 ) 0.0 0.0
(N,2,0,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 0, 0, 50)
0.68 ( 3 2 0 0 50 ) 0.0 0.0
0.17 ( 3 2 0 1 50 ) 0.0 0.0
0.12 ( 2 2 0 0 50 ) 0.0 0.0
0.03 ( 2 2 0 1 50 ) 0.0 0.0
(N,2,0,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 0, 0, 75)
0.68 ( 3 2 0 0 75 ) 0.0 0.0
0.17 ( 3 2 0 1 75 ) 0.0 0.0
0.12 ( 2 2 0 0 75 ) 0.0 0.0
0.03 ( 2 2 0 1 75 ) 0.0 0.0
(N,2,0,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 0, 0, 100)
0.68 ( 3 2 0 0 100 ) 0.0 0.0
0.17 ( 3 2 0 1 100 ) 0.0 0.0
0.12 ( 2 2 0 0 100 ) 0.0 0.0
0.03 ( 2 2 0 1 100 ) 0.0 0.0
(N,2,0,D,100):DOWN=[-10.0]
(N,2,0,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 0, 1, 25)
0.425 ( 3 2 0 1 25 ) 0.0 0.0
0.425 ( 3 2 0 0 25 ) 0.0 0.0
0.075 ( 2 2 0 1 25 ) 0.0 0.0
0.075 ( 2 2 0 0 25 ) 0.0 0.0
(N,2,0,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 0, 1, 50)
0.425 ( 3 2 0 1 50 ) 0.0 0.0
0.425 ( 3 2 0 0 50 ) 0.0 0.0
0.075 ( 2 2 0 1 50 ) 0.0 0.0
0.075 ( 2 2 0 0 50 ) 0.0 0.0
(N,2,0,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 0, 1, 75)
0.425 ( 3 2 0 1 75 ) 0.0 0.0
0.425 ( 3 2 0 0 75 ) 0.0 0.0
0.075 ( 2 2 0 1 75 ) 0.0 0.0
0.075 ( 2 2 0 0 75 ) 0.0 0.0
(N,2,0,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 0, 1, 100)
0.425 ( 3 2 0 1 100 ) 0.0 0.0
0.425 ( 3 2 0 0 100 ) 0.0 0.0
0.075 ( 2 2 0 1 100 ) 0.0 0.0
0.075 ( 2 2 0 0 100 ) 0.0 0.0
(N,2,0,R,100):DOWN=[-10.0]
(N,2,1,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 1, 0, 25)
0.68 ( 3 2 1 0 25 ) 0.0 0.0
0.17 ( 3 2 1 1 25 ) 0.0 0.0
0.12 ( 2 2 1 0 25 ) 0.0 0.0
0.03 ( 2 2 1 1 25 ) 0.0 0.0
(N,2,1,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 1, 0, 50)
0.68 ( 3 2 1 0 50 ) 0.0 0.0
0.17 ( 3 2 1 1 50 ) 0.0 0.0
0.12 ( 2 2 1 0 50 ) 0.0 0.0
0.03 ( 2 2 1 1 50 ) 0.0 0.0
(N,2,1,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 1, 0, 75)
0.68 ( 3 2 1 0 75 ) 0.0 0.0
0.17 ( 3 2 1 1 75 ) 0.0 0.0
0.12 ( 2 2 1 0 75 ) 0.0 0.0
0.03 ( 2 2 1 1 75 ) 0.0 0.0
(N,2,1,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 1, 0, 100)
0.68 ( 3 2 1 0 100 ) 0.0 0.0
0.17 ( 3 2 1 1 100 ) 0.0 0.0
0.12 ( 2 2 1 0 100 ) 0.0 0.0
0.03 ( 2 2 1 1 100 ) 0.0 0.0
(N,2,1,D,100):DOWN=[-10.0]
(N,2,1,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 1, 1, 25)
0.425 ( 3 2 1 1 25 ) 0.0 0.0
0.425 ( 3 2 1 0 25 ) 0.0 0.0
0.075 ( 2 2 1 1 25 ) 0.0 0.0
0.075 ( 2 2 1 0 25 ) 0.0 0.0
(N,2,1,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 1, 1, 50)
0.425 ( 3 2 1 1 50 ) 0.0 0.0
0.425 ( 3 2 1 0 50 ) 0.0 0.0
0.075 ( 2 2 1 1 50 ) 0.0 0.0
0.075 ( 2 2 1 0 50 ) 0.0 0.0
(N,2,1,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 1, 1, 75)
0.425 ( 3 2 1 1 75 ) 0.0 0.0
0.425 ( 3 2 1 0 75 ) 0.0 0.0
0.075 ( 2 2 1 1 75 ) 0.0 0.0
0.075 ( 2 2 1 0 75 ) 0.0 0.0
(N,2,1,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 1, 1, 100)
0.425 ( 3 2 1 1 100 ) 0.0 0.0
0.425 ( 3 2 1 0 100 ) 0.0 0.0
0.075 ( 2 2 1 1 100 ) 0.0 0.0
0.075 ( 2 2 1 0 100 ) 0.0 0.0
(N,2,1,R,100):DOWN=[-10.0]
(N,2,2,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 2, 0, 25)
0.68 ( 3 2 2 0 25 ) 0.0 0.0
0.17 ( 3 2 2 1 25 ) 0.0 0.0
0.12 ( 2 2 2 0 25 ) 0.0 0.0
0.03 ( 2 2 2 1 25 ) 0.0 0.0
(N,2,2,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 2, 0, 50)
0.68 ( 3 2 2 0 50 ) 0.0 0.0
0.17 ( 3 2 2 1 50 ) 0.0 0.0
0.12 ( 2 2 2 0 50 ) 0.0 0.0
0.03 ( 2 2 2 1 50 ) 0.0 0.0
(N,2,2,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 2, 0, 75)
0.68 ( 3 2 2 0 75 ) 0.0 0.0
0.17 ( 3 2 2 1 75 ) 0.0 0.0
0.12 ( 2 2 2 0 75 ) 0.0 0.0
0.03 ( 2 2 2 1 75 ) 0.0 0.0
(N,2,2,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 2, 0, 100)
0.68 ( 3 2 2 0 100 ) 0.0 0.0
0.17 ( 3 2 2 1 100 ) 0.0 0.0
0.12 ( 2 2 2 0 100 ) 0.0 0.0
0.03 ( 2 2 2 1 100 ) 0.0 0.0
(N,2,2,D,100):DOWN=[-10.0]
(N,2,2,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 2, 1, 25)
0.425 ( 3 2 2 1 25 ) 0.0 0.0
0.425 ( 3 2 2 0 25 ) 0.0 0.0
0.075 ( 2 2 2 1 25 ) 0.0 0.0
0.075 ( 2 2 2 0 25 ) 0.0 0.0
(N,2,2,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 2, 1, 50)
0.425 ( 3 2 2 1 50 ) 0.0 0.0
0.425 ( 3 2 2 0 50 ) 0.0 0.0
0.075 ( 2 2 2 1 50 ) 0.0 0.0
0.075 ( 2 2 2 0 50 ) 0.0 0.0
(N,2,2,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 2, 1, 75)
0.425 ( 3 2 2 1 75 ) 0.0 0.0
0.425 ( 3 2 2 0 75 ) 0.0 0.0
0.075 ( 2 2 2 1 75 ) 0.0 0.0
0.075 ( 2 2 2 0 75 ) 0.0 0.0
(N,2,2,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 2, 1, 100)
0.425 ( 3 2 2 1 100 ) 0.0 0.0
0.425 ( 3 2 2 0 100 ) 0.0 0.0
0.075 ( 2 2 2 1 100 ) 0.0 0.0
0.075 ( 2 2 2 0 100 ) 0.0 0.0
(N,2,2,R,100):DOWN=[-10.0]
(N,2,3,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 3, 0, 25)
0.68 ( 3 2 3 0 25 ) 0.0 0.0
0.17 ( 3 2 3 1 25 ) 0.0 0.0
0.12 ( 2 2 3 0 25 ) 0.0 0.0
0.03 ( 2 2 3 1 25 ) 0.0 0.0
(N,2,3,D,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 3, 0, 50)
0.68 ( 3 2 3 0 50 ) 0.0 0.0
0.17 ( 3 2 3 1 50 ) 0.0 0.0
0.12 ( 2 2 3 0 50 ) 0.0 0.0
0.03 ( 2 2 3 1 50 ) 0.0 0.0
(N,2,3,D,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 3, 0, 75)
0.68 ( 3 2 3 0 75 ) 0.0 0.0
0.17 ( 3 2 3 1 75 ) 0.0 0.0
0.12 ( 2 2 3 0 75 ) 0.0 0.0
0.03 ( 2 2 3 1 75 ) 0.0 0.0
(N,2,3,D,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 3, 0, 100)
0.68 ( 3 2 3 0 100 ) 0.0 0.0
0.17 ( 3 2 3 1 100 ) 0.0 0.0
0.12 ( 2 2 3 0 100 ) 0.0 0.0
0.03 ( 2 2 3 1 100 ) 0.0 0.0
(N,2,3,D,100):DOWN=[-10.0]
(N,2,3,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 3, 1, 25)
0.425 ( 3 2 3 1 25 ) 0.0 0.0
0.425 ( 3 2 3 0 25 ) 0.0 0.0
0.075 ( 2 2 3 1 25 ) 0.0 0.0
0.075 ( 2 2 3 0 25 ) 0.0 0.0
(N,2,3,R,25):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 3, 1, 50)
0.425 ( 3 2 3 1 50 ) 0.0 0.0
0.425 ( 3 2 3 0 50 ) 0.0 0.0
0.075 ( 2 2 3 1 50 ) 0.0 0.0
0.075 ( 2 2 3 0 50 ) 0.0 0.0
(N,2,3,R,50):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 3, 1, 75)
0.425 ( 3 2 3 1 75 ) 0.0 0.0
0.425 ( 3 2 3 0 75 ) 0.0 0.0
0.075 ( 2 2 3 1 75 ) 0.0 0.0
0.075 ( 2 2 3 0 75 ) 0.0 0.0
(N,2,3,R,75):DOWN=[-10.0]
Deciding optimal action for state: (1, 2, 3, 1, 100)
0.425 ( 3 2 3 1 100 ) 0.0 0.0
0.425 ( 3 2 3 0 100 ) 0.0 0.0
0.075 ( 2 2 3 1 100 ) 0.0 0.0
0.075 ( 2 2 3 0 100 ) 0.0 0.0
(N,2,3,R,100):DOWN=[-10.0]
(E,0,0,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 0, 0, 25)
0.6400000000000001 ( 2 0 0 0 25 ) 0.0 0.0
0.16000000000000003 ( 2 0 0 1 25 ) 0.0 0.0
0.16000000000000003 ( 2 0 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 0 0 1 0 ) 0.0 50.0
(E,0,0,D,25):HIT=[-0.01]
Deciding optimal action for state: (2, 0, 0, 0, 50)
0.6400000000000001 ( 2 0 0 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 0 0 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 0 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 0 0 1 0 ) 0.0 50.0
(E,0,0,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 0, 0, 0, 75)
0.8 ( 4 0 0 0 75 ) 0.0 0.0
0.2 ( 4 0 0 1 75 ) 0.0 0.0
0.0 ( 2 0 0 0 75 ) 0.0 0.0
0.0 ( 2 0 0 1 75 ) 0.0 0.0
(E,0,0,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 0, 0, 0, 100)
0.8 ( 4 0 0 0 100 ) 0.0 0.0
0.2 ( 4 0 0 1 100 ) 0.0 0.0
0.0 ( 2 0 0 0 100 ) 0.0 0.0
0.0 ( 2 0 0 1 100 ) 0.0 0.0
(E,0,0,D,100):LEFT=[-10.0]
(E,0,0,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 0, 1, 25)
0.4 ( 2 0 0 1 25 ) 0.0 0.0
0.1 ( 2 0 0 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 50 ) 0.0 0.0
(E,0,0,R,25):HIT=[-24.985]
Deciding optimal action for state: (2, 0, 0, 1, 50)
0.4 ( 2 0 0 1 50 ) 0.0 0.0
0.1 ( 2 0 0 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 75 ) 0.0 0.0
(E,0,0,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 0, 0, 1, 75)
0.5 ( 4 0 0 1 75 ) 0.0 0.0
0.0 ( 2 0 0 1 75 ) 0.0 0.0
0.5 ( 2 0 0 0 100 ) 0.0 0.0
(E,0,0,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 0, 0, 1, 100)
0.5 ( 4 0 0 1 100 ) 0.0 0.0
0.0 ( 2 0 0 1 100 ) 0.0 0.0
0.5 ( 2 0 0 0 100 ) 0.0 0.0
(E,0,0,R,100):LEFT=[-29.98]
(E,0,1,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 1, 0, 25)
0.08000000000000002 ( 2 0 0 0 25 ) 0.0 0.0
0.020000000000000004 ( 2 0 0 1 25 ) 0.0 0.0
0.7200000000000001 ( 2 0 0 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 0 0 1 0 ) 0.0 50.0
(E,0,1,D,25):SHOOT=[34.955]
Deciding optimal action for state: (2, 0, 1, 0, 50)
0.6400000000000001 ( 2 0 1 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 0 1 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 0 1 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 0 1 1 0 ) 0.0 50.0
(E,0,1,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 0, 1, 0, 75)
0.8 ( 4 0 1 0 75 ) 0.0 0.0
0.2 ( 4 0 1 1 75 ) 0.0 0.0
0.0 ( 2 0 1 0 75 ) 0.0 0.0
0.0 ( 2 0 1 1 75 ) 0.0 0.0
(E,0,1,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 0, 1, 0, 100)
0.8 ( 4 0 1 0 100 ) 0.0 0.0
0.2 ( 4 0 1 1 100 ) 0.0 0.0
0.0 ( 2 0 1 0 100 ) 0.0 0.0
0.0 ( 2 0 1 1 100 ) 0.0 0.0
(E,0,1,D,100):LEFT=[-10.0]
(E,0,1,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 1, 1, 25)
0.05 ( 2 0 0 1 25 ) 0.0 0.0
0.45 ( 2 0 0 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 50 ) 0.0 0.0
(E,0,1,R,25):SHOOT=[-7.502]
Deciding optimal action for state: (2, 0, 1, 1, 50)
0.4 ( 2 0 1 1 50 ) 0.0 0.0
0.1 ( 2 0 1 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 75 ) 0.0 0.0
(E,0,1,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 0, 1, 1, 75)
0.5 ( 4 0 1 1 75 ) 0.0 0.0
0.0 ( 2 0 1 1 75 ) 0.0 0.0
0.5 ( 2 0 0 0 100 ) 0.0 0.0
(E,0,1,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 0, 1, 1, 100)
0.5 ( 4 0 1 1 100 ) 0.0 0.0
0.0 ( 2 0 1 1 100 ) 0.0 0.0
0.5 ( 2 0 0 0 100 ) 0.0 0.0
(E,0,1,R,100):LEFT=[-29.98]
(E,0,2,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 2, 0, 25)
0.08000000000000002 ( 2 0 1 0 25 ) 0.0 0.0
0.020000000000000004 ( 2 0 1 1 25 ) 0.0 0.0
0.7200000000000001 ( 2 0 1 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 0 1 1 0 ) 0.0 50.0
(E,0,2,D,25):SHOOT=[34.955]
Deciding optimal action for state: (2, 0, 2, 0, 50)
0.6400000000000001 ( 2 0 2 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 0 2 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 0 2 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 0 2 1 0 ) 0.0 50.0
(E,0,2,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 0, 2, 0, 75)
0.8 ( 4 0 2 0 75 ) 0.0 0.0
0.2 ( 4 0 2 1 75 ) 0.0 0.0
0.0 ( 2 0 2 0 75 ) 0.0 0.0
0.0 ( 2 0 2 1 75 ) 0.0 0.0
(E,0,2,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 0, 2, 0, 100)
0.8 ( 4 0 2 0 100 ) 0.0 0.0
0.2 ( 4 0 2 1 100 ) 0.0 0.0
0.0 ( 2 0 2 0 100 ) 0.0 0.0
0.0 ( 2 0 2 1 100 ) 0.0 0.0
(E,0,2,D,100):LEFT=[-10.0]
(E,0,2,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 2, 1, 25)
0.05 ( 2 0 1 1 25 ) 0.0 0.0
0.45 ( 2 0 1 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 50 ) 0.0 0.0
(E,0,2,R,25):SHOOT=[-7.502]
Deciding optimal action for state: (2, 0, 2, 1, 50)
0.4 ( 2 0 2 1 50 ) 0.0 0.0
0.1 ( 2 0 2 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 75 ) 0.0 0.0
(E,0,2,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 0, 2, 1, 75)
0.5 ( 4 0 2 1 75 ) 0.0 0.0
0.0 ( 2 0 2 1 75 ) 0.0 0.0
0.5 ( 2 0 0 0 100 ) 0.0 0.0
(E,0,2,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 0, 2, 1, 100)
0.5 ( 4 0 2 1 100 ) 0.0 0.0
0.0 ( 2 0 2 1 100 ) 0.0 0.0
0.5 ( 2 0 0 0 100 ) 0.0 0.0
(E,0,2,R,100):LEFT=[-29.98]
(E,0,3,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 3, 0, 25)
0.08000000000000002 ( 2 0 2 0 25 ) 0.0 0.0
0.020000000000000004 ( 2 0 2 1 25 ) 0.0 0.0
0.7200000000000001 ( 2 0 2 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 0 2 1 0 ) 0.0 50.0
(E,0,3,D,25):SHOOT=[34.955]
Deciding optimal action for state: (2, 0, 3, 0, 50)
0.6400000000000001 ( 2 0 3 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 0 3 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 0 3 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 0 3 1 0 ) 0.0 50.0
(E,0,3,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 0, 3, 0, 75)
0.8 ( 4 0 3 0 75 ) 0.0 0.0
0.2 ( 4 0 3 1 75 ) 0.0 0.0
0.0 ( 2 0 3 0 75 ) 0.0 0.0
0.0 ( 2 0 3 1 75 ) 0.0 0.0
(E,0,3,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 0, 3, 0, 100)
0.8 ( 4 0 3 0 100 ) 0.0 0.0
0.2 ( 4 0 3 1 100 ) 0.0 0.0
0.0 ( 2 0 3 0 100 ) 0.0 0.0
0.0 ( 2 0 3 1 100 ) 0.0 0.0
(E,0,3,D,100):LEFT=[-10.0]
(E,0,3,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 3, 1, 25)
0.05 ( 2 0 2 1 25 ) 0.0 0.0
0.45 ( 2 0 2 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 50 ) 0.0 0.0
(E,0,3,R,25):SHOOT=[-7.502]
Deciding optimal action for state: (2, 0, 3, 1, 50)
0.4 ( 2 0 3 1 50 ) 0.0 0.0
0.1 ( 2 0 3 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 75 ) 0.0 0.0
(E,0,3,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 0, 3, 1, 75)
0.5 ( 4 0 3 1 75 ) 0.0 0.0
0.0 ( 2 0 3 1 75 ) 0.0 0.0
0.5 ( 2 0 0 0 100 ) 0.0 0.0
(E,0,3,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 0, 3, 1, 100)
0.5 ( 4 0 3 1 100 ) 0.0 0.0
0.0 ( 2 0 3 1 100 ) 0.0 0.0
0.5 ( 2 0 0 0 100 ) 0.0 0.0
(E,0,3,R,100):LEFT=[-29.98]
(E,1,0,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 0, 0, 25)
0.6400000000000001 ( 2 1 0 0 25 ) 0.0 0.0
0.16000000000000003 ( 2 1 0 1 25 ) 0.0 0.0
0.16000000000000003 ( 2 1 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 1 0 1 0 ) 0.0 50.0
(E,1,0,D,25):HIT=[-0.01]
Deciding optimal action for state: (2, 1, 0, 0, 50)
0.6400000000000001 ( 2 1 0 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 1 0 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 1 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 1 0 1 0 ) 0.0 50.0
(E,1,0,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 1, 0, 0, 75)
0.8 ( 4 1 0 0 75 ) 0.0 0.0
0.2 ( 4 1 0 1 75 ) 0.0 0.0
0.0 ( 2 1 0 0 75 ) 0.0 0.0
0.0 ( 2 1 0 1 75 ) 0.0 0.0
(E,1,0,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 1, 0, 0, 100)
0.8 ( 4 1 0 0 100 ) 0.0 0.0
0.2 ( 4 1 0 1 100 ) 0.0 0.0
0.0 ( 2 1 0 0 100 ) 0.0 0.0
0.0 ( 2 1 0 1 100 ) 0.0 0.0
(E,1,0,D,100):LEFT=[-10.0]
(E,1,0,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 0, 1, 25)
0.4 ( 2 1 0 1 25 ) 0.0 0.0
0.1 ( 2 1 0 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 50 ) 0.0 0.0
(E,1,0,R,25):HIT=[-24.985]
Deciding optimal action for state: (2, 1, 0, 1, 50)
0.4 ( 2 1 0 1 50 ) 0.0 0.0
0.1 ( 2 1 0 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 75 ) 0.0 0.0
(E,1,0,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 1, 0, 1, 75)
0.5 ( 4 1 0 1 75 ) 0.0 0.0
0.0 ( 2 1 0 1 75 ) 0.0 0.0
0.5 ( 2 1 0 0 100 ) 0.0 0.0
(E,1,0,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 1, 0, 1, 100)
0.5 ( 4 1 0 1 100 ) 0.0 0.0
0.0 ( 2 1 0 1 100 ) 0.0 0.0
0.5 ( 2 1 0 0 100 ) 0.0 0.0
(E,1,0,R,100):LEFT=[-29.98]
(E,1,1,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 1, 0, 25)
0.08000000000000002 ( 2 1 0 0 25 ) 0.0 0.0
0.020000000000000004 ( 2 1 0 1 25 ) 0.0 0.0
0.7200000000000001 ( 2 1 0 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 1 0 1 0 ) 0.0 50.0
(E,1,1,D,25):SHOOT=[34.955]
Deciding optimal action for state: (2, 1, 1, 0, 50)
0.6400000000000001 ( 2 1 1 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 1 1 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 1 1 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 1 1 1 0 ) 0.0 50.0
(E,1,1,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 1, 1, 0, 75)
0.8 ( 4 1 1 0 75 ) 0.0 0.0
0.2 ( 4 1 1 1 75 ) 0.0 0.0
0.0 ( 2 1 1 0 75 ) 0.0 0.0
0.0 ( 2 1 1 1 75 ) 0.0 0.0
(E,1,1,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 1, 1, 0, 100)
0.8 ( 4 1 1 0 100 ) 0.0 0.0
0.2 ( 4 1 1 1 100 ) 0.0 0.0
0.0 ( 2 1 1 0 100 ) 0.0 0.0
0.0 ( 2 1 1 1 100 ) 0.0 0.0
(E,1,1,D,100):LEFT=[-10.0]
(E,1,1,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 1, 1, 25)
0.05 ( 2 1 0 1 25 ) 0.0 0.0
0.45 ( 2 1 0 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 50 ) 0.0 0.0
(E,1,1,R,25):SHOOT=[-7.502]
Deciding optimal action for state: (2, 1, 1, 1, 50)
0.4 ( 2 1 1 1 50 ) 0.0 0.0
0.1 ( 2 1 1 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 75 ) 0.0 0.0
(E,1,1,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 1, 1, 1, 75)
0.5 ( 4 1 1 1 75 ) 0.0 0.0
0.0 ( 2 1 1 1 75 ) 0.0 0.0
0.5 ( 2 1 0 0 100 ) 0.0 0.0
(E,1,1,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 1, 1, 1, 100)
0.5 ( 4 1 1 1 100 ) 0.0 0.0
0.0 ( 2 1 1 1 100 ) 0.0 0.0
0.5 ( 2 1 0 0 100 ) 0.0 0.0
(E,1,1,R,100):LEFT=[-29.98]
(E,1,2,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 2, 0, 25)
0.08000000000000002 ( 2 1 1 0 25 ) 0.0 0.0
0.020000000000000004 ( 2 1 1 1 25 ) 0.0 0.0
0.7200000000000001 ( 2 1 1 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 1 1 1 0 ) 0.0 50.0
(E,1,2,D,25):SHOOT=[34.955]
Deciding optimal action for state: (2, 1, 2, 0, 50)
0.6400000000000001 ( 2 1 2 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 1 2 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 1 2 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 1 2 1 0 ) 0.0 50.0
(E,1,2,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 1, 2, 0, 75)
0.8 ( 4 1 2 0 75 ) 0.0 0.0
0.2 ( 4 1 2 1 75 ) 0.0 0.0
0.0 ( 2 1 2 0 75 ) 0.0 0.0
0.0 ( 2 1 2 1 75 ) 0.0 0.0
(E,1,2,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 1, 2, 0, 100)
0.8 ( 4 1 2 0 100 ) 0.0 0.0
0.2 ( 4 1 2 1 100 ) 0.0 0.0
0.0 ( 2 1 2 0 100 ) 0.0 0.0
0.0 ( 2 1 2 1 100 ) 0.0 0.0
(E,1,2,D,100):LEFT=[-10.0]
(E,1,2,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 2, 1, 25)
0.05 ( 2 1 1 1 25 ) 0.0 0.0
0.45 ( 2 1 1 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 50 ) 0.0 0.0
(E,1,2,R,25):SHOOT=[-7.502]
Deciding optimal action for state: (2, 1, 2, 1, 50)
0.4 ( 2 1 2 1 50 ) 0.0 0.0
0.1 ( 2 1 2 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 75 ) 0.0 0.0
(E,1,2,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 1, 2, 1, 75)
0.5 ( 4 1 2 1 75 ) 0.0 0.0
0.0 ( 2 1 2 1 75 ) 0.0 0.0
0.5 ( 2 1 0 0 100 ) 0.0 0.0
(E,1,2,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 1, 2, 1, 100)
0.5 ( 4 1 2 1 100 ) 0.0 0.0
0.0 ( 2 1 2 1 100 ) 0.0 0.0
0.5 ( 2 1 0 0 100 ) 0.0 0.0
(E,1,2,R,100):LEFT=[-29.98]
(E,1,3,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 3, 0, 25)
0.08000000000000002 ( 2 1 2 0 25 ) 0.0 0.0
0.020000000000000004 ( 2 1 2 1 25 ) 0.0 0.0
0.7200000000000001 ( 2 1 2 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 1 2 1 0 ) 0.0 50.0
(E,1,3,D,25):SHOOT=[34.955]
Deciding optimal action for state: (2, 1, 3, 0, 50)
0.6400000000000001 ( 2 1 3 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 1 3 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 1 3 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 1 3 1 0 ) 0.0 50.0
(E,1,3,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 1, 3, 0, 75)
0.8 ( 4 1 3 0 75 ) 0.0 0.0
0.2 ( 4 1 3 1 75 ) 0.0 0.0
0.0 ( 2 1 3 0 75 ) 0.0 0.0
0.0 ( 2 1 3 1 75 ) 0.0 0.0
(E,1,3,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 1, 3, 0, 100)
0.8 ( 4 1 3 0 100 ) 0.0 0.0
0.2 ( 4 1 3 1 100 ) 0.0 0.0
0.0 ( 2 1 3 0 100 ) 0.0 0.0
0.0 ( 2 1 3 1 100 ) 0.0 0.0
(E,1,3,D,100):LEFT=[-10.0]
(E,1,3,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 3, 1, 25)
0.05 ( 2 1 2 1 25 ) 0.0 0.0
0.45 ( 2 1 2 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 50 ) 0.0 0.0
(E,1,3,R,25):SHOOT=[-7.502]
Deciding optimal action for state: (2, 1, 3, 1, 50)
0.4 ( 2 1 3 1 50 ) 0.0 0.0
0.1 ( 2 1 3 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 75 ) 0.0 0.0
(E,1,3,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 1, 3, 1, 75)
0.5 ( 4 1 3 1 75 ) 0.0 0.0
0.0 ( 2 1 3 1 75 ) 0.0 0.0
0.5 ( 2 1 0 0 100 ) 0.0 0.0
(E,1,3,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 1, 3, 1, 100)
0.5 ( 4 1 3 1 100 ) 0.0 0.0
0.0 ( 2 1 3 1 100 ) 0.0 0.0
0.5 ( 2 1 0 0 100 ) 0.0 0.0
(E,1,3,R,100):LEFT=[-29.98]
(E,2,0,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 0, 0, 25)
0.6400000000000001 ( 2 2 0 0 25 ) 0.0 0.0
0.16000000000000003 ( 2 2 0 1 25 ) 0.0 0.0
0.16000000000000003 ( 2 2 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 2 0 1 0 ) 0.0 50.0
(E,2,0,D,25):HIT=[-0.01]
Deciding optimal action for state: (2, 2, 0, 0, 50)
0.6400000000000001 ( 2 2 0 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 2 0 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 2 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 2 0 1 0 ) 0.0 50.0
(E,2,0,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 2, 0, 0, 75)
0.8 ( 4 2 0 0 75 ) 0.0 0.0
0.2 ( 4 2 0 1 75 ) 0.0 0.0
0.0 ( 2 2 0 0 75 ) 0.0 0.0
0.0 ( 2 2 0 1 75 ) 0.0 0.0
(E,2,0,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 2, 0, 0, 100)
0.8 ( 4 2 0 0 100 ) 0.0 0.0
0.2 ( 4 2 0 1 100 ) 0.0 0.0
0.0 ( 2 2 0 0 100 ) 0.0 0.0
0.0 ( 2 2 0 1 100 ) 0.0 0.0
(E,2,0,D,100):LEFT=[-10.0]
(E,2,0,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 0, 1, 25)
0.4 ( 2 2 0 1 25 ) 0.0 0.0
0.1 ( 2 2 0 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 50 ) 0.0 0.0
(E,2,0,R,25):HIT=[-24.985]
Deciding optimal action for state: (2, 2, 0, 1, 50)
0.4 ( 2 2 0 1 50 ) 0.0 0.0
0.1 ( 2 2 0 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 75 ) 0.0 0.0
(E,2,0,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 2, 0, 1, 75)
0.5 ( 4 2 0 1 75 ) 0.0 0.0
0.0 ( 2 2 0 1 75 ) 0.0 0.0
0.5 ( 2 2 0 0 100 ) 0.0 0.0
(E,2,0,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 2, 0, 1, 100)
0.5 ( 4 2 0 1 100 ) 0.0 0.0
0.0 ( 2 2 0 1 100 ) 0.0 0.0
0.5 ( 2 2 0 0 100 ) 0.0 0.0
(E,2,0,R,100):LEFT=[-29.98]
(E,2,1,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 1, 0, 25)
0.08000000000000002 ( 2 2 0 0 25 ) 0.0 0.0
0.020000000000000004 ( 2 2 0 1 25 ) 0.0 0.0
0.7200000000000001 ( 2 2 0 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 2 0 1 0 ) 0.0 50.0
(E,2,1,D,25):SHOOT=[34.955]
Deciding optimal action for state: (2, 2, 1, 0, 50)
0.6400000000000001 ( 2 2 1 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 2 1 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 2 1 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 2 1 1 0 ) 0.0 50.0
(E,2,1,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 2, 1, 0, 75)
0.8 ( 4 2 1 0 75 ) 0.0 0.0
0.2 ( 4 2 1 1 75 ) 0.0 0.0
0.0 ( 2 2 1 0 75 ) 0.0 0.0
0.0 ( 2 2 1 1 75 ) 0.0 0.0
(E,2,1,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 2, 1, 0, 100)
0.8 ( 4 2 1 0 100 ) 0.0 0.0
0.2 ( 4 2 1 1 100 ) 0.0 0.0
0.0 ( 2 2 1 0 100 ) 0.0 0.0
0.0 ( 2 2 1 1 100 ) 0.0 0.0
(E,2,1,D,100):LEFT=[-10.0]
(E,2,1,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 1, 1, 25)
0.05 ( 2 2 0 1 25 ) 0.0 0.0
0.45 ( 2 2 0 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 50 ) 0.0 0.0
(E,2,1,R,25):SHOOT=[-7.502]
Deciding optimal action for state: (2, 2, 1, 1, 50)
0.4 ( 2 2 1 1 50 ) 0.0 0.0
0.1 ( 2 2 1 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 75 ) 0.0 0.0
(E,2,1,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 2, 1, 1, 75)
0.5 ( 4 2 1 1 75 ) 0.0 0.0
0.0 ( 2 2 1 1 75 ) 0.0 0.0
0.5 ( 2 2 0 0 100 ) 0.0 0.0
(E,2,1,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 2, 1, 1, 100)
0.5 ( 4 2 1 1 100 ) 0.0 0.0
0.0 ( 2 2 1 1 100 ) 0.0 0.0
0.5 ( 2 2 0 0 100 ) 0.0 0.0
(E,2,1,R,100):LEFT=[-29.98]
(E,2,2,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 2, 0, 25)
0.08000000000000002 ( 2 2 1 0 25 ) 0.0 0.0
0.020000000000000004 ( 2 2 1 1 25 ) 0.0 0.0
0.7200000000000001 ( 2 2 1 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 2 1 1 0 ) 0.0 50.0
(E,2,2,D,25):SHOOT=[34.955]
Deciding optimal action for state: (2, 2, 2, 0, 50)
0.6400000000000001 ( 2 2 2 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 2 2 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 2 2 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 2 2 1 0 ) 0.0 50.0
(E,2,2,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 2, 2, 0, 75)
0.8 ( 4 2 2 0 75 ) 0.0 0.0
0.2 ( 4 2 2 1 75 ) 0.0 0.0
0.0 ( 2 2 2 0 75 ) 0.0 0.0
0.0 ( 2 2 2 1 75 ) 0.0 0.0
(E,2,2,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 2, 2, 0, 100)
0.8 ( 4 2 2 0 100 ) 0.0 0.0
0.2 ( 4 2 2 1 100 ) 0.0 0.0
0.0 ( 2 2 2 0 100 ) 0.0 0.0
0.0 ( 2 2 2 1 100 ) 0.0 0.0
(E,2,2,D,100):LEFT=[-10.0]
(E,2,2,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 2, 1, 25)
0.05 ( 2 2 1 1 25 ) 0.0 0.0
0.45 ( 2 2 1 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 50 ) 0.0 0.0
(E,2,2,R,25):SHOOT=[-7.502]
Deciding optimal action for state: (2, 2, 2, 1, 50)
0.4 ( 2 2 2 1 50 ) 0.0 0.0
0.1 ( 2 2 2 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 75 ) 0.0 0.0
(E,2,2,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 2, 2, 1, 75)
0.5 ( 4 2 2 1 75 ) 0.0 0.0
0.0 ( 2 2 2 1 75 ) 0.0 0.0
0.5 ( 2 2 0 0 100 ) 0.0 0.0
(E,2,2,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 2, 2, 1, 100)
0.5 ( 4 2 2 1 100 ) 0.0 0.0
0.0 ( 2 2 2 1 100 ) 0.0 0.0
0.5 ( 2 2 0 0 100 ) 0.0 0.0
(E,2,2,R,100):LEFT=[-29.98]
(E,2,3,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 3, 0, 25)
0.08000000000000002 ( 2 2 2 0 25 ) 0.0 0.0
0.020000000000000004 ( 2 2 2 1 25 ) 0.0 0.0
0.7200000000000001 ( 2 2 2 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 2 2 1 0 ) 0.0 50.0
(E,2,3,D,25):SHOOT=[34.955]
Deciding optimal action for state: (2, 2, 3, 0, 50)
0.6400000000000001 ( 2 2 3 0 50 ) 0.0 0.0
0.16000000000000003 ( 2 2 3 1 50 ) 0.0 0.0
0.16000000000000003 ( 2 2 3 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 2 3 1 0 ) 0.0 50.0
(E,2,3,D,50):HIT=[-0.01]
Deciding optimal action for state: (2, 2, 3, 0, 75)
0.8 ( 4 2 3 0 75 ) 0.0 0.0
0.2 ( 4 2 3 1 75 ) 0.0 0.0
0.0 ( 2 2 3 0 75 ) 0.0 0.0
0.0 ( 2 2 3 1 75 ) 0.0 0.0
(E,2,3,D,75):LEFT=[-10.0]
Deciding optimal action for state: (2, 2, 3, 0, 100)
0.8 ( 4 2 3 0 100 ) 0.0 0.0
0.2 ( 4 2 3 1 100 ) 0.0 0.0
0.0 ( 2 2 3 0 100 ) 0.0 0.0
0.0 ( 2 2 3 1 100 ) 0.0 0.0
(E,2,3,D,100):LEFT=[-10.0]
(E,2,3,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 3, 1, 25)
0.05 ( 2 2 2 1 25 ) 0.0 0.0
0.45 ( 2 2 2 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 50 ) 0.0 0.0
(E,2,3,R,25):SHOOT=[-7.502]
Deciding optimal action for state: (2, 2, 3, 1, 50)
0.4 ( 2 2 3 1 50 ) 0.0 0.0
0.1 ( 2 2 3 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 75 ) 0.0 0.0
(E,2,3,R,50):HIT=[-24.985]
Deciding optimal action for state: (2, 2, 3, 1, 75)
0.5 ( 4 2 3 1 75 ) 0.0 0.0
0.0 ( 2 2 3 1 75 ) 0.0 0.0
0.5 ( 2 2 0 0 100 ) 0.0 0.0
(E,2,3,R,75):LEFT=[-29.98]
Deciding optimal action for state: (2, 2, 3, 1, 100)
0.5 ( 4 2 3 1 100 ) 0.0 0.0
0.0 ( 2 2 3 1 100 ) 0.0 0.0
0.5 ( 2 2 0 0 100 ) 0.0 0.0
(E,2,3,R,100):LEFT=[-29.98]
(S,0,0,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 0, 0, 25)
0.68 ( 4 0 0 0 25 ) 0.0 0.0
0.17 ( 4 0 0 1 25 ) 0.0 0.0
0.12 ( 2 0 0 0 25 ) 0.0 0.0
0.03 ( 2 0 0 1 25 ) 0.0 0.0
(S,0,0,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 0, 0, 0, 50)
0.68 ( 4 0 0 0 50 ) 0.0 0.0
0.17 ( 4 0 0 1 50 ) 0.0 0.0
0.12 ( 2 0 0 0 50 ) 0.0 0.0
0.03 ( 2 0 0 1 50 ) 0.0 0.0
(S,0,0,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 0, 0, 0, 75)
0.68 ( 4 0 0 0 75 ) 0.0 0.0
0.17 ( 4 0 0 1 75 ) 0.0 0.0
0.12 ( 2 0 0 0 75 ) 0.0 0.0
0.03 ( 2 0 0 1 75 ) 0.0 0.0
(S,0,0,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 0, 0, 0, 100)
0.68 ( 4 0 0 0 100 ) 0.0 0.0
0.17 ( 4 0 0 1 100 ) 0.0 0.0
0.12 ( 2 0 0 0 100 ) 0.0 0.0
0.03 ( 2 0 0 1 100 ) 0.0 0.0
(S,0,0,D,100):UP=[-10.0]
(S,0,0,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 0, 1, 25)
0.425 ( 4 0 0 1 25 ) 0.0 0.0
0.425 ( 4 0 0 0 25 ) 0.0 0.0
0.075 ( 2 0 0 1 25 ) 0.0 0.0
0.075 ( 2 0 0 0 25 ) 0.0 0.0
(S,0,0,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 0, 0, 1, 50)
0.425 ( 4 0 0 1 50 ) 0.0 0.0
0.425 ( 4 0 0 0 50 ) 0.0 0.0
0.075 ( 2 0 0 1 50 ) 0.0 0.0
0.075 ( 2 0 0 0 50 ) 0.0 0.0
(S,0,0,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 0, 0, 1, 75)
0.425 ( 4 0 0 1 75 ) 0.0 0.0
0.425 ( 4 0 0 0 75 ) 0.0 0.0
0.075 ( 2 0 0 1 75 ) 0.0 0.0
0.075 ( 2 0 0 0 75 ) 0.0 0.0
(S,0,0,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 0, 0, 1, 100)
0.425 ( 4 0 0 1 100 ) 0.0 0.0
0.425 ( 4 0 0 0 100 ) 0.0 0.0
0.075 ( 2 0 0 1 100 ) 0.0 0.0
0.075 ( 2 0 0 0 100 ) 0.0 0.0
(S,0,0,R,100):UP=[-10.0]
(S,0,1,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 1, 0, 25)
0.68 ( 4 0 1 0 25 ) 0.0 0.0
0.17 ( 4 0 1 1 25 ) 0.0 0.0
0.12 ( 2 0 1 0 25 ) 0.0 0.0
0.03 ( 2 0 1 1 25 ) 0.0 0.0
(S,0,1,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 0, 1, 0, 50)
0.68 ( 4 0 1 0 50 ) 0.0 0.0
0.17 ( 4 0 1 1 50 ) 0.0 0.0
0.12 ( 2 0 1 0 50 ) 0.0 0.0
0.03 ( 2 0 1 1 50 ) 0.0 0.0
(S,0,1,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 0, 1, 0, 75)
0.68 ( 4 0 1 0 75 ) 0.0 0.0
0.17 ( 4 0 1 1 75 ) 0.0 0.0
0.12 ( 2 0 1 0 75 ) 0.0 0.0
0.03 ( 2 0 1 1 75 ) 0.0 0.0
(S,0,1,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 0, 1, 0, 100)
0.68 ( 4 0 1 0 100 ) 0.0 0.0
0.17 ( 4 0 1 1 100 ) 0.0 0.0
0.12 ( 2 0 1 0 100 ) 0.0 0.0
0.03 ( 2 0 1 1 100 ) 0.0 0.0
(S,0,1,D,100):UP=[-10.0]
(S,0,1,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 1, 1, 25)
0.425 ( 4 0 1 1 25 ) 0.0 0.0
0.425 ( 4 0 1 0 25 ) 0.0 0.0
0.075 ( 2 0 1 1 25 ) 0.0 0.0
0.075 ( 2 0 1 0 25 ) 0.0 0.0
(S,0,1,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 0, 1, 1, 50)
0.425 ( 4 0 1 1 50 ) 0.0 0.0
0.425 ( 4 0 1 0 50 ) 0.0 0.0
0.075 ( 2 0 1 1 50 ) 0.0 0.0
0.075 ( 2 0 1 0 50 ) 0.0 0.0
(S,0,1,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 0, 1, 1, 75)
0.425 ( 4 0 1 1 75 ) 0.0 0.0
0.425 ( 4 0 1 0 75 ) 0.0 0.0
0.075 ( 2 0 1 1 75 ) 0.0 0.0
0.075 ( 2 0 1 0 75 ) 0.0 0.0
(S,0,1,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 0, 1, 1, 100)
0.425 ( 4 0 1 1 100 ) 0.0 0.0
0.425 ( 4 0 1 0 100 ) 0.0 0.0
0.075 ( 2 0 1 1 100 ) 0.0 0.0
0.075 ( 2 0 1 0 100 ) 0.0 0.0
(S,0,1,R,100):UP=[-10.0]
(S,0,2,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 2, 0, 25)
0.68 ( 4 0 2 0 25 ) 0.0 0.0
0.17 ( 4 0 2 1 25 ) 0.0 0.0
0.12 ( 2 0 2 0 25 ) 0.0 0.0
0.03 ( 2 0 2 1 25 ) 0.0 0.0
(S,0,2,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 0, 2, 0, 50)
0.68 ( 4 0 2 0 50 ) 0.0 0.0
0.17 ( 4 0 2 1 50 ) 0.0 0.0
0.12 ( 2 0 2 0 50 ) 0.0 0.0
0.03 ( 2 0 2 1 50 ) 0.0 0.0
(S,0,2,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 0, 2, 0, 75)
0.68 ( 4 0 2 0 75 ) 0.0 0.0
0.17 ( 4 0 2 1 75 ) 0.0 0.0
0.12 ( 2 0 2 0 75 ) 0.0 0.0
0.03 ( 2 0 2 1 75 ) 0.0 0.0
(S,0,2,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 0, 2, 0, 100)
0.68 ( 4 0 2 0 100 ) 0.0 0.0
0.17 ( 4 0 2 1 100 ) 0.0 0.0
0.12 ( 2 0 2 0 100 ) 0.0 0.0
0.03 ( 2 0 2 1 100 ) 0.0 0.0
(S,0,2,D,100):UP=[-10.0]
(S,0,2,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 2, 1, 25)
0.425 ( 4 0 2 1 25 ) 0.0 0.0
0.425 ( 4 0 2 0 25 ) 0.0 0.0
0.075 ( 2 0 2 1 25 ) 0.0 0.0
0.075 ( 2 0 2 0 25 ) 0.0 0.0
(S,0,2,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 0, 2, 1, 50)
0.425 ( 4 0 2 1 50 ) 0.0 0.0
0.425 ( 4 0 2 0 50 ) 0.0 0.0
0.075 ( 2 0 2 1 50 ) 0.0 0.0
0.075 ( 2 0 2 0 50 ) 0.0 0.0
(S,0,2,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 0, 2, 1, 75)
0.425 ( 4 0 2 1 75 ) 0.0 0.0
0.425 ( 4 0 2 0 75 ) 0.0 0.0
0.075 ( 2 0 2 1 75 ) 0.0 0.0
0.075 ( 2 0 2 0 75 ) 0.0 0.0
(S,0,2,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 0, 2, 1, 100)
0.425 ( 4 0 2 1 100 ) 0.0 0.0
0.425 ( 4 0 2 0 100 ) 0.0 0.0
0.075 ( 2 0 2 1 100 ) 0.0 0.0
0.075 ( 2 0 2 0 100 ) 0.0 0.0
(S,0,2,R,100):UP=[-10.0]
(S,0,3,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 3, 0, 25)
0.68 ( 4 0 3 0 25 ) 0.0 0.0
0.17 ( 4 0 3 1 25 ) 0.0 0.0
0.12 ( 2 0 3 0 25 ) 0.0 0.0
0.03 ( 2 0 3 1 25 ) 0.0 0.0
(S,0,3,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 0, 3, 0, 50)
0.68 ( 4 0 3 0 50 ) 0.0 0.0
0.17 ( 4 0 3 1 50 ) 0.0 0.0
0.12 ( 2 0 3 0 50 ) 0.0 0.0
0.03 ( 2 0 3 1 50 ) 0.0 0.0
(S,0,3,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 0, 3, 0, 75)
0.68 ( 4 0 3 0 75 ) 0.0 0.0
0.17 ( 4 0 3 1 75 ) 0.0 0.0
0.12 ( 2 0 3 0 75 ) 0.0 0.0
0.03 ( 2 0 3 1 75 ) 0.0 0.0
(S,0,3,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 0, 3, 0, 100)
0.68 ( 4 0 3 0 100 ) 0.0 0.0
0.17 ( 4 0 3 1 100 ) 0.0 0.0
0.12 ( 2 0 3 0 100 ) 0.0 0.0
0.03 ( 2 0 3 1 100 ) 0.0 0.0
(S,0,3,D,100):UP=[-10.0]
(S,0,3,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 3, 1, 25)
0.425 ( 4 0 3 1 25 ) 0.0 0.0
0.425 ( 4 0 3 0 25 ) 0.0 0.0
0.075 ( 2 0 3 1 25 ) 0.0 0.0
0.075 ( 2 0 3 0 25 ) 0.0 0.0
(S,0,3,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 0, 3, 1, 50)
0.425 ( 4 0 3 1 50 ) 0.0 0.0
0.425 ( 4 0 3 0 50 ) 0.0 0.0
0.075 ( 2 0 3 1 50 ) 0.0 0.0
0.075 ( 2 0 3 0 50 ) 0.0 0.0
(S,0,3,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 0, 3, 1, 75)
0.425 ( 4 0 3 1 75 ) 0.0 0.0
0.425 ( 4 0 3 0 75 ) 0.0 0.0
0.075 ( 2 0 3 1 75 ) 0.0 0.0
0.075 ( 2 0 3 0 75 ) 0.0 0.0
(S,0,3,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 0, 3, 1, 100)
0.425 ( 4 0 3 1 100 ) 0.0 0.0
0.425 ( 4 0 3 0 100 ) 0.0 0.0
0.075 ( 2 0 3 1 100 ) 0.0 0.0
0.075 ( 2 0 3 0 100 ) 0.0 0.0
(S,0,3,R,100):UP=[-10.0]
(S,1,0,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 0, 0, 25)
0.68 ( 4 1 0 0 25 ) 0.0 0.0
0.17 ( 4 1 0 1 25 ) 0.0 0.0
0.12 ( 2 1 0 0 25 ) 0.0 0.0
0.03 ( 2 1 0 1 25 ) 0.0 0.0
(S,1,0,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 1, 0, 0, 50)
0.68 ( 4 1 0 0 50 ) 0.0 0.0
0.17 ( 4 1 0 1 50 ) 0.0 0.0
0.12 ( 2 1 0 0 50 ) 0.0 0.0
0.03 ( 2 1 0 1 50 ) 0.0 0.0
(S,1,0,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 1, 0, 0, 75)
0.68 ( 4 1 0 0 75 ) 0.0 0.0
0.17 ( 4 1 0 1 75 ) 0.0 0.0
0.12 ( 2 1 0 0 75 ) 0.0 0.0
0.03 ( 2 1 0 1 75 ) 0.0 0.0
(S,1,0,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 1, 0, 0, 100)
0.68 ( 4 1 0 0 100 ) 0.0 0.0
0.17 ( 4 1 0 1 100 ) 0.0 0.0
0.12 ( 2 1 0 0 100 ) 0.0 0.0
0.03 ( 2 1 0 1 100 ) 0.0 0.0
(S,1,0,D,100):UP=[-10.0]
(S,1,0,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 0, 1, 25)
0.425 ( 4 1 0 1 25 ) 0.0 0.0
0.425 ( 4 1 0 0 25 ) 0.0 0.0
0.075 ( 2 1 0 1 25 ) 0.0 0.0
0.075 ( 2 1 0 0 25 ) 0.0 0.0
(S,1,0,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 1, 0, 1, 50)
0.425 ( 4 1 0 1 50 ) 0.0 0.0
0.425 ( 4 1 0 0 50 ) 0.0 0.0
0.075 ( 2 1 0 1 50 ) 0.0 0.0
0.075 ( 2 1 0 0 50 ) 0.0 0.0
(S,1,0,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 1, 0, 1, 75)
0.425 ( 4 1 0 1 75 ) 0.0 0.0
0.425 ( 4 1 0 0 75 ) 0.0 0.0
0.075 ( 2 1 0 1 75 ) 0.0 0.0
0.075 ( 2 1 0 0 75 ) 0.0 0.0
(S,1,0,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 1, 0, 1, 100)
0.425 ( 4 1 0 1 100 ) 0.0 0.0
0.425 ( 4 1 0 0 100 ) 0.0 0.0
0.075 ( 2 1 0 1 100 ) 0.0 0.0
0.075 ( 2 1 0 0 100 ) 0.0 0.0
(S,1,0,R,100):UP=[-10.0]
(S,1,1,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 1, 0, 25)
0.68 ( 4 1 1 0 25 ) 0.0 0.0
0.17 ( 4 1 1 1 25 ) 0.0 0.0
0.12 ( 2 1 1 0 25 ) 0.0 0.0
0.03 ( 2 1 1 1 25 ) 0.0 0.0
(S,1,1,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 1, 1, 0, 50)
0.68 ( 4 1 1 0 50 ) 0.0 0.0
0.17 ( 4 1 1 1 50 ) 0.0 0.0
0.12 ( 2 1 1 0 50 ) 0.0 0.0
0.03 ( 2 1 1 1 50 ) 0.0 0.0
(S,1,1,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 1, 1, 0, 75)
0.68 ( 4 1 1 0 75 ) 0.0 0.0
0.17 ( 4 1 1 1 75 ) 0.0 0.0
0.12 ( 2 1 1 0 75 ) 0.0 0.0
0.03 ( 2 1 1 1 75 ) 0.0 0.0
(S,1,1,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 1, 1, 0, 100)
0.68 ( 4 1 1 0 100 ) 0.0 0.0
0.17 ( 4 1 1 1 100 ) 0.0 0.0
0.12 ( 2 1 1 0 100 ) 0.0 0.0
0.03 ( 2 1 1 1 100 ) 0.0 0.0
(S,1,1,D,100):UP=[-10.0]
(S,1,1,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 1, 1, 25)
0.425 ( 4 1 1 1 25 ) 0.0 0.0
0.425 ( 4 1 1 0 25 ) 0.0 0.0
0.075 ( 2 1 1 1 25 ) 0.0 0.0
0.075 ( 2 1 1 0 25 ) 0.0 0.0
(S,1,1,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 1, 1, 1, 50)
0.425 ( 4 1 1 1 50 ) 0.0 0.0
0.425 ( 4 1 1 0 50 ) 0.0 0.0
0.075 ( 2 1 1 1 50 ) 0.0 0.0
0.075 ( 2 1 1 0 50 ) 0.0 0.0
(S,1,1,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 1, 1, 1, 75)
0.425 ( 4 1 1 1 75 ) 0.0 0.0
0.425 ( 4 1 1 0 75 ) 0.0 0.0
0.075 ( 2 1 1 1 75 ) 0.0 0.0
0.075 ( 2 1 1 0 75 ) 0.0 0.0
(S,1,1,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 1, 1, 1, 100)
0.425 ( 4 1 1 1 100 ) 0.0 0.0
0.425 ( 4 1 1 0 100 ) 0.0 0.0
0.075 ( 2 1 1 1 100 ) 0.0 0.0
0.075 ( 2 1 1 0 100 ) 0.0 0.0
(S,1,1,R,100):UP=[-10.0]
(S,1,2,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 2, 0, 25)
0.68 ( 4 1 2 0 25 ) 0.0 0.0
0.17 ( 4 1 2 1 25 ) 0.0 0.0
0.12 ( 2 1 2 0 25 ) 0.0 0.0
0.03 ( 2 1 2 1 25 ) 0.0 0.0
(S,1,2,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 1, 2, 0, 50)
0.68 ( 4 1 2 0 50 ) 0.0 0.0
0.17 ( 4 1 2 1 50 ) 0.0 0.0
0.12 ( 2 1 2 0 50 ) 0.0 0.0
0.03 ( 2 1 2 1 50 ) 0.0 0.0
(S,1,2,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 1, 2, 0, 75)
0.68 ( 4 1 2 0 75 ) 0.0 0.0
0.17 ( 4 1 2 1 75 ) 0.0 0.0
0.12 ( 2 1 2 0 75 ) 0.0 0.0
0.03 ( 2 1 2 1 75 ) 0.0 0.0
(S,1,2,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 1, 2, 0, 100)
0.68 ( 4 1 2 0 100 ) 0.0 0.0
0.17 ( 4 1 2 1 100 ) 0.0 0.0
0.12 ( 2 1 2 0 100 ) 0.0 0.0
0.03 ( 2 1 2 1 100 ) 0.0 0.0
(S,1,2,D,100):UP=[-10.0]
(S,1,2,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 2, 1, 25)
0.425 ( 4 1 2 1 25 ) 0.0 0.0
0.425 ( 4 1 2 0 25 ) 0.0 0.0
0.075 ( 2 1 2 1 25 ) 0.0 0.0
0.075 ( 2 1 2 0 25 ) 0.0 0.0
(S,1,2,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 1, 2, 1, 50)
0.425 ( 4 1 2 1 50 ) 0.0 0.0
0.425 ( 4 1 2 0 50 ) 0.0 0.0
0.075 ( 2 1 2 1 50 ) 0.0 0.0
0.075 ( 2 1 2 0 50 ) 0.0 0.0
(S,1,2,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 1, 2, 1, 75)
0.425 ( 4 1 2 1 75 ) 0.0 0.0
0.425 ( 4 1 2 0 75 ) 0.0 0.0
0.075 ( 2 1 2 1 75 ) 0.0 0.0
0.075 ( 2 1 2 0 75 ) 0.0 0.0
(S,1,2,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 1, 2, 1, 100)
0.425 ( 4 1 2 1 100 ) 0.0 0.0
0.425 ( 4 1 2 0 100 ) 0.0 0.0
0.075 ( 2 1 2 1 100 ) 0.0 0.0
0.075 ( 2 1 2 0 100 ) 0.0 0.0
(S,1,2,R,100):UP=[-10.0]
(S,1,3,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 3, 0, 25)
0.68 ( 4 1 3 0 25 ) 0.0 0.0
0.17 ( 4 1 3 1 25 ) 0.0 0.0
0.12 ( 2 1 3 0 25 ) 0.0 0.0
0.03 ( 2 1 3 1 25 ) 0.0 0.0
(S,1,3,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 1, 3, 0, 50)
0.68 ( 4 1 3 0 50 ) 0.0 0.0
0.17 ( 4 1 3 1 50 ) 0.0 0.0
0.12 ( 2 1 3 0 50 ) 0.0 0.0
0.03 ( 2 1 3 1 50 ) 0.0 0.0
(S,1,3,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 1, 3, 0, 75)
0.68 ( 4 1 3 0 75 ) 0.0 0.0
0.17 ( 4 1 3 1 75 ) 0.0 0.0
0.12 ( 2 1 3 0 75 ) 0.0 0.0
0.03 ( 2 1 3 1 75 ) 0.0 0.0
(S,1,3,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 1, 3, 0, 100)
0.68 ( 4 1 3 0 100 ) 0.0 0.0
0.17 ( 4 1 3 1 100 ) 0.0 0.0
0.12 ( 2 1 3 0 100 ) 0.0 0.0
0.03 ( 2 1 3 1 100 ) 0.0 0.0
(S,1,3,D,100):UP=[-10.0]
(S,1,3,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 3, 1, 25)
0.425 ( 4 1 3 1 25 ) 0.0 0.0
0.425 ( 4 1 3 0 25 ) 0.0 0.0
0.075 ( 2 1 3 1 25 ) 0.0 0.0
0.075 ( 2 1 3 0 25 ) 0.0 0.0
(S,1,3,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 1, 3, 1, 50)
0.425 ( 4 1 3 1 50 ) 0.0 0.0
0.425 ( 4 1 3 0 50 ) 0.0 0.0
0.075 ( 2 1 3 1 50 ) 0.0 0.0
0.075 ( 2 1 3 0 50 ) 0.0 0.0
(S,1,3,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 1, 3, 1, 75)
0.425 ( 4 1 3 1 75 ) 0.0 0.0
0.425 ( 4 1 3 0 75 ) 0.0 0.0
0.075 ( 2 1 3 1 75 ) 0.0 0.0
0.075 ( 2 1 3 0 75 ) 0.0 0.0
(S,1,3,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 1, 3, 1, 100)
0.425 ( 4 1 3 1 100 ) 0.0 0.0
0.425 ( 4 1 3 0 100 ) 0.0 0.0
0.075 ( 2 1 3 1 100 ) 0.0 0.0
0.075 ( 2 1 3 0 100 ) 0.0 0.0
(S,1,3,R,100):UP=[-10.0]
(S,2,0,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 0, 0, 25)
0.68 ( 4 2 0 0 25 ) 0.0 0.0
0.17 ( 4 2 0 1 25 ) 0.0 0.0
0.12 ( 2 2 0 0 25 ) 0.0 0.0
0.03 ( 2 2 0 1 25 ) 0.0 0.0
(S,2,0,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 2, 0, 0, 50)
0.68 ( 4 2 0 0 50 ) 0.0 0.0
0.17 ( 4 2 0 1 50 ) 0.0 0.0
0.12 ( 2 2 0 0 50 ) 0.0 0.0
0.03 ( 2 2 0 1 50 ) 0.0 0.0
(S,2,0,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 2, 0, 0, 75)
0.68 ( 4 2 0 0 75 ) 0.0 0.0
0.17 ( 4 2 0 1 75 ) 0.0 0.0
0.12 ( 2 2 0 0 75 ) 0.0 0.0
0.03 ( 2 2 0 1 75 ) 0.0 0.0
(S,2,0,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 2, 0, 0, 100)
0.68 ( 4 2 0 0 100 ) 0.0 0.0
0.17 ( 4 2 0 1 100 ) 0.0 0.0
0.12 ( 2 2 0 0 100 ) 0.0 0.0
0.03 ( 2 2 0 1 100 ) 0.0 0.0
(S,2,0,D,100):UP=[-10.0]
(S,2,0,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 0, 1, 25)
0.425 ( 4 2 0 1 25 ) 0.0 0.0
0.425 ( 4 2 0 0 25 ) 0.0 0.0
0.075 ( 2 2 0 1 25 ) 0.0 0.0
0.075 ( 2 2 0 0 25 ) 0.0 0.0
(S,2,0,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 2, 0, 1, 50)
0.425 ( 4 2 0 1 50 ) 0.0 0.0
0.425 ( 4 2 0 0 50 ) 0.0 0.0
0.075 ( 2 2 0 1 50 ) 0.0 0.0
0.075 ( 2 2 0 0 50 ) 0.0 0.0
(S,2,0,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 2, 0, 1, 75)
0.425 ( 4 2 0 1 75 ) 0.0 0.0
0.425 ( 4 2 0 0 75 ) 0.0 0.0
0.075 ( 2 2 0 1 75 ) 0.0 0.0
0.075 ( 2 2 0 0 75 ) 0.0 0.0
(S,2,0,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 2, 0, 1, 100)
0.425 ( 4 2 0 1 100 ) 0.0 0.0
0.425 ( 4 2 0 0 100 ) 0.0 0.0
0.075 ( 2 2 0 1 100 ) 0.0 0.0
0.075 ( 2 2 0 0 100 ) 0.0 0.0
(S,2,0,R,100):UP=[-10.0]
(S,2,1,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 1, 0, 25)
0.68 ( 4 2 1 0 25 ) 0.0 0.0
0.17 ( 4 2 1 1 25 ) 0.0 0.0
0.12 ( 2 2 1 0 25 ) 0.0 0.0
0.03 ( 2 2 1 1 25 ) 0.0 0.0
(S,2,1,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 2, 1, 0, 50)
0.68 ( 4 2 1 0 50 ) 0.0 0.0
0.17 ( 4 2 1 1 50 ) 0.0 0.0
0.12 ( 2 2 1 0 50 ) 0.0 0.0
0.03 ( 2 2 1 1 50 ) 0.0 0.0
(S,2,1,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 2, 1, 0, 75)
0.68 ( 4 2 1 0 75 ) 0.0 0.0
0.17 ( 4 2 1 1 75 ) 0.0 0.0
0.12 ( 2 2 1 0 75 ) 0.0 0.0
0.03 ( 2 2 1 1 75 ) 0.0 0.0
(S,2,1,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 2, 1, 0, 100)
0.68 ( 4 2 1 0 100 ) 0.0 0.0
0.17 ( 4 2 1 1 100 ) 0.0 0.0
0.12 ( 2 2 1 0 100 ) 0.0 0.0
0.03 ( 2 2 1 1 100 ) 0.0 0.0
(S,2,1,D,100):UP=[-10.0]
(S,2,1,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 1, 1, 25)
0.425 ( 4 2 1 1 25 ) 0.0 0.0
0.425 ( 4 2 1 0 25 ) 0.0 0.0
0.075 ( 2 2 1 1 25 ) 0.0 0.0
0.075 ( 2 2 1 0 25 ) 0.0 0.0
(S,2,1,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 2, 1, 1, 50)
0.425 ( 4 2 1 1 50 ) 0.0 0.0
0.425 ( 4 2 1 0 50 ) 0.0 0.0
0.075 ( 2 2 1 1 50 ) 0.0 0.0
0.075 ( 2 2 1 0 50 ) 0.0 0.0
(S,2,1,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 2, 1, 1, 75)
0.425 ( 4 2 1 1 75 ) 0.0 0.0
0.425 ( 4 2 1 0 75 ) 0.0 0.0
0.075 ( 2 2 1 1 75 ) 0.0 0.0
0.075 ( 2 2 1 0 75 ) 0.0 0.0
(S,2,1,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 2, 1, 1, 100)
0.425 ( 4 2 1 1 100 ) 0.0 0.0
0.425 ( 4 2 1 0 100 ) 0.0 0.0
0.075 ( 2 2 1 1 100 ) 0.0 0.0
0.075 ( 2 2 1 0 100 ) 0.0 0.0
(S,2,1,R,100):UP=[-10.0]
(S,2,2,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 2, 0, 25)
0.68 ( 4 2 2 0 25 ) 0.0 0.0
0.17 ( 4 2 2 1 25 ) 0.0 0.0
0.12 ( 2 2 2 0 25 ) 0.0 0.0
0.03 ( 2 2 2 1 25 ) 0.0 0.0
(S,2,2,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 2, 2, 0, 50)
0.68 ( 4 2 2 0 50 ) 0.0 0.0
0.17 ( 4 2 2 1 50 ) 0.0 0.0
0.12 ( 2 2 2 0 50 ) 0.0 0.0
0.03 ( 2 2 2 1 50 ) 0.0 0.0
(S,2,2,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 2, 2, 0, 75)
0.68 ( 4 2 2 0 75 ) 0.0 0.0
0.17 ( 4 2 2 1 75 ) 0.0 0.0
0.12 ( 2 2 2 0 75 ) 0.0 0.0
0.03 ( 2 2 2 1 75 ) 0.0 0.0
(S,2,2,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 2, 2, 0, 100)
0.68 ( 4 2 2 0 100 ) 0.0 0.0
0.17 ( 4 2 2 1 100 ) 0.0 0.0
0.12 ( 2 2 2 0 100 ) 0.0 0.0
0.03 ( 2 2 2 1 100 ) 0.0 0.0
(S,2,2,D,100):UP=[-10.0]
(S,2,2,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 2, 1, 25)
0.425 ( 4 2 2 1 25 ) 0.0 0.0
0.425 ( 4 2 2 0 25 ) 0.0 0.0
0.075 ( 2 2 2 1 25 ) 0.0 0.0
0.075 ( 2 2 2 0 25 ) 0.0 0.0
(S,2,2,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 2, 2, 1, 50)
0.425 ( 4 2 2 1 50 ) 0.0 0.0
0.425 ( 4 2 2 0 50 ) 0.0 0.0
0.075 ( 2 2 2 1 50 ) 0.0 0.0
0.075 ( 2 2 2 0 50 ) 0.0 0.0
(S,2,2,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 2, 2, 1, 75)
0.425 ( 4 2 2 1 75 ) 0.0 0.0
0.425 ( 4 2 2 0 75 ) 0.0 0.0
0.075 ( 2 2 2 1 75 ) 0.0 0.0
0.075 ( 2 2 2 0 75 ) 0.0 0.0
(S,2,2,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 2, 2, 1, 100)
0.425 ( 4 2 2 1 100 ) 0.0 0.0
0.425 ( 4 2 2 0 100 ) 0.0 0.0
0.075 ( 2 2 2 1 100 ) 0.0 0.0
0.075 ( 2 2 2 0 100 ) 0.0 0.0
(S,2,2,R,100):UP=[-10.0]
(S,2,3,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 3, 0, 25)
0.68 ( 4 2 3 0 25 ) 0.0 0.0
0.17 ( 4 2 3 1 25 ) 0.0 0.0
0.12 ( 2 2 3 0 25 ) 0.0 0.0
0.03 ( 2 2 3 1 25 ) 0.0 0.0
(S,2,3,D,25):UP=[-10.0]
Deciding optimal action for state: (3, 2, 3, 0, 50)
0.68 ( 4 2 3 0 50 ) 0.0 0.0
0.17 ( 4 2 3 1 50 ) 0.0 0.0
0.12 ( 2 2 3 0 50 ) 0.0 0.0
0.03 ( 2 2 3 1 50 ) 0.0 0.0
(S,2,3,D,50):UP=[-10.0]
Deciding optimal action for state: (3, 2, 3, 0, 75)
0.68 ( 4 2 3 0 75 ) 0.0 0.0
0.17 ( 4 2 3 1 75 ) 0.0 0.0
0.12 ( 2 2 3 0 75 ) 0.0 0.0
0.03 ( 2 2 3 1 75 ) 0.0 0.0
(S,2,3,D,75):UP=[-10.0]
Deciding optimal action for state: (3, 2, 3, 0, 100)
0.68 ( 4 2 3 0 100 ) 0.0 0.0
0.17 ( 4 2 3 1 100 ) 0.0 0.0
0.12 ( 2 2 3 0 100 ) 0.0 0.0
0.03 ( 2 2 3 1 100 ) 0.0 0.0
(S,2,3,D,100):UP=[-10.0]
(S,2,3,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 3, 1, 25)
0.425 ( 4 2 3 1 25 ) 0.0 0.0
0.425 ( 4 2 3 0 25 ) 0.0 0.0
0.075 ( 2 2 3 1 25 ) 0.0 0.0
0.075 ( 2 2 3 0 25 ) 0.0 0.0
(S,2,3,R,25):UP=[-10.0]
Deciding optimal action for state: (3, 2, 3, 1, 50)
0.425 ( 4 2 3 1 50 ) 0.0 0.0
0.425 ( 4 2 3 0 50 ) 0.0 0.0
0.075 ( 2 2 3 1 50 ) 0.0 0.0
0.075 ( 2 2 3 0 50 ) 0.0 0.0
(S,2,3,R,50):UP=[-10.0]
Deciding optimal action for state: (3, 2, 3, 1, 75)
0.425 ( 4 2 3 1 75 ) 0.0 0.0
0.425 ( 4 2 3 0 75 ) 0.0 0.0
0.075 ( 2 2 3 1 75 ) 0.0 0.0
0.075 ( 2 2 3 0 75 ) 0.0 0.0
(S,2,3,R,75):UP=[-10.0]
Deciding optimal action for state: (3, 2, 3, 1, 100)
0.425 ( 4 2 3 1 100 ) 0.0 0.0
0.425 ( 4 2 3 0 100 ) 0.0 0.0
0.075 ( 2 2 3 1 100 ) 0.0 0.0
0.075 ( 2 2 3 0 100 ) 0.0 0.0
(S,2,3,R,100):UP=[-10.0]
(C,0,0,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 0, 0, 25)
0.7200000000000001 ( 4 0 0 0 25 ) 0.0 0.0
0.18000000000000002 ( 4 0 0 1 25 ) 0.0 0.0
0.08000000000000002 ( 4 0 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 0 0 1 0 ) 0.0 50.0
(C,0,0,D,25):HIT=[-5.005]
Deciding optimal action for state: (4, 0, 0, 0, 50)
0.7200000000000001 ( 4 0 0 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 0 0 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 0 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 0 0 1 0 ) 0.0 50.0
(C,0,0,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 0, 0, 0, 75)
0.68 ( 1 0 0 0 75 ) 0.0 0.0
0.17 ( 1 0 0 1 75 ) 0.0 0.0
0.12 ( 2 0 0 0 75 ) 0.0 0.0
0.03 ( 2 0 0 1 75 ) 0.0 0.0
(C,0,0,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 0, 0, 0, 100)
0.68 ( 1 0 0 0 100 ) 0.0 0.0
0.17 ( 1 0 0 1 100 ) 0.0 0.0
0.12 ( 2 0 0 0 100 ) 0.0 0.0
0.03 ( 2 0 0 1 100 ) 0.0 0.0
(C,0,0,D,100):UP=[-10.0]
(C,0,0,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 0, 1, 25)
0.45 ( 4 0 0 1 25 ) 0.0 0.0
0.05 ( 4 0 0 1 0 ) 0.0 50.0
0.5 ( 4 0 0 0 50 ) 0.0 0.0
(C,0,0,R,25):HIT=[-27.482]
Deciding optimal action for state: (4, 0, 0, 1, 50)
0.45 ( 4 0 0 1 50 ) 0.0 0.0
0.05 ( 4 0 0 1 0 ) 0.0 50.0
0.5 ( 4 0 0 0 75 ) 0.0 0.0
(C,0,0,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 0, 0, 1, 75)
0.425 ( 1 0 0 1 75 ) 0.0 0.0
0.075 ( 2 0 0 1 75 ) 0.0 0.0
0.5 ( 4 0 0 0 100 ) 0.0 0.0
(C,0,0,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 0, 0, 1, 100)
0.425 ( 1 0 0 1 100 ) 0.0 0.0
0.075 ( 2 0 0 1 100 ) 0.0 0.0
0.5 ( 4 0 0 0 100 ) 0.0 0.0
(C,0,0,R,100):UP=[-29.98]
(C,0,1,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 1, 0, 25)
0.4 ( 4 0 0 0 25 ) 0.0 0.0
0.1 ( 4 0 0 1 25 ) 0.0 0.0
0.4 ( 4 0 0 0 0 ) 0.0 50.0
0.1 ( 4 0 0 1 0 ) 0.0 50.0
(C,0,1,D,25):SHOOT=[14.975]
Deciding optimal action for state: (4, 0, 1, 0, 50)
0.7200000000000001 ( 4 0 1 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 0 1 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 0 1 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 0 1 1 0 ) 0.0 50.0
(C,0,1,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 0, 1, 0, 75)
0.68 ( 1 0 1 0 75 ) 0.0 0.0
0.17 ( 1 0 1 1 75 ) 0.0 0.0
0.12 ( 2 0 1 0 75 ) 0.0 0.0
0.03 ( 2 0 1 1 75 ) 0.0 0.0
(C,0,1,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 0, 1, 0, 100)
0.68 ( 1 0 1 0 100 ) 0.0 0.0
0.17 ( 1 0 1 1 100 ) 0.0 0.0
0.12 ( 2 0 1 0 100 ) 0.0 0.0
0.03 ( 2 0 1 1 100 ) 0.0 0.0
(C,0,1,D,100):UP=[-10.0]
(C,0,1,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 1, 1, 25)
0.25 ( 4 0 0 1 25 ) 0.0 0.0
0.25 ( 4 0 0 1 0 ) 0.0 50.0
0.5 ( 4 0 0 0 50 ) 0.0 0.0
(C,0,1,R,25):SHOOT=[-17.492]
Deciding optimal action for state: (4, 0, 1, 1, 50)
0.45 ( 4 0 1 1 50 ) 0.0 0.0
0.05 ( 4 0 1 1 0 ) 0.0 50.0
0.5 ( 4 0 0 0 75 ) 0.0 0.0
(C,0,1,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 0, 1, 1, 75)
0.425 ( 1 0 1 1 75 ) 0.0 0.0
0.075 ( 2 0 1 1 75 ) 0.0 0.0
0.5 ( 4 0 0 0 100 ) 0.0 0.0
(C,0,1,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 0, 1, 1, 100)
0.425 ( 1 0 1 1 100 ) 0.0 0.0
0.075 ( 2 0 1 1 100 ) 0.0 0.0
0.5 ( 4 0 0 0 100 ) 0.0 0.0
(C,0,1,R,100):UP=[-29.98]
(C,0,2,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 2, 0, 25)
0.4 ( 4 0 1 0 25 ) 0.0 0.0
0.1 ( 4 0 1 1 25 ) 0.0 0.0
0.4 ( 4 0 1 0 0 ) 0.0 50.0
0.1 ( 4 0 1 1 0 ) 0.0 50.0
(C,0,2,D,25):SHOOT=[14.975]
Deciding optimal action for state: (4, 0, 2, 0, 50)
0.7200000000000001 ( 4 0 2 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 0 2 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 0 2 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 0 2 1 0 ) 0.0 50.0
(C,0,2,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 0, 2, 0, 75)
0.68 ( 1 0 2 0 75 ) 0.0 0.0
0.17 ( 1 0 2 1 75 ) 0.0 0.0
0.12 ( 2 0 2 0 75 ) 0.0 0.0
0.03 ( 2 0 2 1 75 ) 0.0 0.0
(C,0,2,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 0, 2, 0, 100)
0.68 ( 1 0 2 0 100 ) 0.0 0.0
0.17 ( 1 0 2 1 100 ) 0.0 0.0
0.12 ( 2 0 2 0 100 ) 0.0 0.0
0.03 ( 2 0 2 1 100 ) 0.0 0.0
(C,0,2,D,100):UP=[-10.0]
(C,0,2,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 2, 1, 25)
0.25 ( 4 0 1 1 25 ) 0.0 0.0
0.25 ( 4 0 1 1 0 ) 0.0 50.0
0.5 ( 4 0 0 0 50 ) 0.0 0.0
(C,0,2,R,25):SHOOT=[-17.492]
Deciding optimal action for state: (4, 0, 2, 1, 50)
0.45 ( 4 0 2 1 50 ) 0.0 0.0
0.05 ( 4 0 2 1 0 ) 0.0 50.0
0.5 ( 4 0 0 0 75 ) 0.0 0.0
(C,0,2,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 0, 2, 1, 75)
0.425 ( 1 0 2 1 75 ) 0.0 0.0
0.075 ( 2 0 2 1 75 ) 0.0 0.0
0.5 ( 4 0 0 0 100 ) 0.0 0.0
(C,0,2,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 0, 2, 1, 100)
0.425 ( 1 0 2 1 100 ) 0.0 0.0
0.075 ( 2 0 2 1 100 ) 0.0 0.0
0.5 ( 4 0 0 0 100 ) 0.0 0.0
(C,0,2,R,100):UP=[-29.98]
(C,0,3,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 3, 0, 25)
0.4 ( 4 0 2 0 25 ) 0.0 0.0
0.1 ( 4 0 2 1 25 ) 0.0 0.0
0.4 ( 4 0 2 0 0 ) 0.0 50.0
0.1 ( 4 0 2 1 0 ) 0.0 50.0
(C,0,3,D,25):SHOOT=[14.975]
Deciding optimal action for state: (4, 0, 3, 0, 50)
0.7200000000000001 ( 4 0 3 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 0 3 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 0 3 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 0 3 1 0 ) 0.0 50.0
(C,0,3,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 0, 3, 0, 75)
0.68 ( 1 0 3 0 75 ) 0.0 0.0
0.17 ( 1 0 3 1 75 ) 0.0 0.0
0.12 ( 2 0 3 0 75 ) 0.0 0.0
0.03 ( 2 0 3 1 75 ) 0.0 0.0
(C,0,3,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 0, 3, 0, 100)
0.68 ( 1 0 3 0 100 ) 0.0 0.0
0.17 ( 1 0 3 1 100 ) 0.0 0.0
0.12 ( 2 0 3 0 100 ) 0.0 0.0
0.03 ( 2 0 3 1 100 ) 0.0 0.0
(C,0,3,D,100):UP=[-10.0]
(C,0,3,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 3, 1, 25)
0.25 ( 4 0 2 1 25 ) 0.0 0.0
0.25 ( 4 0 2 1 0 ) 0.0 50.0
0.5 ( 4 0 0 0 50 ) 0.0 0.0
(C,0,3,R,25):SHOOT=[-17.492]
Deciding optimal action for state: (4, 0, 3, 1, 50)
0.45 ( 4 0 3 1 50 ) 0.0 0.0
0.05 ( 4 0 3 1 0 ) 0.0 50.0
0.5 ( 4 0 0 0 75 ) 0.0 0.0
(C,0,3,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 0, 3, 1, 75)
0.425 ( 1 0 3 1 75 ) 0.0 0.0
0.075 ( 2 0 3 1 75 ) 0.0 0.0
0.5 ( 4 0 0 0 100 ) 0.0 0.0
(C,0,3,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 0, 3, 1, 100)
0.425 ( 1 0 3 1 100 ) 0.0 0.0
0.075 ( 2 0 3 1 100 ) 0.0 0.0
0.5 ( 4 0 0 0 100 ) 0.0 0.0
(C,0,3,R,100):UP=[-29.98]
(C,1,0,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 0, 0, 25)
0.7200000000000001 ( 4 1 0 0 25 ) 0.0 0.0
0.18000000000000002 ( 4 1 0 1 25 ) 0.0 0.0
0.08000000000000002 ( 4 1 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 1 0 1 0 ) 0.0 50.0
(C,1,0,D,25):HIT=[-5.005]
Deciding optimal action for state: (4, 1, 0, 0, 50)
0.7200000000000001 ( 4 1 0 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 1 0 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 1 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 1 0 1 0 ) 0.0 50.0
(C,1,0,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 1, 0, 0, 75)
0.68 ( 1 1 0 0 75 ) 0.0 0.0
0.17 ( 1 1 0 1 75 ) 0.0 0.0
0.12 ( 2 1 0 0 75 ) 0.0 0.0
0.03 ( 2 1 0 1 75 ) 0.0 0.0
(C,1,0,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 1, 0, 0, 100)
0.68 ( 1 1 0 0 100 ) 0.0 0.0
0.17 ( 1 1 0 1 100 ) 0.0 0.0
0.12 ( 2 1 0 0 100 ) 0.0 0.0
0.03 ( 2 1 0 1 100 ) 0.0 0.0
(C,1,0,D,100):UP=[-10.0]
(C,1,0,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 0, 1, 25)
0.45 ( 4 1 0 1 25 ) 0.0 0.0
0.05 ( 4 1 0 1 0 ) 0.0 50.0
0.5 ( 4 1 0 0 50 ) 0.0 0.0
(C,1,0,R,25):HIT=[-27.482]
Deciding optimal action for state: (4, 1, 0, 1, 50)
0.45 ( 4 1 0 1 50 ) 0.0 0.0
0.05 ( 4 1 0 1 0 ) 0.0 50.0
0.5 ( 4 1 0 0 75 ) 0.0 0.0
(C,1,0,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 1, 0, 1, 75)
0.425 ( 1 1 0 1 75 ) 0.0 0.0
0.075 ( 2 1 0 1 75 ) 0.0 0.0
0.5 ( 4 1 0 0 100 ) 0.0 0.0
(C,1,0,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 1, 0, 1, 100)
0.425 ( 1 1 0 1 100 ) 0.0 0.0
0.075 ( 2 1 0 1 100 ) 0.0 0.0
0.5 ( 4 1 0 0 100 ) 0.0 0.0
(C,1,0,R,100):UP=[-29.98]
(C,1,1,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 1, 0, 25)
0.4 ( 4 1 0 0 25 ) 0.0 0.0
0.1 ( 4 1 0 1 25 ) 0.0 0.0
0.4 ( 4 1 0 0 0 ) 0.0 50.0
0.1 ( 4 1 0 1 0 ) 0.0 50.0
(C,1,1,D,25):SHOOT=[14.975]
Deciding optimal action for state: (4, 1, 1, 0, 50)
0.7200000000000001 ( 4 1 1 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 1 1 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 1 1 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 1 1 1 0 ) 0.0 50.0
(C,1,1,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 1, 1, 0, 75)
0.68 ( 1 1 1 0 75 ) 0.0 0.0
0.17 ( 1 1 1 1 75 ) 0.0 0.0
0.12 ( 2 1 1 0 75 ) 0.0 0.0
0.03 ( 2 1 1 1 75 ) 0.0 0.0
(C,1,1,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 1, 1, 0, 100)
0.68 ( 1 1 1 0 100 ) 0.0 0.0
0.17 ( 1 1 1 1 100 ) 0.0 0.0
0.12 ( 2 1 1 0 100 ) 0.0 0.0
0.03 ( 2 1 1 1 100 ) 0.0 0.0
(C,1,1,D,100):UP=[-10.0]
(C,1,1,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 1, 1, 25)
0.25 ( 4 1 0 1 25 ) 0.0 0.0
0.25 ( 4 1 0 1 0 ) 0.0 50.0
0.5 ( 4 1 0 0 50 ) 0.0 0.0
(C,1,1,R,25):SHOOT=[-17.492]
Deciding optimal action for state: (4, 1, 1, 1, 50)
0.45 ( 4 1 1 1 50 ) 0.0 0.0
0.05 ( 4 1 1 1 0 ) 0.0 50.0
0.5 ( 4 1 0 0 75 ) 0.0 0.0
(C,1,1,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 1, 1, 1, 75)
0.425 ( 1 1 1 1 75 ) 0.0 0.0
0.075 ( 2 1 1 1 75 ) 0.0 0.0
0.5 ( 4 1 0 0 100 ) 0.0 0.0
(C,1,1,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 1, 1, 1, 100)
0.425 ( 1 1 1 1 100 ) 0.0 0.0
0.075 ( 2 1 1 1 100 ) 0.0 0.0
0.5 ( 4 1 0 0 100 ) 0.0 0.0
(C,1,1,R,100):UP=[-29.98]
(C,1,2,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 2, 0, 25)
0.4 ( 4 1 1 0 25 ) 0.0 0.0
0.1 ( 4 1 1 1 25 ) 0.0 0.0
0.4 ( 4 1 1 0 0 ) 0.0 50.0
0.1 ( 4 1 1 1 0 ) 0.0 50.0
(C,1,2,D,25):SHOOT=[14.975]
Deciding optimal action for state: (4, 1, 2, 0, 50)
0.7200000000000001 ( 4 1 2 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 1 2 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 1 2 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 1 2 1 0 ) 0.0 50.0
(C,1,2,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 1, 2, 0, 75)
0.68 ( 1 1 2 0 75 ) 0.0 0.0
0.17 ( 1 1 2 1 75 ) 0.0 0.0
0.12 ( 2 1 2 0 75 ) 0.0 0.0
0.03 ( 2 1 2 1 75 ) 0.0 0.0
(C,1,2,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 1, 2, 0, 100)
0.68 ( 1 1 2 0 100 ) 0.0 0.0
0.17 ( 1 1 2 1 100 ) 0.0 0.0
0.12 ( 2 1 2 0 100 ) 0.0 0.0
0.03 ( 2 1 2 1 100 ) 0.0 0.0
(C,1,2,D,100):UP=[-10.0]
(C,1,2,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 2, 1, 25)
0.25 ( 4 1 1 1 25 ) 0.0 0.0
0.25 ( 4 1 1 1 0 ) 0.0 50.0
0.5 ( 4 1 0 0 50 ) 0.0 0.0
(C,1,2,R,25):SHOOT=[-17.492]
Deciding optimal action for state: (4, 1, 2, 1, 50)
0.45 ( 4 1 2 1 50 ) 0.0 0.0
0.05 ( 4 1 2 1 0 ) 0.0 50.0
0.5 ( 4 1 0 0 75 ) 0.0 0.0
(C,1,2,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 1, 2, 1, 75)
0.425 ( 1 1 2 1 75 ) 0.0 0.0
0.075 ( 2 1 2 1 75 ) 0.0 0.0
0.5 ( 4 1 0 0 100 ) 0.0 0.0
(C,1,2,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 1, 2, 1, 100)
0.425 ( 1 1 2 1 100 ) 0.0 0.0
0.075 ( 2 1 2 1 100 ) 0.0 0.0
0.5 ( 4 1 0 0 100 ) 0.0 0.0
(C,1,2,R,100):UP=[-29.98]
(C,1,3,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 3, 0, 25)
0.4 ( 4 1 2 0 25 ) 0.0 0.0
0.1 ( 4 1 2 1 25 ) 0.0 0.0
0.4 ( 4 1 2 0 0 ) 0.0 50.0
0.1 ( 4 1 2 1 0 ) 0.0 50.0
(C,1,3,D,25):SHOOT=[14.975]
Deciding optimal action for state: (4, 1, 3, 0, 50)
0.7200000000000001 ( 4 1 3 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 1 3 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 1 3 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 1 3 1 0 ) 0.0 50.0
(C,1,3,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 1, 3, 0, 75)
0.68 ( 1 1 3 0 75 ) 0.0 0.0
0.17 ( 1 1 3 1 75 ) 0.0 0.0
0.12 ( 2 1 3 0 75 ) 0.0 0.0
0.03 ( 2 1 3 1 75 ) 0.0 0.0
(C,1,3,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 1, 3, 0, 100)
0.68 ( 1 1 3 0 100 ) 0.0 0.0
0.17 ( 1 1 3 1 100 ) 0.0 0.0
0.12 ( 2 1 3 0 100 ) 0.0 0.0
0.03 ( 2 1 3 1 100 ) 0.0 0.0
(C,1,3,D,100):UP=[-10.0]
(C,1,3,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 3, 1, 25)
0.25 ( 4 1 2 1 25 ) 0.0 0.0
0.25 ( 4 1 2 1 0 ) 0.0 50.0
0.5 ( 4 1 0 0 50 ) 0.0 0.0
(C,1,3,R,25):SHOOT=[-17.492]
Deciding optimal action for state: (4, 1, 3, 1, 50)
0.45 ( 4 1 3 1 50 ) 0.0 0.0
0.05 ( 4 1 3 1 0 ) 0.0 50.0
0.5 ( 4 1 0 0 75 ) 0.0 0.0
(C,1,3,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 1, 3, 1, 75)
0.425 ( 1 1 3 1 75 ) 0.0 0.0
0.075 ( 2 1 3 1 75 ) 0.0 0.0
0.5 ( 4 1 0 0 100 ) 0.0 0.0
(C,1,3,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 1, 3, 1, 100)
0.425 ( 1 1 3 1 100 ) 0.0 0.0
0.075 ( 2 1 3 1 100 ) 0.0 0.0
0.5 ( 4 1 0 0 100 ) 0.0 0.0
(C,1,3,R,100):UP=[-29.98]
(C,2,0,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 0, 0, 25)
0.7200000000000001 ( 4 2 0 0 25 ) 0.0 0.0
0.18000000000000002 ( 4 2 0 1 25 ) 0.0 0.0
0.08000000000000002 ( 4 2 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 2 0 1 0 ) 0.0 50.0
(C,2,0,D,25):HIT=[-5.005]
Deciding optimal action for state: (4, 2, 0, 0, 50)
0.7200000000000001 ( 4 2 0 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 2 0 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 2 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 2 0 1 0 ) 0.0 50.0
(C,2,0,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 2, 0, 0, 75)
0.68 ( 1 2 0 0 75 ) 0.0 0.0
0.17 ( 1 2 0 1 75 ) 0.0 0.0
0.12 ( 2 2 0 0 75 ) 0.0 0.0
0.03 ( 2 2 0 1 75 ) 0.0 0.0
(C,2,0,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 2, 0, 0, 100)
0.68 ( 1 2 0 0 100 ) 0.0 0.0
0.17 ( 1 2 0 1 100 ) 0.0 0.0
0.12 ( 2 2 0 0 100 ) 0.0 0.0
0.03 ( 2 2 0 1 100 ) 0.0 0.0
(C,2,0,D,100):UP=[-10.0]
(C,2,0,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 0, 1, 25)
0.45 ( 4 2 0 1 25 ) 0.0 0.0
0.05 ( 4 2 0 1 0 ) 0.0 50.0
0.5 ( 4 2 0 0 50 ) 0.0 0.0
(C,2,0,R,25):HIT=[-27.482]
Deciding optimal action for state: (4, 2, 0, 1, 50)
0.45 ( 4 2 0 1 50 ) 0.0 0.0
0.05 ( 4 2 0 1 0 ) 0.0 50.0
0.5 ( 4 2 0 0 75 ) 0.0 0.0
(C,2,0,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 2, 0, 1, 75)
0.425 ( 1 2 0 1 75 ) 0.0 0.0
0.075 ( 2 2 0 1 75 ) 0.0 0.0
0.5 ( 4 2 0 0 100 ) 0.0 0.0
(C,2,0,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 2, 0, 1, 100)
0.425 ( 1 2 0 1 100 ) 0.0 0.0
0.075 ( 2 2 0 1 100 ) 0.0 0.0
0.5 ( 4 2 0 0 100 ) 0.0 0.0
(C,2,0,R,100):UP=[-29.98]
(C,2,1,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 1, 0, 25)
0.4 ( 4 2 0 0 25 ) 0.0 0.0
0.1 ( 4 2 0 1 25 ) 0.0 0.0
0.4 ( 4 2 0 0 0 ) 0.0 50.0
0.1 ( 4 2 0 1 0 ) 0.0 50.0
(C,2,1,D,25):SHOOT=[14.975]
Deciding optimal action for state: (4, 2, 1, 0, 50)
0.7200000000000001 ( 4 2 1 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 2 1 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 2 1 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 2 1 1 0 ) 0.0 50.0
(C,2,1,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 2, 1, 0, 75)
0.68 ( 1 2 1 0 75 ) 0.0 0.0
0.17 ( 1 2 1 1 75 ) 0.0 0.0
0.12 ( 2 2 1 0 75 ) 0.0 0.0
0.03 ( 2 2 1 1 75 ) 0.0 0.0
(C,2,1,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 2, 1, 0, 100)
0.68 ( 1 2 1 0 100 ) 0.0 0.0
0.17 ( 1 2 1 1 100 ) 0.0 0.0
0.12 ( 2 2 1 0 100 ) 0.0 0.0
0.03 ( 2 2 1 1 100 ) 0.0 0.0
(C,2,1,D,100):UP=[-10.0]
(C,2,1,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 1, 1, 25)
0.25 ( 4 2 0 1 25 ) 0.0 0.0
0.25 ( 4 2 0 1 0 ) 0.0 50.0
0.5 ( 4 2 0 0 50 ) 0.0 0.0
(C,2,1,R,25):SHOOT=[-17.492]
Deciding optimal action for state: (4, 2, 1, 1, 50)
0.45 ( 4 2 1 1 50 ) 0.0 0.0
0.05 ( 4 2 1 1 0 ) 0.0 50.0
0.5 ( 4 2 0 0 75 ) 0.0 0.0
(C,2,1,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 2, 1, 1, 75)
0.425 ( 1 2 1 1 75 ) 0.0 0.0
0.075 ( 2 2 1 1 75 ) 0.0 0.0
0.5 ( 4 2 0 0 100 ) 0.0 0.0
(C,2,1,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 2, 1, 1, 100)
0.425 ( 1 2 1 1 100 ) 0.0 0.0
0.075 ( 2 2 1 1 100 ) 0.0 0.0
0.5 ( 4 2 0 0 100 ) 0.0 0.0
(C,2,1,R,100):UP=[-29.98]
(C,2,2,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 2, 0, 25)
0.4 ( 4 2 1 0 25 ) 0.0 0.0
0.1 ( 4 2 1 1 25 ) 0.0 0.0
0.4 ( 4 2 1 0 0 ) 0.0 50.0
0.1 ( 4 2 1 1 0 ) 0.0 50.0
(C,2,2,D,25):SHOOT=[14.975]
Deciding optimal action for state: (4, 2, 2, 0, 50)
0.7200000000000001 ( 4 2 2 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 2 2 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 2 2 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 2 2 1 0 ) 0.0 50.0
(C,2,2,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 2, 2, 0, 75)
0.68 ( 1 2 2 0 75 ) 0.0 0.0
0.17 ( 1 2 2 1 75 ) 0.0 0.0
0.12 ( 2 2 2 0 75 ) 0.0 0.0
0.03 ( 2 2 2 1 75 ) 0.0 0.0
(C,2,2,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 2, 2, 0, 100)
0.68 ( 1 2 2 0 100 ) 0.0 0.0
0.17 ( 1 2 2 1 100 ) 0.0 0.0
0.12 ( 2 2 2 0 100 ) 0.0 0.0
0.03 ( 2 2 2 1 100 ) 0.0 0.0
(C,2,2,D,100):UP=[-10.0]
(C,2,2,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 2, 1, 25)
0.25 ( 4 2 1 1 25 ) 0.0 0.0
0.25 ( 4 2 1 1 0 ) 0.0 50.0
0.5 ( 4 2 0 0 50 ) 0.0 0.0
(C,2,2,R,25):SHOOT=[-17.492]
Deciding optimal action for state: (4, 2, 2, 1, 50)
0.45 ( 4 2 2 1 50 ) 0.0 0.0
0.05 ( 4 2 2 1 0 ) 0.0 50.0
0.5 ( 4 2 0 0 75 ) 0.0 0.0
(C,2,2,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 2, 2, 1, 75)
0.425 ( 1 2 2 1 75 ) 0.0 0.0
0.075 ( 2 2 2 1 75 ) 0.0 0.0
0.5 ( 4 2 0 0 100 ) 0.0 0.0
(C,2,2,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 2, 2, 1, 100)
0.425 ( 1 2 2 1 100 ) 0.0 0.0
0.075 ( 2 2 2 1 100 ) 0.0 0.0
0.5 ( 4 2 0 0 100 ) 0.0 0.0
(C,2,2,R,100):UP=[-29.98]
(C,2,3,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 3, 0, 25)
0.4 ( 4 2 2 0 25 ) 0.0 0.0
0.1 ( 4 2 2 1 25 ) 0.0 0.0
0.4 ( 4 2 2 0 0 ) 0.0 50.0
0.1 ( 4 2 2 1 0 ) 0.0 50.0
(C,2,3,D,25):SHOOT=[14.975]
Deciding optimal action for state: (4, 2, 3, 0, 50)
0.7200000000000001 ( 4 2 3 0 50 ) 0.0 0.0
0.18000000000000002 ( 4 2 3 1 50 ) 0.0 0.0
0.08000000000000002 ( 4 2 3 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 2 3 1 0 ) 0.0 50.0
(C,2,3,D,50):HIT=[-5.005]
Deciding optimal action for state: (4, 2, 3, 0, 75)
0.68 ( 1 2 3 0 75 ) 0.0 0.0
0.17 ( 1 2 3 1 75 ) 0.0 0.0
0.12 ( 2 2 3 0 75 ) 0.0 0.0
0.03 ( 2 2 3 1 75 ) 0.0 0.0
(C,2,3,D,75):UP=[-10.0]
Deciding optimal action for state: (4, 2, 3, 0, 100)
0.68 ( 1 2 3 0 100 ) 0.0 0.0
0.17 ( 1 2 3 1 100 ) 0.0 0.0
0.12 ( 2 2 3 0 100 ) 0.0 0.0
0.03 ( 2 2 3 1 100 ) 0.0 0.0
(C,2,3,D,100):UP=[-10.0]
(C,2,3,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 3, 1, 25)
0.25 ( 4 2 2 1 25 ) 0.0 0.0
0.25 ( 4 2 2 1 0 ) 0.0 50.0
0.5 ( 4 2 0 0 50 ) 0.0 0.0
(C,2,3,R,25):SHOOT=[-17.492]
Deciding optimal action for state: (4, 2, 3, 1, 50)
0.45 ( 4 2 3 1 50 ) 0.0 0.0
0.05 ( 4 2 3 1 0 ) 0.0 50.0
0.5 ( 4 2 0 0 75 ) 0.0 0.0
(C,2,3,R,50):HIT=[-27.482]
Deciding optimal action for state: (4, 2, 3, 1, 75)
0.425 ( 1 2 3 1 75 ) 0.0 0.0
0.075 ( 2 2 3 1 75 ) 0.0 0.0
0.5 ( 4 2 0 0 100 ) 0.0 0.0
(C,2,3,R,75):UP=[-29.98]
Deciding optimal action for state: (4, 2, 3, 1, 100)
0.425 ( 1 2 3 1 100 ) 0.0 0.0
0.075 ( 2 2 3 1 100 ) 0.0 0.0
0.5 ( 4 2 0 0 100 ) 0.0 0.0
(C,2,3,R,100):UP=[-29.98]
iteration=1
(W,0,0,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 0, 0, 25)
(W,0,0,D,25):SHOOT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 0, 50)
(W,0,0,D,50):SHOOT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 0, 75)
(W,0,0,D,75):SHOOT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 0, 100)
(W,0,0,D,100):SHOOT=[-10.0]
(W,0,0,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 0, 1, 25)
(W,0,0,R,25):SHOOT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 1, 50)
(W,0,0,R,50):SHOOT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 1, 75)
(W,0,0,R,75):SHOOT=[-10.0]
Deciding optimal action for state: (0, 0, 0, 1, 100)
(W,0,0,R,100):SHOOT=[-10.0]
(W,0,1,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 1, 0, 25)
0.8 ( 4 0 1 0 25 ) 14.975000000000001 0.0
0.2 ( 4 0 1 1 25 ) -17.4925 0.0
0.0 ( 0 0 1 0 25 ) 2.4875000000000007 0.0
0.0 ( 0 0 1 1 25 ) 2.4875000000000007 0.0
(W,0,1,D,25):RIGHT=[-1.527]
Deciding optimal action for state: (0, 0, 1, 0, 50)
0.8 ( 4 0 1 0 50 ) -5.004999999999999 0.0
0.2 ( 4 0 1 1 50 ) -27.4825 0.0
0.0 ( 0 0 1 0 50 ) -10.0 0.0
0.0 ( 0 0 1 1 50 ) -10.0 0.0
(W,0,1,D,50):RIGHT=[-19.491]
Deciding optimal action for state: (0, 0, 1, 0, 75)
0.8 ( 4 0 1 0 75 ) -10.0 0.0
0.2 ( 4 0 1 1 75 ) -29.98 0.0
0.0 ( 0 0 1 0 75 ) -10.0 0.0
0.0 ( 0 0 1 1 75 ) -10.0 0.0
(W,0,1,D,75):RIGHT=[-23.982]
Deciding optimal action for state: (0, 0, 1, 0, 100)
0.8 ( 4 0 1 0 100 ) -10.0 0.0
0.2 ( 4 0 1 1 100 ) -29.98 0.0
0.0 ( 0 0 1 0 100 ) -10.0 0.0
0.0 ( 0 0 1 1 100 ) -10.0 0.0
(W,0,1,D,100):RIGHT=[-23.982]
(W,0,1,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 1, 1, 25)
0.375 ( 0 0 0 1 25 ) -10.0 0.0
0.375 ( 0 0 0 0 25 ) -10.0 0.0
0.125 ( 0 0 0 1 0 ) 0.0 50.0
0.125 ( 0 0 0 0 0 ) 0.0 50.0
(W,0,1,R,25):SHOOT=[-5.005]
Deciding optimal action for state: (0, 0, 1, 1, 50)
0.375 ( 0 0 0 1 50 ) -10.0 0.0
0.375 ( 0 0 0 0 50 ) -10.0 0.0
0.125 ( 0 0 0 1 25 ) -10.0 0.0
0.125 ( 0 0 0 0 25 ) -10.0 0.0
(W,0,1,R,50):SHOOT=[-19.99]
Deciding optimal action for state: (0, 0, 1, 1, 75)
0.375 ( 0 0 0 1 75 ) -10.0 0.0
0.375 ( 0 0 0 0 75 ) -10.0 0.0
0.125 ( 0 0 0 1 50 ) -10.0 0.0
0.125 ( 0 0 0 0 50 ) -10.0 0.0
(W,0,1,R,75):SHOOT=[-19.99]
Deciding optimal action for state: (0, 0, 1, 1, 100)
0.375 ( 0 0 0 1 100 ) -10.0 0.0
0.375 ( 0 0 0 0 100 ) -10.0 0.0
0.125 ( 0 0 0 1 75 ) -10.0 0.0
0.125 ( 0 0 0 0 75 ) -10.0 0.0
(W,0,1,R,100):SHOOT=[-19.99]
(W,0,2,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 2, 0, 25)
0.8 ( 4 0 2 0 25 ) 14.975000000000001 0.0
0.2 ( 4 0 2 1 25 ) -17.4925 0.0
0.0 ( 0 0 2 0 25 ) 2.4875000000000007 0.0
0.0 ( 0 0 2 1 25 ) 2.4875000000000007 0.0
(W,0,2,D,25):RIGHT=[-1.527]
Deciding optimal action for state: (0, 0, 2, 0, 50)
0.8 ( 4 0 2 0 50 ) -5.004999999999999 0.0
0.2 ( 4 0 2 1 50 ) -27.4825 0.0
0.0 ( 0 0 2 0 50 ) -10.0 0.0
0.0 ( 0 0 2 1 50 ) -10.0 0.0
(W,0,2,D,50):RIGHT=[-19.491]
Deciding optimal action for state: (0, 0, 2, 0, 75)
0.8 ( 4 0 2 0 75 ) -10.0 0.0
0.2 ( 4 0 2 1 75 ) -29.98 0.0
0.0 ( 0 0 2 0 75 ) -10.0 0.0
0.0 ( 0 0 2 1 75 ) -10.0 0.0
(W,0,2,D,75):RIGHT=[-23.982]
Deciding optimal action for state: (0, 0, 2, 0, 100)
0.8 ( 4 0 2 0 100 ) -10.0 0.0
0.2 ( 4 0 2 1 100 ) -29.98 0.0
0.0 ( 0 0 2 0 100 ) -10.0 0.0
0.0 ( 0 0 2 1 100 ) -10.0 0.0
(W,0,2,D,100):RIGHT=[-23.982]
(W,0,2,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 2, 1, 25)
0.375 ( 0 0 1 1 25 ) 2.4875000000000007 0.0
0.375 ( 0 0 1 0 25 ) 2.4875000000000007 0.0
0.125 ( 0 0 1 1 0 ) 0.0 50.0
0.125 ( 0 0 1 0 0 ) 0.0 50.0
(W,0,2,R,25):SHOOT=[4.351]
Deciding optimal action for state: (0, 0, 2, 1, 50)
0.375 ( 0 0 1 1 50 ) -10.0 0.0
0.375 ( 0 0 1 0 50 ) -10.0 0.0
0.125 ( 0 0 1 1 25 ) 2.4875000000000007 0.0
0.125 ( 0 0 1 0 25 ) 2.4875000000000007 0.0
(W,0,2,R,50):SHOOT=[-16.871]
Deciding optimal action for state: (0, 0, 2, 1, 75)
0.375 ( 0 0 1 1 75 ) -10.0 0.0
0.375 ( 0 0 1 0 75 ) -10.0 0.0
0.125 ( 0 0 1 1 50 ) -10.0 0.0
0.125 ( 0 0 1 0 50 ) -10.0 0.0
(W,0,2,R,75):SHOOT=[-19.99]
Deciding optimal action for state: (0, 0, 2, 1, 100)
0.375 ( 0 0 1 1 100 ) -10.0 0.0
0.375 ( 0 0 1 0 100 ) -10.0 0.0
0.125 ( 0 0 1 1 75 ) -10.0 0.0
0.125 ( 0 0 1 0 75 ) -10.0 0.0
(W,0,2,R,100):SHOOT=[-19.99]
(W,0,3,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 3, 0, 25)
0.8 ( 4 0 3 0 25 ) 14.975000000000001 0.0
0.2 ( 4 0 3 1 25 ) -17.4925 0.0
0.0 ( 0 0 3 0 25 ) 2.4875000000000007 0.0
0.0 ( 0 0 3 1 25 ) 2.4875000000000007 0.0
(W,0,3,D,25):RIGHT=[-1.527]
Deciding optimal action for state: (0, 0, 3, 0, 50)
0.8 ( 4 0 3 0 50 ) -5.004999999999999 0.0
0.2 ( 4 0 3 1 50 ) -27.4825 0.0
0.0 ( 0 0 3 0 50 ) -10.0 0.0
0.0 ( 0 0 3 1 50 ) -10.0 0.0
(W,0,3,D,50):RIGHT=[-19.491]
Deciding optimal action for state: (0, 0, 3, 0, 75)
0.8 ( 4 0 3 0 75 ) -10.0 0.0
0.2 ( 4 0 3 1 75 ) -29.98 0.0
0.0 ( 0 0 3 0 75 ) -10.0 0.0
0.0 ( 0 0 3 1 75 ) -10.0 0.0
(W,0,3,D,75):RIGHT=[-23.982]
Deciding optimal action for state: (0, 0, 3, 0, 100)
0.8 ( 4 0 3 0 100 ) -10.0 0.0
0.2 ( 4 0 3 1 100 ) -29.98 0.0
0.0 ( 0 0 3 0 100 ) -10.0 0.0
0.0 ( 0 0 3 1 100 ) -10.0 0.0
(W,0,3,D,100):RIGHT=[-23.982]
(W,0,3,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 0, 3, 1, 25)
0.375 ( 0 0 2 1 25 ) 2.4875000000000007 0.0
0.375 ( 0 0 2 0 25 ) 2.4875000000000007 0.0
0.125 ( 0 0 2 1 0 ) 0.0 50.0
0.125 ( 0 0 2 0 0 ) 0.0 50.0
(W,0,3,R,25):SHOOT=[4.351]
Deciding optimal action for state: (0, 0, 3, 1, 50)
0.375 ( 0 0 2 1 50 ) -10.0 0.0
0.375 ( 0 0 2 0 50 ) -10.0 0.0
0.125 ( 0 0 2 1 25 ) 2.4875000000000007 0.0
0.125 ( 0 0 2 0 25 ) 2.4875000000000007 0.0
(W,0,3,R,50):SHOOT=[-16.871]
Deciding optimal action for state: (0, 0, 3, 1, 75)
0.375 ( 0 0 2 1 75 ) -10.0 0.0
0.375 ( 0 0 2 0 75 ) -10.0 0.0
0.125 ( 0 0 2 1 50 ) -10.0 0.0
0.125 ( 0 0 2 0 50 ) -10.0 0.0
(W,0,3,R,75):SHOOT=[-19.99]
Deciding optimal action for state: (0, 0, 3, 1, 100)
0.375 ( 0 0 2 1 100 ) -10.0 0.0
0.375 ( 0 0 2 0 100 ) -10.0 0.0
0.125 ( 0 0 2 1 75 ) -10.0 0.0
0.125 ( 0 0 2 0 75 ) -10.0 0.0
(W,0,3,R,100):SHOOT=[-19.99]
(W,1,0,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 0, 0, 25)
(W,1,0,D,25):SHOOT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 0, 50)
(W,1,0,D,50):SHOOT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 0, 75)
(W,1,0,D,75):SHOOT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 0, 100)
(W,1,0,D,100):SHOOT=[-10.0]
(W,1,0,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 0, 1, 25)
(W,1,0,R,25):SHOOT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 1, 50)
(W,1,0,R,50):SHOOT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 1, 75)
(W,1,0,R,75):SHOOT=[-10.0]
Deciding optimal action for state: (0, 1, 0, 1, 100)
(W,1,0,R,100):SHOOT=[-10.0]
(W,1,1,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 1, 0, 25)
0.8 ( 4 1 1 0 25 ) 14.975000000000001 0.0
0.2 ( 4 1 1 1 25 ) -17.4925 0.0
0.0 ( 0 1 1 0 25 ) 2.4875000000000007 0.0
0.0 ( 0 1 1 1 25 ) 2.4875000000000007 0.0
(W,1,1,D,25):RIGHT=[-1.527]
Deciding optimal action for state: (0, 1, 1, 0, 50)
0.8 ( 4 1 1 0 50 ) -5.004999999999999 0.0
0.2 ( 4 1 1 1 50 ) -27.4825 0.0
0.0 ( 0 1 1 0 50 ) -10.0 0.0
0.0 ( 0 1 1 1 50 ) -10.0 0.0
(W,1,1,D,50):RIGHT=[-19.491]
Deciding optimal action for state: (0, 1, 1, 0, 75)
0.8 ( 4 1 1 0 75 ) -10.0 0.0
0.2 ( 4 1 1 1 75 ) -29.98 0.0
0.0 ( 0 1 1 0 75 ) -10.0 0.0
0.0 ( 0 1 1 1 75 ) -10.0 0.0
(W,1,1,D,75):RIGHT=[-23.982]
Deciding optimal action for state: (0, 1, 1, 0, 100)
0.8 ( 4 1 1 0 100 ) -10.0 0.0
0.2 ( 4 1 1 1 100 ) -29.98 0.0
0.0 ( 0 1 1 0 100 ) -10.0 0.0
0.0 ( 0 1 1 1 100 ) -10.0 0.0
(W,1,1,D,100):RIGHT=[-23.982]
(W,1,1,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 1, 1, 25)
0.375 ( 0 1 0 1 25 ) -10.0 0.0
0.375 ( 0 1 0 0 25 ) -10.0 0.0
0.125 ( 0 1 0 1 0 ) 0.0 50.0
0.125 ( 0 1 0 0 0 ) 0.0 50.0
(W,1,1,R,25):SHOOT=[-5.005]
Deciding optimal action for state: (0, 1, 1, 1, 50)
0.375 ( 0 1 0 1 50 ) -10.0 0.0
0.375 ( 0 1 0 0 50 ) -10.0 0.0
0.125 ( 0 1 0 1 25 ) -10.0 0.0
0.125 ( 0 1 0 0 25 ) -10.0 0.0
(W,1,1,R,50):SHOOT=[-19.99]
Deciding optimal action for state: (0, 1, 1, 1, 75)
0.375 ( 0 1 0 1 75 ) -10.0 0.0
0.375 ( 0 1 0 0 75 ) -10.0 0.0
0.125 ( 0 1 0 1 50 ) -10.0 0.0
0.125 ( 0 1 0 0 50 ) -10.0 0.0
(W,1,1,R,75):SHOOT=[-19.99]
Deciding optimal action for state: (0, 1, 1, 1, 100)
0.375 ( 0 1 0 1 100 ) -10.0 0.0
0.375 ( 0 1 0 0 100 ) -10.0 0.0
0.125 ( 0 1 0 1 75 ) -10.0 0.0
0.125 ( 0 1 0 0 75 ) -10.0 0.0
(W,1,1,R,100):SHOOT=[-19.99]
(W,1,2,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 2, 0, 25)
0.8 ( 4 1 2 0 25 ) 14.975000000000001 0.0
0.2 ( 4 1 2 1 25 ) -17.4925 0.0
0.0 ( 0 1 2 0 25 ) 2.4875000000000007 0.0
0.0 ( 0 1 2 1 25 ) 2.4875000000000007 0.0
(W,1,2,D,25):RIGHT=[-1.527]
Deciding optimal action for state: (0, 1, 2, 0, 50)
0.8 ( 4 1 2 0 50 ) -5.004999999999999 0.0
0.2 ( 4 1 2 1 50 ) -27.4825 0.0
0.0 ( 0 1 2 0 50 ) -10.0 0.0
0.0 ( 0 1 2 1 50 ) -10.0 0.0
(W,1,2,D,50):RIGHT=[-19.491]
Deciding optimal action for state: (0, 1, 2, 0, 75)
0.8 ( 4 1 2 0 75 ) -10.0 0.0
0.2 ( 4 1 2 1 75 ) -29.98 0.0
0.0 ( 0 1 2 0 75 ) -10.0 0.0
0.0 ( 0 1 2 1 75 ) -10.0 0.0
(W,1,2,D,75):RIGHT=[-23.982]
Deciding optimal action for state: (0, 1, 2, 0, 100)
0.8 ( 4 1 2 0 100 ) -10.0 0.0
0.2 ( 4 1 2 1 100 ) -29.98 0.0
0.0 ( 0 1 2 0 100 ) -10.0 0.0
0.0 ( 0 1 2 1 100 ) -10.0 0.0
(W,1,2,D,100):RIGHT=[-23.982]
(W,1,2,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 2, 1, 25)
0.375 ( 0 1 1 1 25 ) 2.4875000000000007 0.0
0.375 ( 0 1 1 0 25 ) 2.4875000000000007 0.0
0.125 ( 0 1 1 1 0 ) 0.0 50.0
0.125 ( 0 1 1 0 0 ) 0.0 50.0
(W,1,2,R,25):SHOOT=[4.351]
Deciding optimal action for state: (0, 1, 2, 1, 50)
0.375 ( 0 1 1 1 50 ) -10.0 0.0
0.375 ( 0 1 1 0 50 ) -10.0 0.0
0.125 ( 0 1 1 1 25 ) 2.4875000000000007 0.0
0.125 ( 0 1 1 0 25 ) 2.4875000000000007 0.0
(W,1,2,R,50):SHOOT=[-16.871]
Deciding optimal action for state: (0, 1, 2, 1, 75)
0.375 ( 0 1 1 1 75 ) -10.0 0.0
0.375 ( 0 1 1 0 75 ) -10.0 0.0
0.125 ( 0 1 1 1 50 ) -10.0 0.0
0.125 ( 0 1 1 0 50 ) -10.0 0.0
(W,1,2,R,75):SHOOT=[-19.99]
Deciding optimal action for state: (0, 1, 2, 1, 100)
0.375 ( 0 1 1 1 100 ) -10.0 0.0
0.375 ( 0 1 1 0 100 ) -10.0 0.0
0.125 ( 0 1 1 1 75 ) -10.0 0.0
0.125 ( 0 1 1 0 75 ) -10.0 0.0
(W,1,2,R,100):SHOOT=[-19.99]
(W,1,3,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 3, 0, 25)
0.8 ( 4 1 3 0 25 ) 14.975000000000001 0.0
0.2 ( 4 1 3 1 25 ) -17.4925 0.0
0.0 ( 0 1 3 0 25 ) 2.4875000000000007 0.0
0.0 ( 0 1 3 1 25 ) 2.4875000000000007 0.0
(W,1,3,D,25):RIGHT=[-1.527]
Deciding optimal action for state: (0, 1, 3, 0, 50)
0.8 ( 4 1 3 0 50 ) -5.004999999999999 0.0
0.2 ( 4 1 3 1 50 ) -27.4825 0.0
0.0 ( 0 1 3 0 50 ) -10.0 0.0
0.0 ( 0 1 3 1 50 ) -10.0 0.0
(W,1,3,D,50):RIGHT=[-19.491]
Deciding optimal action for state: (0, 1, 3, 0, 75)
0.8 ( 4 1 3 0 75 ) -10.0 0.0
0.2 ( 4 1 3 1 75 ) -29.98 0.0
0.0 ( 0 1 3 0 75 ) -10.0 0.0
0.0 ( 0 1 3 1 75 ) -10.0 0.0
(W,1,3,D,75):RIGHT=[-23.982]
Deciding optimal action for state: (0, 1, 3, 0, 100)
0.8 ( 4 1 3 0 100 ) -10.0 0.0
0.2 ( 4 1 3 1 100 ) -29.98 0.0
0.0 ( 0 1 3 0 100 ) -10.0 0.0
0.0 ( 0 1 3 1 100 ) -10.0 0.0
(W,1,3,D,100):RIGHT=[-23.982]
(W,1,3,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 1, 3, 1, 25)
0.375 ( 0 1 2 1 25 ) 2.4875000000000007 0.0
0.375 ( 0 1 2 0 25 ) 2.4875000000000007 0.0
0.125 ( 0 1 2 1 0 ) 0.0 50.0
0.125 ( 0 1 2 0 0 ) 0.0 50.0
(W,1,3,R,25):SHOOT=[4.351]
Deciding optimal action for state: (0, 1, 3, 1, 50)
0.375 ( 0 1 2 1 50 ) -10.0 0.0
0.375 ( 0 1 2 0 50 ) -10.0 0.0
0.125 ( 0 1 2 1 25 ) 2.4875000000000007 0.0
0.125 ( 0 1 2 0 25 ) 2.4875000000000007 0.0
(W,1,3,R,50):SHOOT=[-16.871]
Deciding optimal action for state: (0, 1, 3, 1, 75)
0.375 ( 0 1 2 1 75 ) -10.0 0.0
0.375 ( 0 1 2 0 75 ) -10.0 0.0
0.125 ( 0 1 2 1 50 ) -10.0 0.0
0.125 ( 0 1 2 0 50 ) -10.0 0.0
(W,1,3,R,75):SHOOT=[-19.99]
Deciding optimal action for state: (0, 1, 3, 1, 100)
0.375 ( 0 1 2 1 100 ) -10.0 0.0
0.375 ( 0 1 2 0 100 ) -10.0 0.0
0.125 ( 0 1 2 1 75 ) -10.0 0.0
0.125 ( 0 1 2 0 75 ) -10.0 0.0
(W,1,3,R,100):SHOOT=[-19.99]
(W,2,0,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 0, 0, 25)
(W,2,0,D,25):SHOOT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 0, 50)
(W,2,0,D,50):SHOOT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 0, 75)
(W,2,0,D,75):SHOOT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 0, 100)
(W,2,0,D,100):SHOOT=[-10.0]
(W,2,0,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 0, 1, 25)
(W,2,0,R,25):SHOOT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 1, 50)
(W,2,0,R,50):SHOOT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 1, 75)
(W,2,0,R,75):SHOOT=[-10.0]
Deciding optimal action for state: (0, 2, 0, 1, 100)
(W,2,0,R,100):SHOOT=[-10.0]
(W,2,1,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 1, 0, 25)
0.8 ( 4 2 1 0 25 ) 14.975000000000001 0.0
0.2 ( 4 2 1 1 25 ) -17.4925 0.0
0.0 ( 0 2 1 0 25 ) 2.4875000000000007 0.0
0.0 ( 0 2 1 1 25 ) 2.4875000000000007 0.0
(W,2,1,D,25):RIGHT=[-1.527]
Deciding optimal action for state: (0, 2, 1, 0, 50)
0.8 ( 4 2 1 0 50 ) -5.004999999999999 0.0
0.2 ( 4 2 1 1 50 ) -27.4825 0.0
0.0 ( 0 2 1 0 50 ) -10.0 0.0
0.0 ( 0 2 1 1 50 ) -10.0 0.0
(W,2,1,D,50):RIGHT=[-19.491]
Deciding optimal action for state: (0, 2, 1, 0, 75)
0.8 ( 4 2 1 0 75 ) -10.0 0.0
0.2 ( 4 2 1 1 75 ) -29.98 0.0
0.0 ( 0 2 1 0 75 ) -10.0 0.0
0.0 ( 0 2 1 1 75 ) -10.0 0.0
(W,2,1,D,75):RIGHT=[-23.982]
Deciding optimal action for state: (0, 2, 1, 0, 100)
0.8 ( 4 2 1 0 100 ) -10.0 0.0
0.2 ( 4 2 1 1 100 ) -29.98 0.0
0.0 ( 0 2 1 0 100 ) -10.0 0.0
0.0 ( 0 2 1 1 100 ) -10.0 0.0
(W,2,1,D,100):RIGHT=[-23.982]
(W,2,1,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 1, 1, 25)
0.375 ( 0 2 0 1 25 ) -10.0 0.0
0.375 ( 0 2 0 0 25 ) -10.0 0.0
0.125 ( 0 2 0 1 0 ) 0.0 50.0
0.125 ( 0 2 0 0 0 ) 0.0 50.0
(W,2,1,R,25):SHOOT=[-5.005]
Deciding optimal action for state: (0, 2, 1, 1, 50)
0.375 ( 0 2 0 1 50 ) -10.0 0.0
0.375 ( 0 2 0 0 50 ) -10.0 0.0
0.125 ( 0 2 0 1 25 ) -10.0 0.0
0.125 ( 0 2 0 0 25 ) -10.0 0.0
(W,2,1,R,50):SHOOT=[-19.99]
Deciding optimal action for state: (0, 2, 1, 1, 75)
0.375 ( 0 2 0 1 75 ) -10.0 0.0
0.375 ( 0 2 0 0 75 ) -10.0 0.0
0.125 ( 0 2 0 1 50 ) -10.0 0.0
0.125 ( 0 2 0 0 50 ) -10.0 0.0
(W,2,1,R,75):SHOOT=[-19.99]
Deciding optimal action for state: (0, 2, 1, 1, 100)
0.375 ( 0 2 0 1 100 ) -10.0 0.0
0.375 ( 0 2 0 0 100 ) -10.0 0.0
0.125 ( 0 2 0 1 75 ) -10.0 0.0
0.125 ( 0 2 0 0 75 ) -10.0 0.0
(W,2,1,R,100):SHOOT=[-19.99]
(W,2,2,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 2, 0, 25)
0.8 ( 4 2 2 0 25 ) 14.975000000000001 0.0
0.2 ( 4 2 2 1 25 ) -17.4925 0.0
0.0 ( 0 2 2 0 25 ) 2.4875000000000007 0.0
0.0 ( 0 2 2 1 25 ) 2.4875000000000007 0.0
(W,2,2,D,25):RIGHT=[-1.527]
Deciding optimal action for state: (0, 2, 2, 0, 50)
0.8 ( 4 2 2 0 50 ) -5.004999999999999 0.0
0.2 ( 4 2 2 1 50 ) -27.4825 0.0
0.0 ( 0 2 2 0 50 ) -10.0 0.0
0.0 ( 0 2 2 1 50 ) -10.0 0.0
(W,2,2,D,50):RIGHT=[-19.491]
Deciding optimal action for state: (0, 2, 2, 0, 75)
0.8 ( 4 2 2 0 75 ) -10.0 0.0
0.2 ( 4 2 2 1 75 ) -29.98 0.0
0.0 ( 0 2 2 0 75 ) -10.0 0.0
0.0 ( 0 2 2 1 75 ) -10.0 0.0
(W,2,2,D,75):RIGHT=[-23.982]
Deciding optimal action for state: (0, 2, 2, 0, 100)
0.8 ( 4 2 2 0 100 ) -10.0 0.0
0.2 ( 4 2 2 1 100 ) -29.98 0.0
0.0 ( 0 2 2 0 100 ) -10.0 0.0
0.0 ( 0 2 2 1 100 ) -10.0 0.0
(W,2,2,D,100):RIGHT=[-23.982]
(W,2,2,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 2, 1, 25)
0.375 ( 0 2 1 1 25 ) 2.4875000000000007 0.0
0.375 ( 0 2 1 0 25 ) 2.4875000000000007 0.0
0.125 ( 0 2 1 1 0 ) 0.0 50.0
0.125 ( 0 2 1 0 0 ) 0.0 50.0
(W,2,2,R,25):SHOOT=[4.351]
Deciding optimal action for state: (0, 2, 2, 1, 50)
0.375 ( 0 2 1 1 50 ) -10.0 0.0
0.375 ( 0 2 1 0 50 ) -10.0 0.0
0.125 ( 0 2 1 1 25 ) 2.4875000000000007 0.0
0.125 ( 0 2 1 0 25 ) 2.4875000000000007 0.0
(W,2,2,R,50):SHOOT=[-16.871]
Deciding optimal action for state: (0, 2, 2, 1, 75)
0.375 ( 0 2 1 1 75 ) -10.0 0.0
0.375 ( 0 2 1 0 75 ) -10.0 0.0
0.125 ( 0 2 1 1 50 ) -10.0 0.0
0.125 ( 0 2 1 0 50 ) -10.0 0.0
(W,2,2,R,75):SHOOT=[-19.99]
Deciding optimal action for state: (0, 2, 2, 1, 100)
0.375 ( 0 2 1 1 100 ) -10.0 0.0
0.375 ( 0 2 1 0 100 ) -10.0 0.0
0.125 ( 0 2 1 1 75 ) -10.0 0.0
0.125 ( 0 2 1 0 75 ) -10.0 0.0
(W,2,2,R,100):SHOOT=[-19.99]
(W,2,3,D,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 3, 0, 25)
0.8 ( 4 2 3 0 25 ) 14.975000000000001 0.0
0.2 ( 4 2 3 1 25 ) -17.4925 0.0
0.0 ( 0 2 3 0 25 ) 2.4875000000000007 0.0
0.0 ( 0 2 3 1 25 ) 2.4875000000000007 0.0
(W,2,3,D,25):RIGHT=[-1.527]
Deciding optimal action for state: (0, 2, 3, 0, 50)
0.8 ( 4 2 3 0 50 ) -5.004999999999999 0.0
0.2 ( 4 2 3 1 50 ) -27.4825 0.0
0.0 ( 0 2 3 0 50 ) -10.0 0.0
0.0 ( 0 2 3 1 50 ) -10.0 0.0
(W,2,3,D,50):RIGHT=[-19.491]
Deciding optimal action for state: (0, 2, 3, 0, 75)
0.8 ( 4 2 3 0 75 ) -10.0 0.0
0.2 ( 4 2 3 1 75 ) -29.98 0.0
0.0 ( 0 2 3 0 75 ) -10.0 0.0
0.0 ( 0 2 3 1 75 ) -10.0 0.0
(W,2,3,D,75):RIGHT=[-23.982]
Deciding optimal action for state: (0, 2, 3, 0, 100)
0.8 ( 4 2 3 0 100 ) -10.0 0.0
0.2 ( 4 2 3 1 100 ) -29.98 0.0
0.0 ( 0 2 3 0 100 ) -10.0 0.0
0.0 ( 0 2 3 1 100 ) -10.0 0.0
(W,2,3,D,100):RIGHT=[-23.982]
(W,2,3,R,0):NONE=[0.000]
Deciding optimal action for state: (0, 2, 3, 1, 25)
0.375 ( 0 2 2 1 25 ) 2.4875000000000007 0.0
0.375 ( 0 2 2 0 25 ) 2.4875000000000007 0.0
0.125 ( 0 2 2 1 0 ) 0.0 50.0
0.125 ( 0 2 2 0 0 ) 0.0 50.0
(W,2,3,R,25):SHOOT=[4.351]
Deciding optimal action for state: (0, 2, 3, 1, 50)
0.375 ( 0 2 2 1 50 ) -10.0 0.0
0.375 ( 0 2 2 0 50 ) -10.0 0.0
0.125 ( 0 2 2 1 25 ) 2.4875000000000007 0.0
0.125 ( 0 2 2 0 25 ) 2.4875000000000007 0.0
(W,2,3,R,50):SHOOT=[-16.871]
Deciding optimal action for state: (0, 2, 3, 1, 75)
0.375 ( 0 2 2 1 75 ) -10.0 0.0
0.375 ( 0 2 2 0 75 ) -10.0 0.0
0.125 ( 0 2 2 1 50 ) -10.0 0.0
0.125 ( 0 2 2 0 50 ) -10.0 0.0
(W,2,3,R,75):SHOOT=[-19.99]
Deciding optimal action for state: (0, 2, 3, 1, 100)
0.375 ( 0 2 2 1 100 ) -10.0 0.0
0.375 ( 0 2 2 0 100 ) -10.0 0.0
0.125 ( 0 2 2 1 75 ) -10.0 0.0
0.125 ( 0 2 2 0 75 ) -10.0 0.0
(W,2,3,R,100):SHOOT=[-19.99]
(N,0,0,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 0, 0, 25)
(N,0,0,D,25):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 0, 0, 50)
(N,0,0,D,50):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 0, 0, 75)
(N,0,0,D,75):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 0, 0, 100)
(N,0,0,D,100):CRAFT=[-10.0]
(N,0,0,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 0, 1, 25)
(N,0,0,R,25):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 0, 1, 50)
(N,0,0,R,50):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 0, 1, 75)
(N,0,0,R,75):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 0, 1, 100)
(N,0,0,R,100):CRAFT=[-10.0]
(N,0,1,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 1, 0, 25)
(N,0,1,D,25):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 1, 0, 50)
(N,0,1,D,50):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 1, 0, 75)
(N,0,1,D,75):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 1, 0, 100)
(N,0,1,D,100):CRAFT=[-10.0]
(N,0,1,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 1, 1, 25)
(N,0,1,R,25):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 1, 1, 50)
(N,0,1,R,50):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 1, 1, 75)
(N,0,1,R,75):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 1, 1, 100)
(N,0,1,R,100):CRAFT=[-10.0]
(N,0,2,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 2, 0, 25)
(N,0,2,D,25):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 2, 0, 50)
(N,0,2,D,50):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 2, 0, 75)
(N,0,2,D,75):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 2, 0, 100)
(N,0,2,D,100):CRAFT=[-10.0]
(N,0,2,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 2, 1, 25)
(N,0,2,R,25):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 2, 1, 50)
(N,0,2,R,50):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 2, 1, 75)
(N,0,2,R,75):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 2, 1, 100)
(N,0,2,R,100):CRAFT=[-10.0]
(N,0,3,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 3, 0, 25)
(N,0,3,D,25):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 3, 0, 50)
(N,0,3,D,50):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 3, 0, 75)
(N,0,3,D,75):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 3, 0, 100)
(N,0,3,D,100):CRAFT=[-10.0]
(N,0,3,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 0, 3, 1, 25)
(N,0,3,R,25):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 3, 1, 50)
(N,0,3,R,50):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 3, 1, 75)
(N,0,3,R,75):CRAFT=[-10.0]
Deciding optimal action for state: (1, 0, 3, 1, 100)
(N,0,3,R,100):CRAFT=[-10.0]
(N,1,0,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 0, 0, 25)
0.68 ( 3 1 0 0 25 ) -10.0 0.0
0.17 ( 3 1 0 1 25 ) -10.0 0.0
0.12 ( 2 1 0 0 25 ) -0.00999999999999801 0.0
0.03 ( 2 1 0 1 25 ) -24.985 0.0
(N,1,0,D,25):DOWN=[-19.241]
Deciding optimal action for state: (1, 1, 0, 0, 50)
0.68 ( 3 1 0 0 50 ) -10.0 0.0
0.17 ( 3 1 0 1 50 ) -10.0 0.0
0.12 ( 2 1 0 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 1 0 1 50 ) -24.985 0.0
(N,1,0,D,50):DOWN=[-19.241]
Deciding optimal action for state: (1, 1, 0, 0, 75)
0.68 ( 3 1 0 0 75 ) -10.0 0.0
0.17 ( 3 1 0 1 75 ) -10.0 0.0
0.12 ( 2 1 0 0 75 ) -10.0 0.0
0.03 ( 2 1 0 1 75 ) -29.98 0.0
(N,1,0,D,75):DOWN=[-20.589]
Deciding optimal action for state: (1, 1, 0, 0, 100)
0.68 ( 3 1 0 0 100 ) -10.0 0.0
0.17 ( 3 1 0 1 100 ) -10.0 0.0
0.12 ( 2 1 0 0 100 ) -10.0 0.0
0.03 ( 2 1 0 1 100 ) -29.98 0.0
(N,1,0,D,100):DOWN=[-20.589]
(N,1,0,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 0, 1, 25)
0.25 ( 1 0 1 1 25 ) -10.0 0.0
0.25 ( 1 0 1 0 25 ) -10.0 0.0
0.175 ( 1 0 2 1 25 ) -10.0 0.0
0.175 ( 1 0 2 0 25 ) -10.0 0.0
0.075 ( 1 0 3 1 25 ) -10.0 0.0
0.075 ( 1 0 3 0 25 ) -10.0 0.0
(N,1,0,R,25):CRAFT=[-19.99]
Deciding optimal action for state: (1, 1, 0, 1, 50)
0.25 ( 1 0 1 1 50 ) -10.0 0.0
0.25 ( 1 0 1 0 50 ) -10.0 0.0
0.175 ( 1 0 2 1 50 ) -10.0 0.0
0.175 ( 1 0 2 0 50 ) -10.0 0.0
0.075 ( 1 0 3 1 50 ) -10.0 0.0
0.075 ( 1 0 3 0 50 ) -10.0 0.0
(N,1,0,R,50):CRAFT=[-19.99]
Deciding optimal action for state: (1, 1, 0, 1, 75)
0.25 ( 1 0 1 1 75 ) -10.0 0.0
0.25 ( 1 0 1 0 75 ) -10.0 0.0
0.175 ( 1 0 2 1 75 ) -10.0 0.0
0.175 ( 1 0 2 0 75 ) -10.0 0.0
0.075 ( 1 0 3 1 75 ) -10.0 0.0
0.075 ( 1 0 3 0 75 ) -10.0 0.0
(N,1,0,R,75):CRAFT=[-19.99]
Deciding optimal action for state: (1, 1, 0, 1, 100)
0.25 ( 1 0 1 1 100 ) -10.0 0.0
0.25 ( 1 0 1 0 100 ) -10.0 0.0
0.175 ( 1 0 2 1 100 ) -10.0 0.0
0.175 ( 1 0 2 0 100 ) -10.0 0.0
0.075 ( 1 0 3 1 100 ) -10.0 0.0
0.075 ( 1 0 3 0 100 ) -10.0 0.0
(N,1,0,R,100):CRAFT=[-19.99]
(N,1,1,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 1, 0, 25)
0.68 ( 3 1 1 0 25 ) -10.0 0.0
0.17 ( 3 1 1 1 25 ) -10.0 0.0
0.12 ( 2 1 1 0 25 ) 34.955000000000005 0.0
0.03 ( 2 1 1 1 25 ) -7.5024999999999995 0.0
(N,1,1,D,25):DOWN=[-14.526]
Deciding optimal action for state: (1, 1, 1, 0, 50)
0.68 ( 3 1 1 0 50 ) -10.0 0.0
0.17 ( 3 1 1 1 50 ) -10.0 0.0
0.12 ( 2 1 1 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 1 1 1 50 ) -24.985 0.0
(N,1,1,D,50):DOWN=[-19.241]
Deciding optimal action for state: (1, 1, 1, 0, 75)
0.68 ( 3 1 1 0 75 ) -10.0 0.0
0.17 ( 3 1 1 1 75 ) -10.0 0.0
0.12 ( 2 1 1 0 75 ) -10.0 0.0
0.03 ( 2 1 1 1 75 ) -29.98 0.0
(N,1,1,D,75):DOWN=[-20.589]
Deciding optimal action for state: (1, 1, 1, 0, 100)
0.68 ( 3 1 1 0 100 ) -10.0 0.0
0.17 ( 3 1 1 1 100 ) -10.0 0.0
0.12 ( 2 1 1 0 100 ) -10.0 0.0
0.03 ( 2 1 1 1 100 ) -29.98 0.0
(N,1,1,D,100):DOWN=[-20.589]
(N,1,1,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 1, 1, 25)
0.425 ( 3 1 1 1 25 ) -10.0 0.0
0.425 ( 3 1 1 0 25 ) -10.0 0.0
0.075 ( 2 1 1 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 1 1 0 25 ) 34.955000000000005 0.0
(N,1,1,R,25):DOWN=[-16.435]
Deciding optimal action for state: (1, 1, 1, 1, 50)
0.25 ( 1 0 2 1 50 ) -10.0 0.0
0.25 ( 1 0 2 0 50 ) -10.0 0.0
0.175 ( 1 0 3 1 50 ) -10.0 0.0
0.175 ( 1 0 3 0 50 ) -10.0 0.0
0.075 ( 1 0 3 1 50 ) -10.0 0.0
0.075 ( 1 0 3 0 50 ) -10.0 0.0
(N,1,1,R,50):CRAFT=[-19.99]
Deciding optimal action for state: (1, 1, 1, 1, 75)
0.25 ( 1 0 2 1 75 ) -10.0 0.0
0.25 ( 1 0 2 0 75 ) -10.0 0.0
0.175 ( 1 0 3 1 75 ) -10.0 0.0
0.175 ( 1 0 3 0 75 ) -10.0 0.0
0.075 ( 1 0 3 1 75 ) -10.0 0.0
0.075 ( 1 0 3 0 75 ) -10.0 0.0
(N,1,1,R,75):CRAFT=[-19.99]
Deciding optimal action for state: (1, 1, 1, 1, 100)
0.25 ( 1 0 2 1 100 ) -10.0 0.0
0.25 ( 1 0 2 0 100 ) -10.0 0.0
0.175 ( 1 0 3 1 100 ) -10.0 0.0
0.175 ( 1 0 3 0 100 ) -10.0 0.0
0.075 ( 1 0 3 1 100 ) -10.0 0.0
0.075 ( 1 0 3 0 100 ) -10.0 0.0
(N,1,1,R,100):CRAFT=[-19.99]
(N,1,2,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 2, 0, 25)
0.68 ( 3 1 2 0 25 ) -10.0 0.0
0.17 ( 3 1 2 1 25 ) -10.0 0.0
0.12 ( 2 1 2 0 25 ) 34.955000000000005 0.0
0.03 ( 2 1 2 1 25 ) -7.5024999999999995 0.0
(N,1,2,D,25):DOWN=[-14.526]
Deciding optimal action for state: (1, 1, 2, 0, 50)
0.68 ( 3 1 2 0 50 ) -10.0 0.0
0.17 ( 3 1 2 1 50 ) -10.0 0.0
0.12 ( 2 1 2 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 1 2 1 50 ) -24.985 0.0
(N,1,2,D,50):DOWN=[-19.241]
Deciding optimal action for state: (1, 1, 2, 0, 75)
0.68 ( 3 1 2 0 75 ) -10.0 0.0
0.17 ( 3 1 2 1 75 ) -10.0 0.0
0.12 ( 2 1 2 0 75 ) -10.0 0.0
0.03 ( 2 1 2 1 75 ) -29.98 0.0
(N,1,2,D,75):DOWN=[-20.589]
Deciding optimal action for state: (1, 1, 2, 0, 100)
0.68 ( 3 1 2 0 100 ) -10.0 0.0
0.17 ( 3 1 2 1 100 ) -10.0 0.0
0.12 ( 2 1 2 0 100 ) -10.0 0.0
0.03 ( 2 1 2 1 100 ) -29.98 0.0
(N,1,2,D,100):DOWN=[-20.589]
(N,1,2,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 2, 1, 25)
0.425 ( 3 1 2 1 25 ) -10.0 0.0
0.425 ( 3 1 2 0 25 ) -10.0 0.0
0.075 ( 2 1 2 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 1 2 0 25 ) 34.955000000000005 0.0
(N,1,2,R,25):DOWN=[-16.435]
Deciding optimal action for state: (1, 1, 2, 1, 50)
0.25 ( 1 0 3 1 50 ) -10.0 0.0
0.25 ( 1 0 3 0 50 ) -10.0 0.0
0.175 ( 1 0 3 1 50 ) -10.0 0.0
0.175 ( 1 0 3 0 50 ) -10.0 0.0
0.075 ( 1 0 3 1 50 ) -10.0 0.0
0.075 ( 1 0 3 0 50 ) -10.0 0.0
(N,1,2,R,50):CRAFT=[-19.99]
Deciding optimal action for state: (1, 1, 2, 1, 75)
0.25 ( 1 0 3 1 75 ) -10.0 0.0
0.25 ( 1 0 3 0 75 ) -10.0 0.0
0.175 ( 1 0 3 1 75 ) -10.0 0.0
0.175 ( 1 0 3 0 75 ) -10.0 0.0
0.075 ( 1 0 3 1 75 ) -10.0 0.0
0.075 ( 1 0 3 0 75 ) -10.0 0.0
(N,1,2,R,75):CRAFT=[-19.99]
Deciding optimal action for state: (1, 1, 2, 1, 100)
0.25 ( 1 0 3 1 100 ) -10.0 0.0
0.25 ( 1 0 3 0 100 ) -10.0 0.0
0.175 ( 1 0 3 1 100 ) -10.0 0.0
0.175 ( 1 0 3 0 100 ) -10.0 0.0
0.075 ( 1 0 3 1 100 ) -10.0 0.0
0.075 ( 1 0 3 0 100 ) -10.0 0.0
(N,1,2,R,100):CRAFT=[-19.99]
(N,1,3,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 3, 0, 25)
0.68 ( 3 1 3 0 25 ) -10.0 0.0
0.17 ( 3 1 3 1 25 ) -10.0 0.0
0.12 ( 2 1 3 0 25 ) 34.955000000000005 0.0
0.03 ( 2 1 3 1 25 ) -7.5024999999999995 0.0
(N,1,3,D,25):DOWN=[-14.526]
Deciding optimal action for state: (1, 1, 3, 0, 50)
0.68 ( 3 1 3 0 50 ) -10.0 0.0
0.17 ( 3 1 3 1 50 ) -10.0 0.0
0.12 ( 2 1 3 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 1 3 1 50 ) -24.985 0.0
(N,1,3,D,50):DOWN=[-19.241]
Deciding optimal action for state: (1, 1, 3, 0, 75)
0.68 ( 3 1 3 0 75 ) -10.0 0.0
0.17 ( 3 1 3 1 75 ) -10.0 0.0
0.12 ( 2 1 3 0 75 ) -10.0 0.0
0.03 ( 2 1 3 1 75 ) -29.98 0.0
(N,1,3,D,75):DOWN=[-20.589]
Deciding optimal action for state: (1, 1, 3, 0, 100)
0.68 ( 3 1 3 0 100 ) -10.0 0.0
0.17 ( 3 1 3 1 100 ) -10.0 0.0
0.12 ( 2 1 3 0 100 ) -10.0 0.0
0.03 ( 2 1 3 1 100 ) -29.98 0.0
(N,1,3,D,100):DOWN=[-20.589]
(N,1,3,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 1, 3, 1, 25)
0.425 ( 3 1 3 1 25 ) -10.0 0.0
0.425 ( 3 1 3 0 25 ) -10.0 0.0
0.075 ( 2 1 3 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 1 3 0 25 ) 34.955000000000005 0.0
(N,1,3,R,25):DOWN=[-16.435]
Deciding optimal action for state: (1, 1, 3, 1, 50)
0.25 ( 1 0 3 1 50 ) -10.0 0.0
0.25 ( 1 0 3 0 50 ) -10.0 0.0
0.175 ( 1 0 3 1 50 ) -10.0 0.0
0.175 ( 1 0 3 0 50 ) -10.0 0.0
0.075 ( 1 0 3 1 50 ) -10.0 0.0
0.075 ( 1 0 3 0 50 ) -10.0 0.0
(N,1,3,R,50):CRAFT=[-19.99]
Deciding optimal action for state: (1, 1, 3, 1, 75)
0.25 ( 1 0 3 1 75 ) -10.0 0.0
0.25 ( 1 0 3 0 75 ) -10.0 0.0
0.175 ( 1 0 3 1 75 ) -10.0 0.0
0.175 ( 1 0 3 0 75 ) -10.0 0.0
0.075 ( 1 0 3 1 75 ) -10.0 0.0
0.075 ( 1 0 3 0 75 ) -10.0 0.0
(N,1,3,R,75):CRAFT=[-19.99]
Deciding optimal action for state: (1, 1, 3, 1, 100)
0.25 ( 1 0 3 1 100 ) -10.0 0.0
0.25 ( 1 0 3 0 100 ) -10.0 0.0
0.175 ( 1 0 3 1 100 ) -10.0 0.0
0.175 ( 1 0 3 0 100 ) -10.0 0.0
0.075 ( 1 0 3 1 100 ) -10.0 0.0
0.075 ( 1 0 3 0 100 ) -10.0 0.0
(N,1,3,R,100):CRAFT=[-19.99]
(N,2,0,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 0, 0, 25)
0.68 ( 3 2 0 0 25 ) -10.0 0.0
0.17 ( 3 2 0 1 25 ) -10.0 0.0
0.12 ( 2 2 0 0 25 ) -0.00999999999999801 0.0
0.03 ( 2 2 0 1 25 ) -24.985 0.0
(N,2,0,D,25):DOWN=[-19.241]
Deciding optimal action for state: (1, 2, 0, 0, 50)
0.68 ( 3 2 0 0 50 ) -10.0 0.0
0.17 ( 3 2 0 1 50 ) -10.0 0.0
0.12 ( 2 2 0 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 2 0 1 50 ) -24.985 0.0
(N,2,0,D,50):DOWN=[-19.241]
Deciding optimal action for state: (1, 2, 0, 0, 75)
0.68 ( 3 2 0 0 75 ) -10.0 0.0
0.17 ( 3 2 0 1 75 ) -10.0 0.0
0.12 ( 2 2 0 0 75 ) -10.0 0.0
0.03 ( 2 2 0 1 75 ) -29.98 0.0
(N,2,0,D,75):DOWN=[-20.589]
Deciding optimal action for state: (1, 2, 0, 0, 100)
0.68 ( 3 2 0 0 100 ) -10.0 0.0
0.17 ( 3 2 0 1 100 ) -10.0 0.0
0.12 ( 2 2 0 0 100 ) -10.0 0.0
0.03 ( 2 2 0 1 100 ) -29.98 0.0
(N,2,0,D,100):DOWN=[-20.589]
(N,2,0,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 0, 1, 25)
0.25 ( 1 1 1 1 25 ) -10.0 0.0
0.25 ( 1 1 1 0 25 ) -10.0 0.0
0.175 ( 1 1 2 1 25 ) -10.0 0.0
0.175 ( 1 1 2 0 25 ) -10.0 0.0
0.075 ( 1 1 3 1 25 ) -10.0 0.0
0.075 ( 1 1 3 0 25 ) -10.0 0.0
(N,2,0,R,25):CRAFT=[-19.99]
Deciding optimal action for state: (1, 2, 0, 1, 50)
0.25 ( 1 1 1 1 50 ) -10.0 0.0
0.25 ( 1 1 1 0 50 ) -10.0 0.0
0.175 ( 1 1 2 1 50 ) -10.0 0.0
0.175 ( 1 1 2 0 50 ) -10.0 0.0
0.075 ( 1 1 3 1 50 ) -10.0 0.0
0.075 ( 1 1 3 0 50 ) -10.0 0.0
(N,2,0,R,50):CRAFT=[-19.99]
Deciding optimal action for state: (1, 2, 0, 1, 75)
0.25 ( 1 1 1 1 75 ) -10.0 0.0
0.25 ( 1 1 1 0 75 ) -10.0 0.0
0.175 ( 1 1 2 1 75 ) -10.0 0.0
0.175 ( 1 1 2 0 75 ) -10.0 0.0
0.075 ( 1 1 3 1 75 ) -10.0 0.0
0.075 ( 1 1 3 0 75 ) -10.0 0.0
(N,2,0,R,75):CRAFT=[-19.99]
Deciding optimal action for state: (1, 2, 0, 1, 100)
0.25 ( 1 1 1 1 100 ) -10.0 0.0
0.25 ( 1 1 1 0 100 ) -10.0 0.0
0.175 ( 1 1 2 1 100 ) -10.0 0.0
0.175 ( 1 1 2 0 100 ) -10.0 0.0
0.075 ( 1 1 3 1 100 ) -10.0 0.0
0.075 ( 1 1 3 0 100 ) -10.0 0.0
(N,2,0,R,100):CRAFT=[-19.99]
(N,2,1,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 1, 0, 25)
0.68 ( 3 2 1 0 25 ) -10.0 0.0
0.17 ( 3 2 1 1 25 ) -10.0 0.0
0.12 ( 2 2 1 0 25 ) 34.955000000000005 0.0
0.03 ( 2 2 1 1 25 ) -7.5024999999999995 0.0
(N,2,1,D,25):DOWN=[-14.526]
Deciding optimal action for state: (1, 2, 1, 0, 50)
0.68 ( 3 2 1 0 50 ) -10.0 0.0
0.17 ( 3 2 1 1 50 ) -10.0 0.0
0.12 ( 2 2 1 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 2 1 1 50 ) -24.985 0.0
(N,2,1,D,50):DOWN=[-19.241]
Deciding optimal action for state: (1, 2, 1, 0, 75)
0.68 ( 3 2 1 0 75 ) -10.0 0.0
0.17 ( 3 2 1 1 75 ) -10.0 0.0
0.12 ( 2 2 1 0 75 ) -10.0 0.0
0.03 ( 2 2 1 1 75 ) -29.98 0.0
(N,2,1,D,75):DOWN=[-20.589]
Deciding optimal action for state: (1, 2, 1, 0, 100)
0.68 ( 3 2 1 0 100 ) -10.0 0.0
0.17 ( 3 2 1 1 100 ) -10.0 0.0
0.12 ( 2 2 1 0 100 ) -10.0 0.0
0.03 ( 2 2 1 1 100 ) -29.98 0.0
(N,2,1,D,100):DOWN=[-20.589]
(N,2,1,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 1, 1, 25)
0.425 ( 3 2 1 1 25 ) -10.0 0.0
0.425 ( 3 2 1 0 25 ) -10.0 0.0
0.075 ( 2 2 1 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 2 1 0 25 ) 34.955000000000005 0.0
(N,2,1,R,25):DOWN=[-16.435]
Deciding optimal action for state: (1, 2, 1, 1, 50)
0.25 ( 1 1 2 1 50 ) -10.0 0.0
0.25 ( 1 1 2 0 50 ) -10.0 0.0
0.175 ( 1 1 3 1 50 ) -10.0 0.0
0.175 ( 1 1 3 0 50 ) -10.0 0.0
0.075 ( 1 1 3 1 50 ) -10.0 0.0
0.075 ( 1 1 3 0 50 ) -10.0 0.0
(N,2,1,R,50):CRAFT=[-19.99]
Deciding optimal action for state: (1, 2, 1, 1, 75)
0.25 ( 1 1 2 1 75 ) -10.0 0.0
0.25 ( 1 1 2 0 75 ) -10.0 0.0
0.175 ( 1 1 3 1 75 ) -10.0 0.0
0.175 ( 1 1 3 0 75 ) -10.0 0.0
0.075 ( 1 1 3 1 75 ) -10.0 0.0
0.075 ( 1 1 3 0 75 ) -10.0 0.0
(N,2,1,R,75):CRAFT=[-19.99]
Deciding optimal action for state: (1, 2, 1, 1, 100)
0.25 ( 1 1 2 1 100 ) -10.0 0.0
0.25 ( 1 1 2 0 100 ) -10.0 0.0
0.175 ( 1 1 3 1 100 ) -10.0 0.0
0.175 ( 1 1 3 0 100 ) -10.0 0.0
0.075 ( 1 1 3 1 100 ) -10.0 0.0
0.075 ( 1 1 3 0 100 ) -10.0 0.0
(N,2,1,R,100):CRAFT=[-19.99]
(N,2,2,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 2, 0, 25)
0.68 ( 3 2 2 0 25 ) -10.0 0.0
0.17 ( 3 2 2 1 25 ) -10.0 0.0
0.12 ( 2 2 2 0 25 ) 34.955000000000005 0.0
0.03 ( 2 2 2 1 25 ) -7.5024999999999995 0.0
(N,2,2,D,25):DOWN=[-14.526]
Deciding optimal action for state: (1, 2, 2, 0, 50)
0.68 ( 3 2 2 0 50 ) -10.0 0.0
0.17 ( 3 2 2 1 50 ) -10.0 0.0
0.12 ( 2 2 2 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 2 2 1 50 ) -24.985 0.0
(N,2,2,D,50):DOWN=[-19.241]
Deciding optimal action for state: (1, 2, 2, 0, 75)
0.68 ( 3 2 2 0 75 ) -10.0 0.0
0.17 ( 3 2 2 1 75 ) -10.0 0.0
0.12 ( 2 2 2 0 75 ) -10.0 0.0
0.03 ( 2 2 2 1 75 ) -29.98 0.0
(N,2,2,D,75):DOWN=[-20.589]
Deciding optimal action for state: (1, 2, 2, 0, 100)
0.68 ( 3 2 2 0 100 ) -10.0 0.0
0.17 ( 3 2 2 1 100 ) -10.0 0.0
0.12 ( 2 2 2 0 100 ) -10.0 0.0
0.03 ( 2 2 2 1 100 ) -29.98 0.0
(N,2,2,D,100):DOWN=[-20.589]
(N,2,2,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 2, 1, 25)
0.425 ( 3 2 2 1 25 ) -10.0 0.0
0.425 ( 3 2 2 0 25 ) -10.0 0.0
0.075 ( 2 2 2 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 2 2 0 25 ) 34.955000000000005 0.0
(N,2,2,R,25):DOWN=[-16.435]
Deciding optimal action for state: (1, 2, 2, 1, 50)
0.25 ( 1 1 3 1 50 ) -10.0 0.0
0.25 ( 1 1 3 0 50 ) -10.0 0.0
0.175 ( 1 1 3 1 50 ) -10.0 0.0
0.175 ( 1 1 3 0 50 ) -10.0 0.0
0.075 ( 1 1 3 1 50 ) -10.0 0.0
0.075 ( 1 1 3 0 50 ) -10.0 0.0
(N,2,2,R,50):CRAFT=[-19.99]
Deciding optimal action for state: (1, 2, 2, 1, 75)
0.25 ( 1 1 3 1 75 ) -10.0 0.0
0.25 ( 1 1 3 0 75 ) -10.0 0.0
0.175 ( 1 1 3 1 75 ) -10.0 0.0
0.175 ( 1 1 3 0 75 ) -10.0 0.0
0.075 ( 1 1 3 1 75 ) -10.0 0.0
0.075 ( 1 1 3 0 75 ) -10.0 0.0
(N,2,2,R,75):CRAFT=[-19.99]
Deciding optimal action for state: (1, 2, 2, 1, 100)
0.25 ( 1 1 3 1 100 ) -10.0 0.0
0.25 ( 1 1 3 0 100 ) -10.0 0.0
0.175 ( 1 1 3 1 100 ) -10.0 0.0
0.175 ( 1 1 3 0 100 ) -10.0 0.0
0.075 ( 1 1 3 1 100 ) -10.0 0.0
0.075 ( 1 1 3 0 100 ) -10.0 0.0
(N,2,2,R,100):CRAFT=[-19.99]
(N,2,3,D,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 3, 0, 25)
0.68 ( 3 2 3 0 25 ) -10.0 0.0
0.17 ( 3 2 3 1 25 ) -10.0 0.0
0.12 ( 2 2 3 0 25 ) 34.955000000000005 0.0
0.03 ( 2 2 3 1 25 ) -7.5024999999999995 0.0
(N,2,3,D,25):DOWN=[-14.526]
Deciding optimal action for state: (1, 2, 3, 0, 50)
0.68 ( 3 2 3 0 50 ) -10.0 0.0
0.17 ( 3 2 3 1 50 ) -10.0 0.0
0.12 ( 2 2 3 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 2 3 1 50 ) -24.985 0.0
(N,2,3,D,50):DOWN=[-19.241]
Deciding optimal action for state: (1, 2, 3, 0, 75)
0.68 ( 3 2 3 0 75 ) -10.0 0.0
0.17 ( 3 2 3 1 75 ) -10.0 0.0
0.12 ( 2 2 3 0 75 ) -10.0 0.0
0.03 ( 2 2 3 1 75 ) -29.98 0.0
(N,2,3,D,75):DOWN=[-20.589]
Deciding optimal action for state: (1, 2, 3, 0, 100)
0.68 ( 3 2 3 0 100 ) -10.0 0.0
0.17 ( 3 2 3 1 100 ) -10.0 0.0
0.12 ( 2 2 3 0 100 ) -10.0 0.0
0.03 ( 2 2 3 1 100 ) -29.98 0.0
(N,2,3,D,100):DOWN=[-20.589]
(N,2,3,R,0):NONE=[0.000]
Deciding optimal action for state: (1, 2, 3, 1, 25)
0.425 ( 3 2 3 1 25 ) -10.0 0.0
0.425 ( 3 2 3 0 25 ) -10.0 0.0
0.075 ( 2 2 3 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 2 3 0 25 ) 34.955000000000005 0.0
(N,2,3,R,25):DOWN=[-16.435]
Deciding optimal action for state: (1, 2, 3, 1, 50)
0.25 ( 1 1 3 1 50 ) -10.0 0.0
0.25 ( 1 1 3 0 50 ) -10.0 0.0
0.175 ( 1 1 3 1 50 ) -10.0 0.0
0.175 ( 1 1 3 0 50 ) -10.0 0.0
0.075 ( 1 1 3 1 50 ) -10.0 0.0
0.075 ( 1 1 3 0 50 ) -10.0 0.0
(N,2,3,R,50):CRAFT=[-19.99]
Deciding optimal action for state: (1, 2, 3, 1, 75)
0.25 ( 1 1 3 1 75 ) -10.0 0.0
0.25 ( 1 1 3 0 75 ) -10.0 0.0
0.175 ( 1 1 3 1 75 ) -10.0 0.0
0.175 ( 1 1 3 0 75 ) -10.0 0.0
0.075 ( 1 1 3 1 75 ) -10.0 0.0
0.075 ( 1 1 3 0 75 ) -10.0 0.0
(N,2,3,R,75):CRAFT=[-19.99]
Deciding optimal action for state: (1, 2, 3, 1, 100)
0.25 ( 1 1 3 1 100 ) -10.0 0.0
0.25 ( 1 1 3 0 100 ) -10.0 0.0
0.175 ( 1 1 3 1 100 ) -10.0 0.0
0.175 ( 1 1 3 0 100 ) -10.0 0.0
0.075 ( 1 1 3 1 100 ) -10.0 0.0
0.075 ( 1 1 3 0 100 ) -10.0 0.0
(N,2,3,R,100):CRAFT=[-19.99]
(E,0,0,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 0, 0, 25)
0.6400000000000001 ( 2 0 0 0 25 ) -0.00999999999999801 0.0
0.16000000000000003 ( 2 0 0 1 25 ) -24.985 0.0
0.16000000000000003 ( 2 0 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 0 0 1 0 ) 0.0 50.0
(E,0,0,D,25):HIT=[-4.01]
Deciding optimal action for state: (2, 0, 0, 0, 50)
0.6400000000000001 ( 2 0 0 0 50 ) -0.00999999999999801 0.0
0.16000000000000003 ( 2 0 0 1 50 ) -24.985 0.0
0.16000000000000003 ( 2 0 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 0 0 1 0 ) 0.0 50.0
(E,0,0,D,50):HIT=[-4.01]
Deciding optimal action for state: (2, 0, 0, 0, 75)
(E,0,0,D,75):SHOOT=[-10.0]
Deciding optimal action for state: (2, 0, 0, 0, 100)
(E,0,0,D,100):SHOOT=[-10.0]
(E,0,0,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 0, 1, 25)
0.5 ( 2 0 0 0 50 ) -0.00999999999999801 0.0
(E,0,0,R,25):SHOOT=[-29.985]
Deciding optimal action for state: (2, 0, 0, 1, 50)
0.5 ( 2 0 0 0 75 ) -10.0 0.0
(E,0,0,R,50):SHOOT=[-34.975]
Deciding optimal action for state: (2, 0, 0, 1, 75)
0.5 ( 2 0 0 0 100 ) -10.0 0.0
(E,0,0,R,75):SHOOT=[-34.975]
Deciding optimal action for state: (2, 0, 0, 1, 100)
0.5 ( 2 0 0 0 100 ) -10.0 0.0
(E,0,0,R,100):SHOOT=[-34.975]
(E,0,1,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 1, 0, 25)
0.08000000000000002 ( 2 0 0 0 25 ) -0.00999999999999801 0.0
0.020000000000000004 ( 2 0 0 1 25 ) -24.985 0.0
0.7200000000000001 ( 2 0 0 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 0 0 1 0 ) 0.0 50.0
(E,0,1,D,25):SHOOT=[34.455]
Deciding optimal action for state: (2, 0, 1, 0, 50)
0.6400000000000001 ( 2 0 1 0 50 ) -0.00999999999999801 0.0
0.16000000000000003 ( 2 0 1 1 50 ) -24.985 0.0
0.16000000000000003 ( 2 0 1 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 0 1 1 0 ) 0.0 50.0
(E,0,1,D,50):HIT=[-4.01]
Deciding optimal action for state: (2, 0, 1, 0, 75)
0.08000000000000002 ( 2 0 0 0 75 ) -10.0 0.0
0.020000000000000004 ( 2 0 0 1 75 ) -29.98 0.0
0.7200000000000001 ( 2 0 0 0 50 ) -0.00999999999999801 0.0
0.18000000000000002 ( 2 0 0 1 50 ) -24.985 0.0
(E,0,1,D,75):SHOOT=[-15.898]
Deciding optimal action for state: (2, 0, 1, 0, 100)
0.6400000000000001 ( 2 0 1 0 100 ) -10.0 0.0
0.16000000000000003 ( 2 0 1 1 100 ) -29.98 0.0
0.16000000000000003 ( 2 0 1 0 50 ) -0.00999999999999801 0.0
0.04000000000000001 ( 2 0 1 1 50 ) -24.985 0.0
(E,0,1,D,100):HIT=[-22.186]
(E,0,1,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 1, 1, 25)
0.05 ( 2 0 0 1 25 ) -24.985 0.0
0.45 ( 2 0 0 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 50 ) -0.00999999999999801 0.0
(E,0,1,R,25):SHOOT=[-8.755]
Deciding optimal action for state: (2, 0, 1, 1, 50)
0.4 ( 2 0 1 1 50 ) -24.985 0.0
0.1 ( 2 0 1 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 75 ) -10.0 0.0
(E,0,1,R,50):HIT=[-39.964]
Deciding optimal action for state: (2, 0, 1, 1, 75)
0.05 ( 2 0 0 1 75 ) -29.98 0.0
0.45 ( 2 0 0 1 50 ) -24.985 0.0
0.5 ( 2 0 0 0 100 ) -10.0 0.0
(E,0,1,R,75):SHOOT=[-47.705]
Deciding optimal action for state: (2, 0, 1, 1, 100)
0.4 ( 2 0 1 1 100 ) -29.98 0.0
0.1 ( 2 0 1 1 50 ) -24.985 0.0
0.5 ( 2 0 0 0 100 ) -10.0 0.0
(E,0,1,R,100):HIT=[-49.451]
(E,0,2,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 2, 0, 25)
0.08000000000000002 ( 2 0 1 0 25 ) 34.955000000000005 0.0
0.020000000000000004 ( 2 0 1 1 25 ) -7.5024999999999995 0.0
0.7200000000000001 ( 2 0 1 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 0 1 1 0 ) 0.0 50.0
(E,0,2,D,25):SHOOT=[37.599]
Deciding optimal action for state: (2, 0, 2, 0, 50)
0.08000000000000002 ( 2 0 1 0 50 ) -0.00999999999999801 0.0
0.020000000000000004 ( 2 0 1 1 50 ) -24.985 0.0
0.7200000000000001 ( 2 0 1 0 25 ) 34.955000000000005 0.0
0.18000000000000002 ( 2 0 1 1 25 ) -7.5024999999999995 0.0
(E,0,2,D,50):SHOOT=[13.293]
Deciding optimal action for state: (2, 0, 2, 0, 75)
0.08000000000000002 ( 2 0 1 0 75 ) -10.0 0.0
0.020000000000000004 ( 2 0 1 1 75 ) -29.98 0.0
0.7200000000000001 ( 2 0 1 0 50 ) -0.00999999999999801 0.0
0.18000000000000002 ( 2 0 1 1 50 ) -24.985 0.0
(E,0,2,D,75):SHOOT=[-15.898]
Deciding optimal action for state: (2, 0, 2, 0, 100)
0.6400000000000001 ( 2 0 2 0 100 ) -10.0 0.0
0.16000000000000003 ( 2 0 2 1 100 ) -29.98 0.0
0.16000000000000003 ( 2 0 2 0 50 ) -0.00999999999999801 0.0
0.04000000000000001 ( 2 0 2 1 50 ) -24.985 0.0
(E,0,2,D,100):HIT=[-22.186]
(E,0,2,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 2, 1, 25)
0.05 ( 2 0 1 1 25 ) -7.5024999999999995 0.0
0.45 ( 2 0 1 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 50 ) -0.00999999999999801 0.0
(E,0,2,R,25):SHOOT=[-7.882]
Deciding optimal action for state: (2, 0, 2, 1, 50)
0.05 ( 2 0 1 1 50 ) -24.985 0.0
0.45 ( 2 0 1 1 25 ) -7.5024999999999995 0.0
0.5 ( 2 0 0 0 75 ) -10.0 0.0
(E,0,2,R,50):SHOOT=[-39.596]
Deciding optimal action for state: (2, 0, 2, 1, 75)
0.05 ( 2 0 1 1 75 ) -29.98 0.0
0.45 ( 2 0 1 1 50 ) -24.985 0.0
0.5 ( 2 0 0 0 100 ) -10.0 0.0
(E,0,2,R,75):SHOOT=[-47.705]
Deciding optimal action for state: (2, 0, 2, 1, 100)
0.4 ( 2 0 2 1 100 ) -29.98 0.0
0.1 ( 2 0 2 1 50 ) -24.985 0.0
0.5 ( 2 0 0 0 100 ) -10.0 0.0
(E,0,2,R,100):HIT=[-49.451]
(E,0,3,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 3, 0, 25)
0.08000000000000002 ( 2 0 2 0 25 ) 34.955000000000005 0.0
0.020000000000000004 ( 2 0 2 1 25 ) -7.5024999999999995 0.0
0.7200000000000001 ( 2 0 2 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 0 2 1 0 ) 0.0 50.0
(E,0,3,D,25):SHOOT=[37.599]
Deciding optimal action for state: (2, 0, 3, 0, 50)
0.08000000000000002 ( 2 0 2 0 50 ) -0.00999999999999801 0.0
0.020000000000000004 ( 2 0 2 1 50 ) -24.985 0.0
0.7200000000000001 ( 2 0 2 0 25 ) 34.955000000000005 0.0
0.18000000000000002 ( 2 0 2 1 25 ) -7.5024999999999995 0.0
(E,0,3,D,50):SHOOT=[13.293]
Deciding optimal action for state: (2, 0, 3, 0, 75)
0.08000000000000002 ( 2 0 2 0 75 ) -10.0 0.0
0.020000000000000004 ( 2 0 2 1 75 ) -29.98 0.0
0.7200000000000001 ( 2 0 2 0 50 ) -0.00999999999999801 0.0
0.18000000000000002 ( 2 0 2 1 50 ) -24.985 0.0
(E,0,3,D,75):SHOOT=[-15.898]
Deciding optimal action for state: (2, 0, 3, 0, 100)
0.6400000000000001 ( 2 0 3 0 100 ) -10.0 0.0
0.16000000000000003 ( 2 0 3 1 100 ) -29.98 0.0
0.16000000000000003 ( 2 0 3 0 50 ) -0.00999999999999801 0.0
0.04000000000000001 ( 2 0 3 1 50 ) -24.985 0.0
(E,0,3,D,100):HIT=[-22.186]
(E,0,3,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 0, 3, 1, 25)
0.05 ( 2 0 2 1 25 ) -7.5024999999999995 0.0
0.45 ( 2 0 2 1 0 ) 0.0 50.0
0.5 ( 2 0 0 0 50 ) -0.00999999999999801 0.0
(E,0,3,R,25):SHOOT=[-7.882]
Deciding optimal action for state: (2, 0, 3, 1, 50)
0.05 ( 2 0 2 1 50 ) -24.985 0.0
0.45 ( 2 0 2 1 25 ) -7.5024999999999995 0.0
0.5 ( 2 0 0 0 75 ) -10.0 0.0
(E,0,3,R,50):SHOOT=[-39.596]
Deciding optimal action for state: (2, 0, 3, 1, 75)
0.05 ( 2 0 2 1 75 ) -29.98 0.0
0.45 ( 2 0 2 1 50 ) -24.985 0.0
0.5 ( 2 0 0 0 100 ) -10.0 0.0
(E,0,3,R,75):SHOOT=[-47.705]
Deciding optimal action for state: (2, 0, 3, 1, 100)
0.4 ( 2 0 3 1 100 ) -29.98 0.0
0.1 ( 2 0 3 1 50 ) -24.985 0.0
0.5 ( 2 0 0 0 100 ) -10.0 0.0
(E,0,3,R,100):HIT=[-49.451]
(E,1,0,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 0, 0, 25)
0.6400000000000001 ( 2 1 0 0 25 ) -0.00999999999999801 0.0
0.16000000000000003 ( 2 1 0 1 25 ) -24.985 0.0
0.16000000000000003 ( 2 1 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 1 0 1 0 ) 0.0 50.0
(E,1,0,D,25):HIT=[-4.01]
Deciding optimal action for state: (2, 1, 0, 0, 50)
0.6400000000000001 ( 2 1 0 0 50 ) -0.00999999999999801 0.0
0.16000000000000003 ( 2 1 0 1 50 ) -24.985 0.0
0.16000000000000003 ( 2 1 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 1 0 1 0 ) 0.0 50.0
(E,1,0,D,50):HIT=[-4.01]
Deciding optimal action for state: (2, 1, 0, 0, 75)
(E,1,0,D,75):SHOOT=[-10.0]
Deciding optimal action for state: (2, 1, 0, 0, 100)
(E,1,0,D,100):SHOOT=[-10.0]
(E,1,0,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 0, 1, 25)
0.5 ( 2 1 0 0 50 ) -0.00999999999999801 0.0
(E,1,0,R,25):SHOOT=[-29.985]
Deciding optimal action for state: (2, 1, 0, 1, 50)
0.5 ( 2 1 0 0 75 ) -10.0 0.0
(E,1,0,R,50):SHOOT=[-34.975]
Deciding optimal action for state: (2, 1, 0, 1, 75)
0.5 ( 2 1 0 0 100 ) -10.0 0.0
(E,1,0,R,75):SHOOT=[-34.975]
Deciding optimal action for state: (2, 1, 0, 1, 100)
0.5 ( 2 1 0 0 100 ) -10.0 0.0
(E,1,0,R,100):SHOOT=[-34.975]
(E,1,1,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 1, 0, 25)
0.08000000000000002 ( 2 1 0 0 25 ) -0.00999999999999801 0.0
0.020000000000000004 ( 2 1 0 1 25 ) -24.985 0.0
0.7200000000000001 ( 2 1 0 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 1 0 1 0 ) 0.0 50.0
(E,1,1,D,25):SHOOT=[34.455]
Deciding optimal action for state: (2, 1, 1, 0, 50)
0.6400000000000001 ( 2 1 1 0 50 ) -0.00999999999999801 0.0
0.16000000000000003 ( 2 1 1 1 50 ) -24.985 0.0
0.16000000000000003 ( 2 1 1 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 1 1 1 0 ) 0.0 50.0
(E,1,1,D,50):HIT=[-4.01]
Deciding optimal action for state: (2, 1, 1, 0, 75)
0.08000000000000002 ( 2 1 0 0 75 ) -10.0 0.0
0.020000000000000004 ( 2 1 0 1 75 ) -29.98 0.0
0.7200000000000001 ( 2 1 0 0 50 ) -0.00999999999999801 0.0
0.18000000000000002 ( 2 1 0 1 50 ) -24.985 0.0
(E,1,1,D,75):SHOOT=[-15.898]
Deciding optimal action for state: (2, 1, 1, 0, 100)
0.6400000000000001 ( 2 1 1 0 100 ) -10.0 0.0
0.16000000000000003 ( 2 1 1 1 100 ) -29.98 0.0
0.16000000000000003 ( 2 1 1 0 50 ) -0.00999999999999801 0.0
0.04000000000000001 ( 2 1 1 1 50 ) -24.985 0.0
(E,1,1,D,100):HIT=[-22.186]
(E,1,1,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 1, 1, 25)
0.05 ( 2 1 0 1 25 ) -24.985 0.0
0.45 ( 2 1 0 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 50 ) -0.00999999999999801 0.0
(E,1,1,R,25):SHOOT=[-8.755]
Deciding optimal action for state: (2, 1, 1, 1, 50)
0.4 ( 2 1 1 1 50 ) -24.985 0.0
0.1 ( 2 1 1 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 75 ) -10.0 0.0
(E,1,1,R,50):HIT=[-39.964]
Deciding optimal action for state: (2, 1, 1, 1, 75)
0.05 ( 2 1 0 1 75 ) -29.98 0.0
0.45 ( 2 1 0 1 50 ) -24.985 0.0
0.5 ( 2 1 0 0 100 ) -10.0 0.0
(E,1,1,R,75):SHOOT=[-47.705]
Deciding optimal action for state: (2, 1, 1, 1, 100)
0.4 ( 2 1 1 1 100 ) -29.98 0.0
0.1 ( 2 1 1 1 50 ) -24.985 0.0
0.5 ( 2 1 0 0 100 ) -10.0 0.0
(E,1,1,R,100):HIT=[-49.451]
(E,1,2,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 2, 0, 25)
0.08000000000000002 ( 2 1 1 0 25 ) 34.955000000000005 0.0
0.020000000000000004 ( 2 1 1 1 25 ) -7.5024999999999995 0.0
0.7200000000000001 ( 2 1 1 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 1 1 1 0 ) 0.0 50.0
(E,1,2,D,25):SHOOT=[37.599]
Deciding optimal action for state: (2, 1, 2, 0, 50)
0.08000000000000002 ( 2 1 1 0 50 ) -0.00999999999999801 0.0
0.020000000000000004 ( 2 1 1 1 50 ) -24.985 0.0
0.7200000000000001 ( 2 1 1 0 25 ) 34.955000000000005 0.0
0.18000000000000002 ( 2 1 1 1 25 ) -7.5024999999999995 0.0
(E,1,2,D,50):SHOOT=[13.293]
Deciding optimal action for state: (2, 1, 2, 0, 75)
0.08000000000000002 ( 2 1 1 0 75 ) -10.0 0.0
0.020000000000000004 ( 2 1 1 1 75 ) -29.98 0.0
0.7200000000000001 ( 2 1 1 0 50 ) -0.00999999999999801 0.0
0.18000000000000002 ( 2 1 1 1 50 ) -24.985 0.0
(E,1,2,D,75):SHOOT=[-15.898]
Deciding optimal action for state: (2, 1, 2, 0, 100)
0.6400000000000001 ( 2 1 2 0 100 ) -10.0 0.0
0.16000000000000003 ( 2 1 2 1 100 ) -29.98 0.0
0.16000000000000003 ( 2 1 2 0 50 ) -0.00999999999999801 0.0
0.04000000000000001 ( 2 1 2 1 50 ) -24.985 0.0
(E,1,2,D,100):HIT=[-22.186]
(E,1,2,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 2, 1, 25)
0.05 ( 2 1 1 1 25 ) -7.5024999999999995 0.0
0.45 ( 2 1 1 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 50 ) -0.00999999999999801 0.0
(E,1,2,R,25):SHOOT=[-7.882]
Deciding optimal action for state: (2, 1, 2, 1, 50)
0.05 ( 2 1 1 1 50 ) -24.985 0.0
0.45 ( 2 1 1 1 25 ) -7.5024999999999995 0.0
0.5 ( 2 1 0 0 75 ) -10.0 0.0
(E,1,2,R,50):SHOOT=[-39.596]
Deciding optimal action for state: (2, 1, 2, 1, 75)
0.05 ( 2 1 1 1 75 ) -29.98 0.0
0.45 ( 2 1 1 1 50 ) -24.985 0.0
0.5 ( 2 1 0 0 100 ) -10.0 0.0
(E,1,2,R,75):SHOOT=[-47.705]
Deciding optimal action for state: (2, 1, 2, 1, 100)
0.4 ( 2 1 2 1 100 ) -29.98 0.0
0.1 ( 2 1 2 1 50 ) -24.985 0.0
0.5 ( 2 1 0 0 100 ) -10.0 0.0
(E,1,2,R,100):HIT=[-49.451]
(E,1,3,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 3, 0, 25)
0.08000000000000002 ( 2 1 2 0 25 ) 34.955000000000005 0.0
0.020000000000000004 ( 2 1 2 1 25 ) -7.5024999999999995 0.0
0.7200000000000001 ( 2 1 2 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 1 2 1 0 ) 0.0 50.0
(E,1,3,D,25):SHOOT=[37.599]
Deciding optimal action for state: (2, 1, 3, 0, 50)
0.08000000000000002 ( 2 1 2 0 50 ) -0.00999999999999801 0.0
0.020000000000000004 ( 2 1 2 1 50 ) -24.985 0.0
0.7200000000000001 ( 2 1 2 0 25 ) 34.955000000000005 0.0
0.18000000000000002 ( 2 1 2 1 25 ) -7.5024999999999995 0.0
(E,1,3,D,50):SHOOT=[13.293]
Deciding optimal action for state: (2, 1, 3, 0, 75)
0.08000000000000002 ( 2 1 2 0 75 ) -10.0 0.0
0.020000000000000004 ( 2 1 2 1 75 ) -29.98 0.0
0.7200000000000001 ( 2 1 2 0 50 ) -0.00999999999999801 0.0
0.18000000000000002 ( 2 1 2 1 50 ) -24.985 0.0
(E,1,3,D,75):SHOOT=[-15.898]
Deciding optimal action for state: (2, 1, 3, 0, 100)
0.6400000000000001 ( 2 1 3 0 100 ) -10.0 0.0
0.16000000000000003 ( 2 1 3 1 100 ) -29.98 0.0
0.16000000000000003 ( 2 1 3 0 50 ) -0.00999999999999801 0.0
0.04000000000000001 ( 2 1 3 1 50 ) -24.985 0.0
(E,1,3,D,100):HIT=[-22.186]
(E,1,3,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 1, 3, 1, 25)
0.05 ( 2 1 2 1 25 ) -7.5024999999999995 0.0
0.45 ( 2 1 2 1 0 ) 0.0 50.0
0.5 ( 2 1 0 0 50 ) -0.00999999999999801 0.0
(E,1,3,R,25):SHOOT=[-7.882]
Deciding optimal action for state: (2, 1, 3, 1, 50)
0.05 ( 2 1 2 1 50 ) -24.985 0.0
0.45 ( 2 1 2 1 25 ) -7.5024999999999995 0.0
0.5 ( 2 1 0 0 75 ) -10.0 0.0
(E,1,3,R,50):SHOOT=[-39.596]
Deciding optimal action for state: (2, 1, 3, 1, 75)
0.05 ( 2 1 2 1 75 ) -29.98 0.0
0.45 ( 2 1 2 1 50 ) -24.985 0.0
0.5 ( 2 1 0 0 100 ) -10.0 0.0
(E,1,3,R,75):SHOOT=[-47.705]
Deciding optimal action for state: (2, 1, 3, 1, 100)
0.4 ( 2 1 3 1 100 ) -29.98 0.0
0.1 ( 2 1 3 1 50 ) -24.985 0.0
0.5 ( 2 1 0 0 100 ) -10.0 0.0
(E,1,3,R,100):HIT=[-49.451]
(E,2,0,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 0, 0, 25)
0.6400000000000001 ( 2 2 0 0 25 ) -0.00999999999999801 0.0
0.16000000000000003 ( 2 2 0 1 25 ) -24.985 0.0
0.16000000000000003 ( 2 2 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 2 0 1 0 ) 0.0 50.0
(E,2,0,D,25):HIT=[-4.01]
Deciding optimal action for state: (2, 2, 0, 0, 50)
0.6400000000000001 ( 2 2 0 0 50 ) -0.00999999999999801 0.0
0.16000000000000003 ( 2 2 0 1 50 ) -24.985 0.0
0.16000000000000003 ( 2 2 0 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 2 0 1 0 ) 0.0 50.0
(E,2,0,D,50):HIT=[-4.01]
Deciding optimal action for state: (2, 2, 0, 0, 75)
(E,2,0,D,75):SHOOT=[-10.0]
Deciding optimal action for state: (2, 2, 0, 0, 100)
(E,2,0,D,100):SHOOT=[-10.0]
(E,2,0,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 0, 1, 25)
0.5 ( 2 2 0 0 50 ) -0.00999999999999801 0.0
(E,2,0,R,25):SHOOT=[-29.985]
Deciding optimal action for state: (2, 2, 0, 1, 50)
0.5 ( 2 2 0 0 75 ) -10.0 0.0
(E,2,0,R,50):SHOOT=[-34.975]
Deciding optimal action for state: (2, 2, 0, 1, 75)
0.5 ( 2 2 0 0 100 ) -10.0 0.0
(E,2,0,R,75):SHOOT=[-34.975]
Deciding optimal action for state: (2, 2, 0, 1, 100)
0.5 ( 2 2 0 0 100 ) -10.0 0.0
(E,2,0,R,100):SHOOT=[-34.975]
(E,2,1,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 1, 0, 25)
0.08000000000000002 ( 2 2 0 0 25 ) -0.00999999999999801 0.0
0.020000000000000004 ( 2 2 0 1 25 ) -24.985 0.0
0.7200000000000001 ( 2 2 0 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 2 0 1 0 ) 0.0 50.0
(E,2,1,D,25):SHOOT=[34.455]
Deciding optimal action for state: (2, 2, 1, 0, 50)
0.6400000000000001 ( 2 2 1 0 50 ) -0.00999999999999801 0.0
0.16000000000000003 ( 2 2 1 1 50 ) -24.985 0.0
0.16000000000000003 ( 2 2 1 0 0 ) 0.0 50.0
0.04000000000000001 ( 2 2 1 1 0 ) 0.0 50.0
(E,2,1,D,50):HIT=[-4.01]
Deciding optimal action for state: (2, 2, 1, 0, 75)
0.08000000000000002 ( 2 2 0 0 75 ) -10.0 0.0
0.020000000000000004 ( 2 2 0 1 75 ) -29.98 0.0
0.7200000000000001 ( 2 2 0 0 50 ) -0.00999999999999801 0.0
0.18000000000000002 ( 2 2 0 1 50 ) -24.985 0.0
(E,2,1,D,75):SHOOT=[-15.898]
Deciding optimal action for state: (2, 2, 1, 0, 100)
0.6400000000000001 ( 2 2 1 0 100 ) -10.0 0.0
0.16000000000000003 ( 2 2 1 1 100 ) -29.98 0.0
0.16000000000000003 ( 2 2 1 0 50 ) -0.00999999999999801 0.0
0.04000000000000001 ( 2 2 1 1 50 ) -24.985 0.0
(E,2,1,D,100):HIT=[-22.186]
(E,2,1,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 1, 1, 25)
0.05 ( 2 2 0 1 25 ) -24.985 0.0
0.45 ( 2 2 0 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 50 ) -0.00999999999999801 0.0
(E,2,1,R,25):SHOOT=[-8.755]
Deciding optimal action for state: (2, 2, 1, 1, 50)
0.4 ( 2 2 1 1 50 ) -24.985 0.0
0.1 ( 2 2 1 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 75 ) -10.0 0.0
(E,2,1,R,50):HIT=[-39.964]
Deciding optimal action for state: (2, 2, 1, 1, 75)
0.05 ( 2 2 0 1 75 ) -29.98 0.0
0.45 ( 2 2 0 1 50 ) -24.985 0.0
0.5 ( 2 2 0 0 100 ) -10.0 0.0
(E,2,1,R,75):SHOOT=[-47.705]
Deciding optimal action for state: (2, 2, 1, 1, 100)
0.4 ( 2 2 1 1 100 ) -29.98 0.0
0.1 ( 2 2 1 1 50 ) -24.985 0.0
0.5 ( 2 2 0 0 100 ) -10.0 0.0
(E,2,1,R,100):HIT=[-49.451]
(E,2,2,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 2, 0, 25)
0.08000000000000002 ( 2 2 1 0 25 ) 34.955000000000005 0.0
0.020000000000000004 ( 2 2 1 1 25 ) -7.5024999999999995 0.0
0.7200000000000001 ( 2 2 1 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 2 1 1 0 ) 0.0 50.0
(E,2,2,D,25):SHOOT=[37.599]
Deciding optimal action for state: (2, 2, 2, 0, 50)
0.08000000000000002 ( 2 2 1 0 50 ) -0.00999999999999801 0.0
0.020000000000000004 ( 2 2 1 1 50 ) -24.985 0.0
0.7200000000000001 ( 2 2 1 0 25 ) 34.955000000000005 0.0
0.18000000000000002 ( 2 2 1 1 25 ) -7.5024999999999995 0.0
(E,2,2,D,50):SHOOT=[13.293]
Deciding optimal action for state: (2, 2, 2, 0, 75)
0.08000000000000002 ( 2 2 1 0 75 ) -10.0 0.0
0.020000000000000004 ( 2 2 1 1 75 ) -29.98 0.0
0.7200000000000001 ( 2 2 1 0 50 ) -0.00999999999999801 0.0
0.18000000000000002 ( 2 2 1 1 50 ) -24.985 0.0
(E,2,2,D,75):SHOOT=[-15.898]
Deciding optimal action for state: (2, 2, 2, 0, 100)
0.6400000000000001 ( 2 2 2 0 100 ) -10.0 0.0
0.16000000000000003 ( 2 2 2 1 100 ) -29.98 0.0
0.16000000000000003 ( 2 2 2 0 50 ) -0.00999999999999801 0.0
0.04000000000000001 ( 2 2 2 1 50 ) -24.985 0.0
(E,2,2,D,100):HIT=[-22.186]
(E,2,2,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 2, 1, 25)
0.05 ( 2 2 1 1 25 ) -7.5024999999999995 0.0
0.45 ( 2 2 1 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 50 ) -0.00999999999999801 0.0
(E,2,2,R,25):SHOOT=[-7.882]
Deciding optimal action for state: (2, 2, 2, 1, 50)
0.05 ( 2 2 1 1 50 ) -24.985 0.0
0.45 ( 2 2 1 1 25 ) -7.5024999999999995 0.0
0.5 ( 2 2 0 0 75 ) -10.0 0.0
(E,2,2,R,50):SHOOT=[-39.596]
Deciding optimal action for state: (2, 2, 2, 1, 75)
0.05 ( 2 2 1 1 75 ) -29.98 0.0
0.45 ( 2 2 1 1 50 ) -24.985 0.0
0.5 ( 2 2 0 0 100 ) -10.0 0.0
(E,2,2,R,75):SHOOT=[-47.705]
Deciding optimal action for state: (2, 2, 2, 1, 100)
0.4 ( 2 2 2 1 100 ) -29.98 0.0
0.1 ( 2 2 2 1 50 ) -24.985 0.0
0.5 ( 2 2 0 0 100 ) -10.0 0.0
(E,2,2,R,100):HIT=[-49.451]
(E,2,3,D,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 3, 0, 25)
0.08000000000000002 ( 2 2 2 0 25 ) 34.955000000000005 0.0
0.020000000000000004 ( 2 2 2 1 25 ) -7.5024999999999995 0.0
0.7200000000000001 ( 2 2 2 0 0 ) 0.0 50.0
0.18000000000000002 ( 2 2 2 1 0 ) 0.0 50.0
(E,2,3,D,25):SHOOT=[37.599]
Deciding optimal action for state: (2, 2, 3, 0, 50)
0.08000000000000002 ( 2 2 2 0 50 ) -0.00999999999999801 0.0
0.020000000000000004 ( 2 2 2 1 50 ) -24.985 0.0
0.7200000000000001 ( 2 2 2 0 25 ) 34.955000000000005 0.0
0.18000000000000002 ( 2 2 2 1 25 ) -7.5024999999999995 0.0
(E,2,3,D,50):SHOOT=[13.293]
Deciding optimal action for state: (2, 2, 3, 0, 75)
0.08000000000000002 ( 2 2 2 0 75 ) -10.0 0.0
0.020000000000000004 ( 2 2 2 1 75 ) -29.98 0.0
0.7200000000000001 ( 2 2 2 0 50 ) -0.00999999999999801 0.0
0.18000000000000002 ( 2 2 2 1 50 ) -24.985 0.0
(E,2,3,D,75):SHOOT=[-15.898]
Deciding optimal action for state: (2, 2, 3, 0, 100)
0.6400000000000001 ( 2 2 3 0 100 ) -10.0 0.0
0.16000000000000003 ( 2 2 3 1 100 ) -29.98 0.0
0.16000000000000003 ( 2 2 3 0 50 ) -0.00999999999999801 0.0
0.04000000000000001 ( 2 2 3 1 50 ) -24.985 0.0
(E,2,3,D,100):HIT=[-22.186]
(E,2,3,R,0):NONE=[0.000]
Deciding optimal action for state: (2, 2, 3, 1, 25)
0.05 ( 2 2 2 1 25 ) -7.5024999999999995 0.0
0.45 ( 2 2 2 1 0 ) 0.0 50.0
0.5 ( 2 2 0 0 50 ) -0.00999999999999801 0.0
(E,2,3,R,25):SHOOT=[-7.882]
Deciding optimal action for state: (2, 2, 3, 1, 50)
0.05 ( 2 2 2 1 50 ) -24.985 0.0
0.45 ( 2 2 2 1 25 ) -7.5024999999999995 0.0
0.5 ( 2 2 0 0 75 ) -10.0 0.0
(E,2,3,R,50):SHOOT=[-39.596]
Deciding optimal action for state: (2, 2, 3, 1, 75)
0.05 ( 2 2 2 1 75 ) -29.98 0.0
0.45 ( 2 2 2 1 50 ) -24.985 0.0
0.5 ( 2 2 0 0 100 ) -10.0 0.0
(E,2,3,R,75):SHOOT=[-47.705]
Deciding optimal action for state: (2, 2, 3, 1, 100)
0.4 ( 2 2 3 1 100 ) -29.98 0.0
0.1 ( 2 2 3 1 50 ) -24.985 0.0
0.5 ( 2 2 0 0 100 ) -10.0 0.0
(E,2,3,R,100):HIT=[-49.451]
(S,0,0,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 0, 0, 25)
0.68 ( 4 0 0 0 25 ) -5.004999999999999 0.0
0.17 ( 4 0 0 1 25 ) -27.4825 0.0
0.12 ( 2 0 0 0 25 ) -0.00999999999999801 0.0
0.03 ( 2 0 0 1 25 ) -24.985 0.0
(S,0,0,D,25):UP=[-18.817]
Deciding optimal action for state: (3, 0, 0, 0, 50)
0.68 ( 4 0 0 0 50 ) -5.004999999999999 0.0
0.17 ( 4 0 0 1 50 ) -27.4825 0.0
0.12 ( 2 0 0 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 0 0 1 50 ) -24.985 0.0
(S,0,0,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 0, 0, 0, 75)
0.68 ( 4 0 0 0 75 ) -10.0 0.0
0.17 ( 4 0 0 1 75 ) -29.98 0.0
0.12 ( 2 0 0 0 75 ) -10.0 0.0
0.03 ( 2 0 0 1 75 ) -29.98 0.0
(S,0,0,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 0, 0, 0, 100)
0.68 ( 4 0 0 0 100 ) -10.0 0.0
0.17 ( 4 0 0 1 100 ) -29.98 0.0
0.12 ( 2 0 0 0 100 ) -10.0 0.0
0.03 ( 2 0 0 1 100 ) -29.98 0.0
(S,0,0,D,100):UP=[-23.982]
(S,0,0,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 0, 1, 25)
0.125 ( 3 0 0 1 25 ) -10.0 0.0
0.125 ( 3 0 0 0 25 ) -10.0 0.0
0.375 ( 3 1 0 1 25 ) -10.0 0.0
0.375 ( 3 1 0 0 25 ) -10.0 0.0
(S,0,0,R,25):GATHER=[-19.99]
Deciding optimal action for state: (3, 0, 0, 1, 50)
0.125 ( 3 0 0 1 50 ) -10.0 0.0
0.125 ( 3 0 0 0 50 ) -10.0 0.0
0.375 ( 3 1 0 1 50 ) -10.0 0.0
0.375 ( 3 1 0 0 50 ) -10.0 0.0
(S,0,0,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 0, 0, 1, 75)
0.125 ( 3 0 0 1 75 ) -10.0 0.0
0.125 ( 3 0 0 0 75 ) -10.0 0.0
0.375 ( 3 1 0 1 75 ) -10.0 0.0
0.375 ( 3 1 0 0 75 ) -10.0 0.0
(S,0,0,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 0, 0, 1, 100)
0.125 ( 3 0 0 1 100 ) -10.0 0.0
0.125 ( 3 0 0 0 100 ) -10.0 0.0
0.375 ( 3 1 0 1 100 ) -10.0 0.0
0.375 ( 3 1 0 0 100 ) -10.0 0.0
(S,0,0,R,100):GATHER=[-19.99]
(S,0,1,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 1, 0, 25)
0.68 ( 4 0 1 0 25 ) 14.975000000000001 0.0
0.17 ( 4 0 1 1 25 ) -17.4925 0.0
0.12 ( 2 0 1 0 25 ) 34.955000000000005 0.0
0.03 ( 2 0 1 1 25 ) -7.5024999999999995 0.0
(S,0,1,D,25):UP=[1.168]
Deciding optimal action for state: (3, 0, 1, 0, 50)
0.68 ( 4 0 1 0 50 ) -5.004999999999999 0.0
0.17 ( 4 0 1 1 50 ) -27.4825 0.0
0.12 ( 2 0 1 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 0 1 1 50 ) -24.985 0.0
(S,0,1,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 0, 1, 0, 75)
0.68 ( 4 0 1 0 75 ) -10.0 0.0
0.17 ( 4 0 1 1 75 ) -29.98 0.0
0.12 ( 2 0 1 0 75 ) -10.0 0.0
0.03 ( 2 0 1 1 75 ) -29.98 0.0
(S,0,1,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 0, 1, 0, 100)
0.68 ( 4 0 1 0 100 ) -10.0 0.0
0.17 ( 4 0 1 1 100 ) -29.98 0.0
0.12 ( 2 0 1 0 100 ) -10.0 0.0
0.03 ( 2 0 1 1 100 ) -29.98 0.0
(S,0,1,D,100):UP=[-23.982]
(S,0,1,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 1, 1, 25)
0.425 ( 3 0 1 1 25 ) -10.0 0.0
0.425 ( 3 0 1 0 25 ) -10.0 0.0
0.075 ( 2 0 1 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 0 1 0 25 ) 34.955000000000005 0.0
(S,0,1,R,25):STAY=[-16.435]
Deciding optimal action for state: (3, 0, 1, 1, 50)
0.125 ( 3 0 1 1 50 ) -10.0 0.0
0.125 ( 3 0 1 0 50 ) -10.0 0.0
0.375 ( 3 1 1 1 50 ) -10.0 0.0
0.375 ( 3 1 1 0 50 ) -10.0 0.0
(S,0,1,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 0, 1, 1, 75)
0.125 ( 3 0 1 1 75 ) -10.0 0.0
0.125 ( 3 0 1 0 75 ) -10.0 0.0
0.375 ( 3 1 1 1 75 ) -10.0 0.0
0.375 ( 3 1 1 0 75 ) -10.0 0.0
(S,0,1,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 0, 1, 1, 100)
0.125 ( 3 0 1 1 100 ) -10.0 0.0
0.125 ( 3 0 1 0 100 ) -10.0 0.0
0.375 ( 3 1 1 1 100 ) -10.0 0.0
0.375 ( 3 1 1 0 100 ) -10.0 0.0
(S,0,1,R,100):GATHER=[-19.99]
(S,0,2,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 2, 0, 25)
0.68 ( 4 0 2 0 25 ) 14.975000000000001 0.0
0.17 ( 4 0 2 1 25 ) -17.4925 0.0
0.12 ( 2 0 2 0 25 ) 34.955000000000005 0.0
0.03 ( 2 0 2 1 25 ) -7.5024999999999995 0.0
(S,0,2,D,25):UP=[1.168]
Deciding optimal action for state: (3, 0, 2, 0, 50)
0.68 ( 4 0 2 0 50 ) -5.004999999999999 0.0
0.17 ( 4 0 2 1 50 ) -27.4825 0.0
0.12 ( 2 0 2 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 0 2 1 50 ) -24.985 0.0
(S,0,2,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 0, 2, 0, 75)
0.68 ( 4 0 2 0 75 ) -10.0 0.0
0.17 ( 4 0 2 1 75 ) -29.98 0.0
0.12 ( 2 0 2 0 75 ) -10.0 0.0
0.03 ( 2 0 2 1 75 ) -29.98 0.0
(S,0,2,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 0, 2, 0, 100)
0.68 ( 4 0 2 0 100 ) -10.0 0.0
0.17 ( 4 0 2 1 100 ) -29.98 0.0
0.12 ( 2 0 2 0 100 ) -10.0 0.0
0.03 ( 2 0 2 1 100 ) -29.98 0.0
(S,0,2,D,100):UP=[-23.982]
(S,0,2,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 2, 1, 25)
0.425 ( 3 0 2 1 25 ) -10.0 0.0
0.425 ( 3 0 2 0 25 ) -10.0 0.0
0.075 ( 2 0 2 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 0 2 0 25 ) 34.955000000000005 0.0
(S,0,2,R,25):STAY=[-16.435]
Deciding optimal action for state: (3, 0, 2, 1, 50)
0.125 ( 3 0 2 1 50 ) -10.0 0.0
0.125 ( 3 0 2 0 50 ) -10.0 0.0
0.375 ( 3 1 2 1 50 ) -10.0 0.0
0.375 ( 3 1 2 0 50 ) -10.0 0.0
(S,0,2,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 0, 2, 1, 75)
0.125 ( 3 0 2 1 75 ) -10.0 0.0
0.125 ( 3 0 2 0 75 ) -10.0 0.0
0.375 ( 3 1 2 1 75 ) -10.0 0.0
0.375 ( 3 1 2 0 75 ) -10.0 0.0
(S,0,2,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 0, 2, 1, 100)
0.125 ( 3 0 2 1 100 ) -10.0 0.0
0.125 ( 3 0 2 0 100 ) -10.0 0.0
0.375 ( 3 1 2 1 100 ) -10.0 0.0
0.375 ( 3 1 2 0 100 ) -10.0 0.0
(S,0,2,R,100):GATHER=[-19.99]
(S,0,3,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 3, 0, 25)
0.68 ( 4 0 3 0 25 ) 14.975000000000001 0.0
0.17 ( 4 0 3 1 25 ) -17.4925 0.0
0.12 ( 2 0 3 0 25 ) 34.955000000000005 0.0
0.03 ( 2 0 3 1 25 ) -7.5024999999999995 0.0
(S,0,3,D,25):UP=[1.168]
Deciding optimal action for state: (3, 0, 3, 0, 50)
0.68 ( 4 0 3 0 50 ) -5.004999999999999 0.0
0.17 ( 4 0 3 1 50 ) -27.4825 0.0
0.12 ( 2 0 3 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 0 3 1 50 ) -24.985 0.0
(S,0,3,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 0, 3, 0, 75)
0.68 ( 4 0 3 0 75 ) -10.0 0.0
0.17 ( 4 0 3 1 75 ) -29.98 0.0
0.12 ( 2 0 3 0 75 ) -10.0 0.0
0.03 ( 2 0 3 1 75 ) -29.98 0.0
(S,0,3,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 0, 3, 0, 100)
0.68 ( 4 0 3 0 100 ) -10.0 0.0
0.17 ( 4 0 3 1 100 ) -29.98 0.0
0.12 ( 2 0 3 0 100 ) -10.0 0.0
0.03 ( 2 0 3 1 100 ) -29.98 0.0
(S,0,3,D,100):UP=[-23.982]
(S,0,3,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 0, 3, 1, 25)
0.425 ( 3 0 3 1 25 ) -10.0 0.0
0.425 ( 3 0 3 0 25 ) -10.0 0.0
0.075 ( 2 0 3 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 0 3 0 25 ) 34.955000000000005 0.0
(S,0,3,R,25):STAY=[-16.435]
Deciding optimal action for state: (3, 0, 3, 1, 50)
0.125 ( 3 0 3 1 50 ) -10.0 0.0
0.125 ( 3 0 3 0 50 ) -10.0 0.0
0.375 ( 3 1 3 1 50 ) -10.0 0.0
0.375 ( 3 1 3 0 50 ) -10.0 0.0
(S,0,3,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 0, 3, 1, 75)
0.125 ( 3 0 3 1 75 ) -10.0 0.0
0.125 ( 3 0 3 0 75 ) -10.0 0.0
0.375 ( 3 1 3 1 75 ) -10.0 0.0
0.375 ( 3 1 3 0 75 ) -10.0 0.0
(S,0,3,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 0, 3, 1, 100)
0.125 ( 3 0 3 1 100 ) -10.0 0.0
0.125 ( 3 0 3 0 100 ) -10.0 0.0
0.375 ( 3 1 3 1 100 ) -10.0 0.0
0.375 ( 3 1 3 0 100 ) -10.0 0.0
(S,0,3,R,100):GATHER=[-19.99]
(S,1,0,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 0, 0, 25)
0.68 ( 4 1 0 0 25 ) -5.004999999999999 0.0
0.17 ( 4 1 0 1 25 ) -27.4825 0.0
0.12 ( 2 1 0 0 25 ) -0.00999999999999801 0.0
0.03 ( 2 1 0 1 25 ) -24.985 0.0
(S,1,0,D,25):UP=[-18.817]
Deciding optimal action for state: (3, 1, 0, 0, 50)
0.68 ( 4 1 0 0 50 ) -5.004999999999999 0.0
0.17 ( 4 1 0 1 50 ) -27.4825 0.0
0.12 ( 2 1 0 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 1 0 1 50 ) -24.985 0.0
(S,1,0,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 1, 0, 0, 75)
0.68 ( 4 1 0 0 75 ) -10.0 0.0
0.17 ( 4 1 0 1 75 ) -29.98 0.0
0.12 ( 2 1 0 0 75 ) -10.0 0.0
0.03 ( 2 1 0 1 75 ) -29.98 0.0
(S,1,0,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 1, 0, 0, 100)
0.68 ( 4 1 0 0 100 ) -10.0 0.0
0.17 ( 4 1 0 1 100 ) -29.98 0.0
0.12 ( 2 1 0 0 100 ) -10.0 0.0
0.03 ( 2 1 0 1 100 ) -29.98 0.0
(S,1,0,D,100):UP=[-23.982]
(S,1,0,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 0, 1, 25)
0.125 ( 3 1 0 1 25 ) -10.0 0.0
0.125 ( 3 1 0 0 25 ) -10.0 0.0
0.375 ( 3 2 0 1 25 ) -10.0 0.0
0.375 ( 3 2 0 0 25 ) -10.0 0.0
(S,1,0,R,25):GATHER=[-19.99]
Deciding optimal action for state: (3, 1, 0, 1, 50)
0.125 ( 3 1 0 1 50 ) -10.0 0.0
0.125 ( 3 1 0 0 50 ) -10.0 0.0
0.375 ( 3 2 0 1 50 ) -10.0 0.0
0.375 ( 3 2 0 0 50 ) -10.0 0.0
(S,1,0,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 1, 0, 1, 75)
0.125 ( 3 1 0 1 75 ) -10.0 0.0
0.125 ( 3 1 0 0 75 ) -10.0 0.0
0.375 ( 3 2 0 1 75 ) -10.0 0.0
0.375 ( 3 2 0 0 75 ) -10.0 0.0
(S,1,0,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 1, 0, 1, 100)
0.125 ( 3 1 0 1 100 ) -10.0 0.0
0.125 ( 3 1 0 0 100 ) -10.0 0.0
0.375 ( 3 2 0 1 100 ) -10.0 0.0
0.375 ( 3 2 0 0 100 ) -10.0 0.0
(S,1,0,R,100):GATHER=[-19.99]
(S,1,1,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 1, 0, 25)
0.68 ( 4 1 1 0 25 ) 14.975000000000001 0.0
0.17 ( 4 1 1 1 25 ) -17.4925 0.0
0.12 ( 2 1 1 0 25 ) 34.955000000000005 0.0
0.03 ( 2 1 1 1 25 ) -7.5024999999999995 0.0
(S,1,1,D,25):UP=[1.168]
Deciding optimal action for state: (3, 1, 1, 0, 50)
0.68 ( 4 1 1 0 50 ) -5.004999999999999 0.0
0.17 ( 4 1 1 1 50 ) -27.4825 0.0
0.12 ( 2 1 1 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 1 1 1 50 ) -24.985 0.0
(S,1,1,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 1, 1, 0, 75)
0.68 ( 4 1 1 0 75 ) -10.0 0.0
0.17 ( 4 1 1 1 75 ) -29.98 0.0
0.12 ( 2 1 1 0 75 ) -10.0 0.0
0.03 ( 2 1 1 1 75 ) -29.98 0.0
(S,1,1,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 1, 1, 0, 100)
0.68 ( 4 1 1 0 100 ) -10.0 0.0
0.17 ( 4 1 1 1 100 ) -29.98 0.0
0.12 ( 2 1 1 0 100 ) -10.0 0.0
0.03 ( 2 1 1 1 100 ) -29.98 0.0
(S,1,1,D,100):UP=[-23.982]
(S,1,1,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 1, 1, 25)
0.425 ( 3 1 1 1 25 ) -10.0 0.0
0.425 ( 3 1 1 0 25 ) -10.0 0.0
0.075 ( 2 1 1 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 1 1 0 25 ) 34.955000000000005 0.0
(S,1,1,R,25):STAY=[-16.435]
Deciding optimal action for state: (3, 1, 1, 1, 50)
0.125 ( 3 1 1 1 50 ) -10.0 0.0
0.125 ( 3 1 1 0 50 ) -10.0 0.0
0.375 ( 3 2 1 1 50 ) -10.0 0.0
0.375 ( 3 2 1 0 50 ) -10.0 0.0
(S,1,1,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 1, 1, 1, 75)
0.125 ( 3 1 1 1 75 ) -10.0 0.0
0.125 ( 3 1 1 0 75 ) -10.0 0.0
0.375 ( 3 2 1 1 75 ) -10.0 0.0
0.375 ( 3 2 1 0 75 ) -10.0 0.0
(S,1,1,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 1, 1, 1, 100)
0.125 ( 3 1 1 1 100 ) -10.0 0.0
0.125 ( 3 1 1 0 100 ) -10.0 0.0
0.375 ( 3 2 1 1 100 ) -10.0 0.0
0.375 ( 3 2 1 0 100 ) -10.0 0.0
(S,1,1,R,100):GATHER=[-19.99]
(S,1,2,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 2, 0, 25)
0.68 ( 4 1 2 0 25 ) 14.975000000000001 0.0
0.17 ( 4 1 2 1 25 ) -17.4925 0.0
0.12 ( 2 1 2 0 25 ) 34.955000000000005 0.0
0.03 ( 2 1 2 1 25 ) -7.5024999999999995 0.0
(S,1,2,D,25):UP=[1.168]
Deciding optimal action for state: (3, 1, 2, 0, 50)
0.68 ( 4 1 2 0 50 ) -5.004999999999999 0.0
0.17 ( 4 1 2 1 50 ) -27.4825 0.0
0.12 ( 2 1 2 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 1 2 1 50 ) -24.985 0.0
(S,1,2,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 1, 2, 0, 75)
0.68 ( 4 1 2 0 75 ) -10.0 0.0
0.17 ( 4 1 2 1 75 ) -29.98 0.0
0.12 ( 2 1 2 0 75 ) -10.0 0.0
0.03 ( 2 1 2 1 75 ) -29.98 0.0
(S,1,2,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 1, 2, 0, 100)
0.68 ( 4 1 2 0 100 ) -10.0 0.0
0.17 ( 4 1 2 1 100 ) -29.98 0.0
0.12 ( 2 1 2 0 100 ) -10.0 0.0
0.03 ( 2 1 2 1 100 ) -29.98 0.0
(S,1,2,D,100):UP=[-23.982]
(S,1,2,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 2, 1, 25)
0.425 ( 3 1 2 1 25 ) -10.0 0.0
0.425 ( 3 1 2 0 25 ) -10.0 0.0
0.075 ( 2 1 2 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 1 2 0 25 ) 34.955000000000005 0.0
(S,1,2,R,25):STAY=[-16.435]
Deciding optimal action for state: (3, 1, 2, 1, 50)
0.125 ( 3 1 2 1 50 ) -10.0 0.0
0.125 ( 3 1 2 0 50 ) -10.0 0.0
0.375 ( 3 2 2 1 50 ) -10.0 0.0
0.375 ( 3 2 2 0 50 ) -10.0 0.0
(S,1,2,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 1, 2, 1, 75)
0.125 ( 3 1 2 1 75 ) -10.0 0.0
0.125 ( 3 1 2 0 75 ) -10.0 0.0
0.375 ( 3 2 2 1 75 ) -10.0 0.0
0.375 ( 3 2 2 0 75 ) -10.0 0.0
(S,1,2,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 1, 2, 1, 100)
0.125 ( 3 1 2 1 100 ) -10.0 0.0
0.125 ( 3 1 2 0 100 ) -10.0 0.0
0.375 ( 3 2 2 1 100 ) -10.0 0.0
0.375 ( 3 2 2 0 100 ) -10.0 0.0
(S,1,2,R,100):GATHER=[-19.99]
(S,1,3,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 3, 0, 25)
0.68 ( 4 1 3 0 25 ) 14.975000000000001 0.0
0.17 ( 4 1 3 1 25 ) -17.4925 0.0
0.12 ( 2 1 3 0 25 ) 34.955000000000005 0.0
0.03 ( 2 1 3 1 25 ) -7.5024999999999995 0.0
(S,1,3,D,25):UP=[1.168]
Deciding optimal action for state: (3, 1, 3, 0, 50)
0.68 ( 4 1 3 0 50 ) -5.004999999999999 0.0
0.17 ( 4 1 3 1 50 ) -27.4825 0.0
0.12 ( 2 1 3 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 1 3 1 50 ) -24.985 0.0
(S,1,3,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 1, 3, 0, 75)
0.68 ( 4 1 3 0 75 ) -10.0 0.0
0.17 ( 4 1 3 1 75 ) -29.98 0.0
0.12 ( 2 1 3 0 75 ) -10.0 0.0
0.03 ( 2 1 3 1 75 ) -29.98 0.0
(S,1,3,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 1, 3, 0, 100)
0.68 ( 4 1 3 0 100 ) -10.0 0.0
0.17 ( 4 1 3 1 100 ) -29.98 0.0
0.12 ( 2 1 3 0 100 ) -10.0 0.0
0.03 ( 2 1 3 1 100 ) -29.98 0.0
(S,1,3,D,100):UP=[-23.982]
(S,1,3,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 1, 3, 1, 25)
0.425 ( 3 1 3 1 25 ) -10.0 0.0
0.425 ( 3 1 3 0 25 ) -10.0 0.0
0.075 ( 2 1 3 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 1 3 0 25 ) 34.955000000000005 0.0
(S,1,3,R,25):STAY=[-16.435]
Deciding optimal action for state: (3, 1, 3, 1, 50)
0.125 ( 3 1 3 1 50 ) -10.0 0.0
0.125 ( 3 1 3 0 50 ) -10.0 0.0
0.375 ( 3 2 3 1 50 ) -10.0 0.0
0.375 ( 3 2 3 0 50 ) -10.0 0.0
(S,1,3,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 1, 3, 1, 75)
0.125 ( 3 1 3 1 75 ) -10.0 0.0
0.125 ( 3 1 3 0 75 ) -10.0 0.0
0.375 ( 3 2 3 1 75 ) -10.0 0.0
0.375 ( 3 2 3 0 75 ) -10.0 0.0
(S,1,3,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 1, 3, 1, 100)
0.125 ( 3 1 3 1 100 ) -10.0 0.0
0.125 ( 3 1 3 0 100 ) -10.0 0.0
0.375 ( 3 2 3 1 100 ) -10.0 0.0
0.375 ( 3 2 3 0 100 ) -10.0 0.0
(S,1,3,R,100):GATHER=[-19.99]
(S,2,0,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 0, 0, 25)
0.68 ( 4 2 0 0 25 ) -5.004999999999999 0.0
0.17 ( 4 2 0 1 25 ) -27.4825 0.0
0.12 ( 2 2 0 0 25 ) -0.00999999999999801 0.0
0.03 ( 2 2 0 1 25 ) -24.985 0.0
(S,2,0,D,25):UP=[-18.817]
Deciding optimal action for state: (3, 2, 0, 0, 50)
0.68 ( 4 2 0 0 50 ) -5.004999999999999 0.0
0.17 ( 4 2 0 1 50 ) -27.4825 0.0
0.12 ( 2 2 0 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 2 0 1 50 ) -24.985 0.0
(S,2,0,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 2, 0, 0, 75)
0.68 ( 4 2 0 0 75 ) -10.0 0.0
0.17 ( 4 2 0 1 75 ) -29.98 0.0
0.12 ( 2 2 0 0 75 ) -10.0 0.0
0.03 ( 2 2 0 1 75 ) -29.98 0.0
(S,2,0,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 2, 0, 0, 100)
0.68 ( 4 2 0 0 100 ) -10.0 0.0
0.17 ( 4 2 0 1 100 ) -29.98 0.0
0.12 ( 2 2 0 0 100 ) -10.0 0.0
0.03 ( 2 2 0 1 100 ) -29.98 0.0
(S,2,0,D,100):UP=[-23.982]
(S,2,0,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 0, 1, 25)
0.125 ( 3 2 0 1 25 ) -10.0 0.0
0.125 ( 3 2 0 0 25 ) -10.0 0.0
0.375 ( 3 2 0 1 25 ) -10.0 0.0
0.375 ( 3 2 0 0 25 ) -10.0 0.0
(S,2,0,R,25):GATHER=[-19.99]
Deciding optimal action for state: (3, 2, 0, 1, 50)
0.125 ( 3 2 0 1 50 ) -10.0 0.0
0.125 ( 3 2 0 0 50 ) -10.0 0.0
0.375 ( 3 2 0 1 50 ) -10.0 0.0
0.375 ( 3 2 0 0 50 ) -10.0 0.0
(S,2,0,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 2, 0, 1, 75)
0.125 ( 3 2 0 1 75 ) -10.0 0.0
0.125 ( 3 2 0 0 75 ) -10.0 0.0
0.375 ( 3 2 0 1 75 ) -10.0 0.0
0.375 ( 3 2 0 0 75 ) -10.0 0.0
(S,2,0,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 2, 0, 1, 100)
0.125 ( 3 2 0 1 100 ) -10.0 0.0
0.125 ( 3 2 0 0 100 ) -10.0 0.0
0.375 ( 3 2 0 1 100 ) -10.0 0.0
0.375 ( 3 2 0 0 100 ) -10.0 0.0
(S,2,0,R,100):GATHER=[-19.99]
(S,2,1,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 1, 0, 25)
0.68 ( 4 2 1 0 25 ) 14.975000000000001 0.0
0.17 ( 4 2 1 1 25 ) -17.4925 0.0
0.12 ( 2 2 1 0 25 ) 34.955000000000005 0.0
0.03 ( 2 2 1 1 25 ) -7.5024999999999995 0.0
(S,2,1,D,25):UP=[1.168]
Deciding optimal action for state: (3, 2, 1, 0, 50)
0.68 ( 4 2 1 0 50 ) -5.004999999999999 0.0
0.17 ( 4 2 1 1 50 ) -27.4825 0.0
0.12 ( 2 2 1 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 2 1 1 50 ) -24.985 0.0
(S,2,1,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 2, 1, 0, 75)
0.68 ( 4 2 1 0 75 ) -10.0 0.0
0.17 ( 4 2 1 1 75 ) -29.98 0.0
0.12 ( 2 2 1 0 75 ) -10.0 0.0
0.03 ( 2 2 1 1 75 ) -29.98 0.0
(S,2,1,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 2, 1, 0, 100)
0.68 ( 4 2 1 0 100 ) -10.0 0.0
0.17 ( 4 2 1 1 100 ) -29.98 0.0
0.12 ( 2 2 1 0 100 ) -10.0 0.0
0.03 ( 2 2 1 1 100 ) -29.98 0.0
(S,2,1,D,100):UP=[-23.982]
(S,2,1,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 1, 1, 25)
0.425 ( 3 2 1 1 25 ) -10.0 0.0
0.425 ( 3 2 1 0 25 ) -10.0 0.0
0.075 ( 2 2 1 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 2 1 0 25 ) 34.955000000000005 0.0
(S,2,1,R,25):STAY=[-16.435]
Deciding optimal action for state: (3, 2, 1, 1, 50)
0.125 ( 3 2 1 1 50 ) -10.0 0.0
0.125 ( 3 2 1 0 50 ) -10.0 0.0
0.375 ( 3 2 1 1 50 ) -10.0 0.0
0.375 ( 3 2 1 0 50 ) -10.0 0.0
(S,2,1,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 2, 1, 1, 75)
0.125 ( 3 2 1 1 75 ) -10.0 0.0
0.125 ( 3 2 1 0 75 ) -10.0 0.0
0.375 ( 3 2 1 1 75 ) -10.0 0.0
0.375 ( 3 2 1 0 75 ) -10.0 0.0
(S,2,1,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 2, 1, 1, 100)
0.125 ( 3 2 1 1 100 ) -10.0 0.0
0.125 ( 3 2 1 0 100 ) -10.0 0.0
0.375 ( 3 2 1 1 100 ) -10.0 0.0
0.375 ( 3 2 1 0 100 ) -10.0 0.0
(S,2,1,R,100):GATHER=[-19.99]
(S,2,2,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 2, 0, 25)
0.68 ( 4 2 2 0 25 ) 14.975000000000001 0.0
0.17 ( 4 2 2 1 25 ) -17.4925 0.0
0.12 ( 2 2 2 0 25 ) 34.955000000000005 0.0
0.03 ( 2 2 2 1 25 ) -7.5024999999999995 0.0
(S,2,2,D,25):UP=[1.168]
Deciding optimal action for state: (3, 2, 2, 0, 50)
0.68 ( 4 2 2 0 50 ) -5.004999999999999 0.0
0.17 ( 4 2 2 1 50 ) -27.4825 0.0
0.12 ( 2 2 2 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 2 2 1 50 ) -24.985 0.0
(S,2,2,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 2, 2, 0, 75)
0.68 ( 4 2 2 0 75 ) -10.0 0.0
0.17 ( 4 2 2 1 75 ) -29.98 0.0
0.12 ( 2 2 2 0 75 ) -10.0 0.0
0.03 ( 2 2 2 1 75 ) -29.98 0.0
(S,2,2,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 2, 2, 0, 100)
0.68 ( 4 2 2 0 100 ) -10.0 0.0
0.17 ( 4 2 2 1 100 ) -29.98 0.0
0.12 ( 2 2 2 0 100 ) -10.0 0.0
0.03 ( 2 2 2 1 100 ) -29.98 0.0
(S,2,2,D,100):UP=[-23.982]
(S,2,2,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 2, 1, 25)
0.425 ( 3 2 2 1 25 ) -10.0 0.0
0.425 ( 3 2 2 0 25 ) -10.0 0.0
0.075 ( 2 2 2 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 2 2 0 25 ) 34.955000000000005 0.0
(S,2,2,R,25):STAY=[-16.435]
Deciding optimal action for state: (3, 2, 2, 1, 50)
0.125 ( 3 2 2 1 50 ) -10.0 0.0
0.125 ( 3 2 2 0 50 ) -10.0 0.0
0.375 ( 3 2 2 1 50 ) -10.0 0.0
0.375 ( 3 2 2 0 50 ) -10.0 0.0
(S,2,2,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 2, 2, 1, 75)
0.125 ( 3 2 2 1 75 ) -10.0 0.0
0.125 ( 3 2 2 0 75 ) -10.0 0.0
0.375 ( 3 2 2 1 75 ) -10.0 0.0
0.375 ( 3 2 2 0 75 ) -10.0 0.0
(S,2,2,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 2, 2, 1, 100)
0.125 ( 3 2 2 1 100 ) -10.0 0.0
0.125 ( 3 2 2 0 100 ) -10.0 0.0
0.375 ( 3 2 2 1 100 ) -10.0 0.0
0.375 ( 3 2 2 0 100 ) -10.0 0.0
(S,2,2,R,100):GATHER=[-19.99]
(S,2,3,D,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 3, 0, 25)
0.68 ( 4 2 3 0 25 ) 14.975000000000001 0.0
0.17 ( 4 2 3 1 25 ) -17.4925 0.0
0.12 ( 2 2 3 0 25 ) 34.955000000000005 0.0
0.03 ( 2 2 3 1 25 ) -7.5024999999999995 0.0
(S,2,3,D,25):UP=[1.168]
Deciding optimal action for state: (3, 2, 3, 0, 50)
0.68 ( 4 2 3 0 50 ) -5.004999999999999 0.0
0.17 ( 4 2 3 1 50 ) -27.4825 0.0
0.12 ( 2 2 3 0 50 ) -0.00999999999999801 0.0
0.03 ( 2 2 3 1 50 ) -24.985 0.0
(S,2,3,D,50):UP=[-18.817]
Deciding optimal action for state: (3, 2, 3, 0, 75)
0.68 ( 4 2 3 0 75 ) -10.0 0.0
0.17 ( 4 2 3 1 75 ) -29.98 0.0
0.12 ( 2 2 3 0 75 ) -10.0 0.0
0.03 ( 2 2 3 1 75 ) -29.98 0.0
(S,2,3,D,75):UP=[-23.982]
Deciding optimal action for state: (3, 2, 3, 0, 100)
0.68 ( 4 2 3 0 100 ) -10.0 0.0
0.17 ( 4 2 3 1 100 ) -29.98 0.0
0.12 ( 2 2 3 0 100 ) -10.0 0.0
0.03 ( 2 2 3 1 100 ) -29.98 0.0
(S,2,3,D,100):UP=[-23.982]
(S,2,3,R,0):NONE=[0.000]
Deciding optimal action for state: (3, 2, 3, 1, 25)
0.425 ( 3 2 3 1 25 ) -10.0 0.0
0.425 ( 3 2 3 0 25 ) -10.0 0.0
0.075 ( 2 2 3 1 25 ) -7.5024999999999995 0.0
0.075 ( 2 2 3 0 25 ) 34.955000000000005 0.0
(S,2,3,R,25):STAY=[-16.435]
Deciding optimal action for state: (3, 2, 3, 1, 50)
0.125 ( 3 2 3 1 50 ) -10.0 0.0
0.125 ( 3 2 3 0 50 ) -10.0 0.0
0.375 ( 3 2 3 1 50 ) -10.0 0.0
0.375 ( 3 2 3 0 50 ) -10.0 0.0
(S,2,3,R,50):GATHER=[-19.99]
Deciding optimal action for state: (3, 2, 3, 1, 75)
0.125 ( 3 2 3 1 75 ) -10.0 0.0
0.125 ( 3 2 3 0 75 ) -10.0 0.0
0.375 ( 3 2 3 1 75 ) -10.0 0.0
0.375 ( 3 2 3 0 75 ) -10.0 0.0
(S,2,3,R,75):GATHER=[-19.99]
Deciding optimal action for state: (3, 2, 3, 1, 100)
0.125 ( 3 2 3 1 100 ) -10.0 0.0
0.125 ( 3 2 3 0 100 ) -10.0 0.0
0.375 ( 3 2 3 1 100 ) -10.0 0.0
0.375 ( 3 2 3 0 100 ) -10.0 0.0
(S,2,3,R,100):GATHER=[-19.99]
(C,0,0,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 0, 0, 25)
0.7200000000000001 ( 4 0 0 0 25 ) -5.004999999999999 0.0
0.18000000000000002 ( 4 0 0 1 25 ) -27.4825 0.0
0.08000000000000002 ( 4 0 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 0 0 1 0 ) 0.0 50.0
(C,0,0,D,25):HIT=[-13.547]
Deciding optimal action for state: (4, 0, 0, 0, 50)
0.7200000000000001 ( 4 0 0 0 50 ) -5.004999999999999 0.0
0.18000000000000002 ( 4 0 0 1 50 ) -27.4825 0.0
0.08000000000000002 ( 4 0 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 0 0 1 0 ) 0.0 50.0
(C,0,0,D,50):HIT=[-13.547]
Deciding optimal action for state: (4, 0, 0, 0, 75)
(C,0,0,D,75):SHOOT=[-10.0]
Deciding optimal action for state: (4, 0, 0, 0, 100)
(C,0,0,D,100):SHOOT=[-10.0]
(C,0,0,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 0, 1, 25)
0.5 ( 4 0 0 0 50 ) -5.004999999999999 0.0
(C,0,0,R,25):SHOOT=[-32.48]
Deciding optimal action for state: (4, 0, 0, 1, 50)
0.5 ( 4 0 0 0 75 ) -10.0 0.0
(C,0,0,R,50):SHOOT=[-34.975]
Deciding optimal action for state: (4, 0, 0, 1, 75)
0.5 ( 4 0 0 0 100 ) -10.0 0.0
(C,0,0,R,75):SHOOT=[-34.975]
Deciding optimal action for state: (4, 0, 0, 1, 100)
0.5 ( 4 0 0 0 100 ) -10.0 0.0
(C,0,0,R,100):SHOOT=[-34.975]
(C,0,1,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 1, 0, 25)
0.68 ( 2 0 1 0 25 ) 34.955000000000005 0.0
0.17 ( 2 0 1 1 25 ) -7.5024999999999995 0.0
0.12 ( 2 0 1 0 25 ) 34.955000000000005 0.0
0.03 ( 2 0 1 1 25 ) -7.5024999999999995 0.0
(C,0,1,D,25):RIGHT=[16.437]
Deciding optimal action for state: (4, 0, 1, 0, 50)
0.7200000000000001 ( 4 0 1 0 50 ) -5.004999999999999 0.0
0.18000000000000002 ( 4 0 1 1 50 ) -27.4825 0.0
0.08000000000000002 ( 4 0 1 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 0 1 1 0 ) 0.0 50.0
(C,0,1,D,50):HIT=[-13.547]
Deciding optimal action for state: (4, 0, 1, 0, 75)
0.4 ( 4 0 0 0 75 ) -10.0 0.0
0.1 ( 4 0 0 1 75 ) -29.98 0.0
0.4 ( 4 0 0 0 50 ) -5.004999999999999 0.0
0.1 ( 4 0 0 1 50 ) -27.4825 0.0
(C,0,1,D,75):SHOOT=[-21.737]
Deciding optimal action for state: (4, 0, 1, 0, 100)
0.7200000000000001 ( 4 0 1 0 100 ) -10.0 0.0
0.18000000000000002 ( 4 0 1 1 100 ) -29.98 0.0
0.08000000000000002 ( 4 0 1 0 50 ) -5.004999999999999 0.0
0.020000000000000004 ( 4 0 1 1 50 ) -27.4825 0.0
(C,0,1,D,100):HIT=[-23.533]
(C,0,1,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 1, 1, 25)
0.25 ( 4 0 0 1 25 ) -27.4825 0.0
0.25 ( 4 0 0 1 0 ) 0.0 50.0
0.5 ( 4 0 0 0 50 ) -5.004999999999999 0.0
(C,0,1,R,25):SHOOT=[-26.856]
Deciding optimal action for state: (4, 0, 1, 1, 50)
0.425 ( 1 0 1 1 50 ) -10.0 0.0
0.075 ( 2 0 1 1 50 ) -24.985 0.0
0.5 ( 4 0 0 0 75 ) -10.0 0.0
(C,0,1,R,50):UP=[-41.093]
Deciding optimal action for state: (4, 0, 1, 1, 75)
0.425 ( 1 0 1 1 75 ) -10.0 0.0
0.075 ( 2 0 1 1 75 ) -29.98 0.0
0.5 ( 4 0 0 0 100 ) -10.0 0.0
(C,0,1,R,75):UP=[-41.467]
Deciding optimal action for state: (4, 0, 1, 1, 100)
0.425 ( 1 0 1 1 100 ) -10.0 0.0
0.075 ( 2 0 1 1 100 ) -29.98 0.0
0.5 ( 4 0 0 0 100 ) -10.0 0.0
(C,0,1,R,100):UP=[-41.467]
(C,0,2,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 2, 0, 25)
0.68 ( 2 0 2 0 25 ) 34.955000000000005 0.0
0.17 ( 2 0 2 1 25 ) -7.5024999999999995 0.0
0.12 ( 2 0 2 0 25 ) 34.955000000000005 0.0
0.03 ( 2 0 2 1 25 ) -7.5024999999999995 0.0
(C,0,2,D,25):RIGHT=[16.437]
Deciding optimal action for state: (4, 0, 2, 0, 50)
0.4 ( 4 0 1 0 50 ) -5.004999999999999 0.0
0.1 ( 4 0 1 1 50 ) -27.4825 0.0
0.4 ( 4 0 1 0 25 ) 14.975000000000001 0.0
0.1 ( 4 0 1 1 25 ) -17.4925 0.0
(C,0,2,D,50):SHOOT=[-10.509]
Deciding optimal action for state: (4, 0, 2, 0, 75)
0.4 ( 4 0 1 0 75 ) -10.0 0.0
0.1 ( 4 0 1 1 75 ) -29.98 0.0
0.4 ( 4 0 1 0 50 ) -5.004999999999999 0.0
0.1 ( 4 0 1 1 50 ) -27.4825 0.0
(C,0,2,D,75):SHOOT=[-21.737]
Deciding optimal action for state: (4, 0, 2, 0, 100)
0.7200000000000001 ( 4 0 2 0 100 ) -10.0 0.0
0.18000000000000002 ( 4 0 2 1 100 ) -29.98 0.0
0.08000000000000002 ( 4 0 2 0 50 ) -5.004999999999999 0.0
0.020000000000000004 ( 4 0 2 1 50 ) -27.4825 0.0
(C,0,2,D,100):HIT=[-23.533]
(C,0,2,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 2, 1, 25)
0.25 ( 4 0 1 1 25 ) -17.4925 0.0
0.25 ( 4 0 1 1 0 ) 0.0 50.0
0.5 ( 4 0 0 0 50 ) -5.004999999999999 0.0
(C,0,2,R,25):SHOOT=[-24.361]
Deciding optimal action for state: (4, 0, 2, 1, 50)
0.425 ( 1 0 2 1 50 ) -10.0 0.0
0.075 ( 2 0 2 1 50 ) -24.985 0.0
0.5 ( 4 0 0 0 75 ) -10.0 0.0
(C,0,2,R,50):UP=[-41.093]
Deciding optimal action for state: (4, 0, 2, 1, 75)
0.425 ( 1 0 2 1 75 ) -10.0 0.0
0.075 ( 2 0 2 1 75 ) -29.98 0.0
0.5 ( 4 0 0 0 100 ) -10.0 0.0
(C,0,2,R,75):UP=[-41.467]
Deciding optimal action for state: (4, 0, 2, 1, 100)
0.425 ( 1 0 2 1 100 ) -10.0 0.0
0.075 ( 2 0 2 1 100 ) -29.98 0.0
0.5 ( 4 0 0 0 100 ) -10.0 0.0
(C,0,2,R,100):UP=[-41.467]
(C,0,3,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 3, 0, 25)
0.68 ( 2 0 3 0 25 ) 34.955000000000005 0.0
0.17 ( 2 0 3 1 25 ) -7.5024999999999995 0.0
0.12 ( 2 0 3 0 25 ) 34.955000000000005 0.0
0.03 ( 2 0 3 1 25 ) -7.5024999999999995 0.0
(C,0,3,D,25):RIGHT=[16.437]
Deciding optimal action for state: (4, 0, 3, 0, 50)
0.4 ( 4 0 2 0 50 ) -5.004999999999999 0.0
0.1 ( 4 0 2 1 50 ) -27.4825 0.0
0.4 ( 4 0 2 0 25 ) 14.975000000000001 0.0
0.1 ( 4 0 2 1 25 ) -17.4925 0.0
(C,0,3,D,50):SHOOT=[-10.509]
Deciding optimal action for state: (4, 0, 3, 0, 75)
0.4 ( 4 0 2 0 75 ) -10.0 0.0
0.1 ( 4 0 2 1 75 ) -29.98 0.0
0.4 ( 4 0 2 0 50 ) -5.004999999999999 0.0
0.1 ( 4 0 2 1 50 ) -27.4825 0.0
(C,0,3,D,75):SHOOT=[-21.737]
Deciding optimal action for state: (4, 0, 3, 0, 100)
0.7200000000000001 ( 4 0 3 0 100 ) -10.0 0.0
0.18000000000000002 ( 4 0 3 1 100 ) -29.98 0.0
0.08000000000000002 ( 4 0 3 0 50 ) -5.004999999999999 0.0
0.020000000000000004 ( 4 0 3 1 50 ) -27.4825 0.0
(C,0,3,D,100):HIT=[-23.533]
(C,0,3,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 0, 3, 1, 25)
0.25 ( 4 0 2 1 25 ) -17.4925 0.0
0.25 ( 4 0 2 1 0 ) 0.0 50.0
0.5 ( 4 0 0 0 50 ) -5.004999999999999 0.0
(C,0,3,R,25):SHOOT=[-24.361]
Deciding optimal action for state: (4, 0, 3, 1, 50)
0.425 ( 1 0 3 1 50 ) -10.0 0.0
0.075 ( 2 0 3 1 50 ) -24.985 0.0
0.5 ( 4 0 0 0 75 ) -10.0 0.0
(C,0,3,R,50):UP=[-41.093]
Deciding optimal action for state: (4, 0, 3, 1, 75)
0.425 ( 1 0 3 1 75 ) -10.0 0.0
0.075 ( 2 0 3 1 75 ) -29.98 0.0
0.5 ( 4 0 0 0 100 ) -10.0 0.0
(C,0,3,R,75):UP=[-41.467]
Deciding optimal action for state: (4, 0, 3, 1, 100)
0.425 ( 1 0 3 1 100 ) -10.0 0.0
0.075 ( 2 0 3 1 100 ) -29.98 0.0
0.5 ( 4 0 0 0 100 ) -10.0 0.0
(C,0,3,R,100):UP=[-41.467]
(C,1,0,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 0, 0, 25)
0.7200000000000001 ( 4 1 0 0 25 ) -5.004999999999999 0.0
0.18000000000000002 ( 4 1 0 1 25 ) -27.4825 0.0
0.08000000000000002 ( 4 1 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 1 0 1 0 ) 0.0 50.0
(C,1,0,D,25):HIT=[-13.547]
Deciding optimal action for state: (4, 1, 0, 0, 50)
0.7200000000000001 ( 4 1 0 0 50 ) -5.004999999999999 0.0
0.18000000000000002 ( 4 1 0 1 50 ) -27.4825 0.0
0.08000000000000002 ( 4 1 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 1 0 1 0 ) 0.0 50.0
(C,1,0,D,50):HIT=[-13.547]
Deciding optimal action for state: (4, 1, 0, 0, 75)
(C,1,0,D,75):SHOOT=[-10.0]
Deciding optimal action for state: (4, 1, 0, 0, 100)
(C,1,0,D,100):SHOOT=[-10.0]
(C,1,0,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 0, 1, 25)
0.5 ( 4 1 0 0 50 ) -5.004999999999999 0.0
(C,1,0,R,25):SHOOT=[-32.48]
Deciding optimal action for state: (4, 1, 0, 1, 50)
0.5 ( 4 1 0 0 75 ) -10.0 0.0
(C,1,0,R,50):SHOOT=[-34.975]
Deciding optimal action for state: (4, 1, 0, 1, 75)
0.5 ( 4 1 0 0 100 ) -10.0 0.0
(C,1,0,R,75):SHOOT=[-34.975]
Deciding optimal action for state: (4, 1, 0, 1, 100)
0.5 ( 4 1 0 0 100 ) -10.0 0.0
(C,1,0,R,100):SHOOT=[-34.975]
(C,1,1,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 1, 0, 25)
0.68 ( 2 1 1 0 25 ) 34.955000000000005 0.0
0.17 ( 2 1 1 1 25 ) -7.5024999999999995 0.0
0.12 ( 2 1 1 0 25 ) 34.955000000000005 0.0
0.03 ( 2 1 1 1 25 ) -7.5024999999999995 0.0
(C,1,1,D,25):RIGHT=[16.437]
Deciding optimal action for state: (4, 1, 1, 0, 50)
0.7200000000000001 ( 4 1 1 0 50 ) -5.004999999999999 0.0
0.18000000000000002 ( 4 1 1 1 50 ) -27.4825 0.0
0.08000000000000002 ( 4 1 1 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 1 1 1 0 ) 0.0 50.0
(C,1,1,D,50):HIT=[-13.547]
Deciding optimal action for state: (4, 1, 1, 0, 75)
0.4 ( 4 1 0 0 75 ) -10.0 0.0
0.1 ( 4 1 0 1 75 ) -29.98 0.0
0.4 ( 4 1 0 0 50 ) -5.004999999999999 0.0
0.1 ( 4 1 0 1 50 ) -27.4825 0.0
(C,1,1,D,75):SHOOT=[-21.737]
Deciding optimal action for state: (4, 1, 1, 0, 100)
0.7200000000000001 ( 4 1 1 0 100 ) -10.0 0.0
0.18000000000000002 ( 4 1 1 1 100 ) -29.98 0.0
0.08000000000000002 ( 4 1 1 0 50 ) -5.004999999999999 0.0
0.020000000000000004 ( 4 1 1 1 50 ) -27.4825 0.0
(C,1,1,D,100):HIT=[-23.533]
(C,1,1,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 1, 1, 25)
0.25 ( 4 1 0 1 25 ) -27.4825 0.0
0.25 ( 4 1 0 1 0 ) 0.0 50.0
0.5 ( 4 1 0 0 50 ) -5.004999999999999 0.0
(C,1,1,R,25):SHOOT=[-26.856]
Deciding optimal action for state: (4, 1, 1, 1, 50)
0.425 ( 1 1 1 1 50 ) -10.0 0.0
0.075 ( 2 1 1 1 50 ) -24.985 0.0
0.5 ( 4 1 0 0 75 ) -10.0 0.0
(C,1,1,R,50):UP=[-41.093]
Deciding optimal action for state: (4, 1, 1, 1, 75)
0.425 ( 1 1 1 1 75 ) -10.0 0.0
0.075 ( 2 1 1 1 75 ) -29.98 0.0
0.5 ( 4 1 0 0 100 ) -10.0 0.0
(C,1,1,R,75):UP=[-41.467]
Deciding optimal action for state: (4, 1, 1, 1, 100)
0.425 ( 1 1 1 1 100 ) -10.0 0.0
0.075 ( 2 1 1 1 100 ) -29.98 0.0
0.5 ( 4 1 0 0 100 ) -10.0 0.0
(C,1,1,R,100):UP=[-41.467]
(C,1,2,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 2, 0, 25)
0.68 ( 2 1 2 0 25 ) 34.955000000000005 0.0
0.17 ( 2 1 2 1 25 ) -7.5024999999999995 0.0
0.12 ( 2 1 2 0 25 ) 34.955000000000005 0.0
0.03 ( 2 1 2 1 25 ) -7.5024999999999995 0.0
(C,1,2,D,25):RIGHT=[16.437]
Deciding optimal action for state: (4, 1, 2, 0, 50)
0.4 ( 4 1 1 0 50 ) -5.004999999999999 0.0
0.1 ( 4 1 1 1 50 ) -27.4825 0.0
0.4 ( 4 1 1 0 25 ) 14.975000000000001 0.0
0.1 ( 4 1 1 1 25 ) -17.4925 0.0
(C,1,2,D,50):SHOOT=[-10.509]
Deciding optimal action for state: (4, 1, 2, 0, 75)
0.4 ( 4 1 1 0 75 ) -10.0 0.0
0.1 ( 4 1 1 1 75 ) -29.98 0.0
0.4 ( 4 1 1 0 50 ) -5.004999999999999 0.0
0.1 ( 4 1 1 1 50 ) -27.4825 0.0
(C,1,2,D,75):SHOOT=[-21.737]
Deciding optimal action for state: (4, 1, 2, 0, 100)
0.7200000000000001 ( 4 1 2 0 100 ) -10.0 0.0
0.18000000000000002 ( 4 1 2 1 100 ) -29.98 0.0
0.08000000000000002 ( 4 1 2 0 50 ) -5.004999999999999 0.0
0.020000000000000004 ( 4 1 2 1 50 ) -27.4825 0.0
(C,1,2,D,100):HIT=[-23.533]
(C,1,2,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 2, 1, 25)
0.25 ( 4 1 1 1 25 ) -17.4925 0.0
0.25 ( 4 1 1 1 0 ) 0.0 50.0
0.5 ( 4 1 0 0 50 ) -5.004999999999999 0.0
(C,1,2,R,25):SHOOT=[-24.361]
Deciding optimal action for state: (4, 1, 2, 1, 50)
0.425 ( 1 1 2 1 50 ) -10.0 0.0
0.075 ( 2 1 2 1 50 ) -24.985 0.0
0.5 ( 4 1 0 0 75 ) -10.0 0.0
(C,1,2,R,50):UP=[-41.093]
Deciding optimal action for state: (4, 1, 2, 1, 75)
0.425 ( 1 1 2 1 75 ) -10.0 0.0
0.075 ( 2 1 2 1 75 ) -29.98 0.0
0.5 ( 4 1 0 0 100 ) -10.0 0.0
(C,1,2,R,75):UP=[-41.467]
Deciding optimal action for state: (4, 1, 2, 1, 100)
0.425 ( 1 1 2 1 100 ) -10.0 0.0
0.075 ( 2 1 2 1 100 ) -29.98 0.0
0.5 ( 4 1 0 0 100 ) -10.0 0.0
(C,1,2,R,100):UP=[-41.467]
(C,1,3,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 3, 0, 25)
0.68 ( 2 1 3 0 25 ) 34.955000000000005 0.0
0.17 ( 2 1 3 1 25 ) -7.5024999999999995 0.0
0.12 ( 2 1 3 0 25 ) 34.955000000000005 0.0
0.03 ( 2 1 3 1 25 ) -7.5024999999999995 0.0
(C,1,3,D,25):RIGHT=[16.437]
Deciding optimal action for state: (4, 1, 3, 0, 50)
0.4 ( 4 1 2 0 50 ) -5.004999999999999 0.0
0.1 ( 4 1 2 1 50 ) -27.4825 0.0
0.4 ( 4 1 2 0 25 ) 14.975000000000001 0.0
0.1 ( 4 1 2 1 25 ) -17.4925 0.0
(C,1,3,D,50):SHOOT=[-10.509]
Deciding optimal action for state: (4, 1, 3, 0, 75)
0.4 ( 4 1 2 0 75 ) -10.0 0.0
0.1 ( 4 1 2 1 75 ) -29.98 0.0
0.4 ( 4 1 2 0 50 ) -5.004999999999999 0.0
0.1 ( 4 1 2 1 50 ) -27.4825 0.0
(C,1,3,D,75):SHOOT=[-21.737]
Deciding optimal action for state: (4, 1, 3, 0, 100)
0.7200000000000001 ( 4 1 3 0 100 ) -10.0 0.0
0.18000000000000002 ( 4 1 3 1 100 ) -29.98 0.0
0.08000000000000002 ( 4 1 3 0 50 ) -5.004999999999999 0.0
0.020000000000000004 ( 4 1 3 1 50 ) -27.4825 0.0
(C,1,3,D,100):HIT=[-23.533]
(C,1,3,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 1, 3, 1, 25)
0.25 ( 4 1 2 1 25 ) -17.4925 0.0
0.25 ( 4 1 2 1 0 ) 0.0 50.0
0.5 ( 4 1 0 0 50 ) -5.004999999999999 0.0
(C,1,3,R,25):SHOOT=[-24.361]
Deciding optimal action for state: (4, 1, 3, 1, 50)
0.425 ( 1 1 3 1 50 ) -10.0 0.0
0.075 ( 2 1 3 1 50 ) -24.985 0.0
0.5 ( 4 1 0 0 75 ) -10.0 0.0
(C,1,3,R,50):UP=[-41.093]
Deciding optimal action for state: (4, 1, 3, 1, 75)
0.425 ( 1 1 3 1 75 ) -10.0 0.0
0.075 ( 2 1 3 1 75 ) -29.98 0.0
0.5 ( 4 1 0 0 100 ) -10.0 0.0
(C,1,3,R,75):UP=[-41.467]
Deciding optimal action for state: (4, 1, 3, 1, 100)
0.425 ( 1 1 3 1 100 ) -10.0 0.0
0.075 ( 2 1 3 1 100 ) -29.98 0.0
0.5 ( 4 1 0 0 100 ) -10.0 0.0
(C,1,3,R,100):UP=[-41.467]
(C,2,0,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 0, 0, 25)
0.7200000000000001 ( 4 2 0 0 25 ) -5.004999999999999 0.0
0.18000000000000002 ( 4 2 0 1 25 ) -27.4825 0.0
0.08000000000000002 ( 4 2 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 2 0 1 0 ) 0.0 50.0
(C,2,0,D,25):HIT=[-13.547]
Deciding optimal action for state: (4, 2, 0, 0, 50)
0.7200000000000001 ( 4 2 0 0 50 ) -5.004999999999999 0.0
0.18000000000000002 ( 4 2 0 1 50 ) -27.4825 0.0
0.08000000000000002 ( 4 2 0 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 2 0 1 0 ) 0.0 50.0
(C,2,0,D,50):HIT=[-13.547]
Deciding optimal action for state: (4, 2, 0, 0, 75)
(C,2,0,D,75):SHOOT=[-10.0]
Deciding optimal action for state: (4, 2, 0, 0, 100)
(C,2,0,D,100):SHOOT=[-10.0]
(C,2,0,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 0, 1, 25)
0.5 ( 4 2 0 0 50 ) -5.004999999999999 0.0
(C,2,0,R,25):SHOOT=[-32.48]
Deciding optimal action for state: (4, 2, 0, 1, 50)
0.5 ( 4 2 0 0 75 ) -10.0 0.0
(C,2,0,R,50):SHOOT=[-34.975]
Deciding optimal action for state: (4, 2, 0, 1, 75)
0.5 ( 4 2 0 0 100 ) -10.0 0.0
(C,2,0,R,75):SHOOT=[-34.975]
Deciding optimal action for state: (4, 2, 0, 1, 100)
0.5 ( 4 2 0 0 100 ) -10.0 0.0
(C,2,0,R,100):SHOOT=[-34.975]
(C,2,1,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 1, 0, 25)
0.68 ( 2 2 1 0 25 ) 34.955000000000005 0.0
0.17 ( 2 2 1 1 25 ) -7.5024999999999995 0.0
0.12 ( 2 2 1 0 25 ) 34.955000000000005 0.0
0.03 ( 2 2 1 1 25 ) -7.5024999999999995 0.0
(C,2,1,D,25):RIGHT=[16.437]
Deciding optimal action for state: (4, 2, 1, 0, 50)
0.7200000000000001 ( 4 2 1 0 50 ) -5.004999999999999 0.0
0.18000000000000002 ( 4 2 1 1 50 ) -27.4825 0.0
0.08000000000000002 ( 4 2 1 0 0 ) 0.0 50.0
0.020000000000000004 ( 4 2 1 1 0 ) 0.0 50.0
(C,2,1,D,50):HIT=[-13.547]
Deciding optimal action for state: (4, 2, 1, 0, 75)
0.4 ( 4 2 0 0 75 ) -10.0 0.0
0.1 ( 4 2 0 1 75 ) -29.98 0.0
0.4 ( 4 2 0 0 50 ) -5.004999999999999 0.0
0.1 ( 4 2 0 1 50 ) -27.4825 0.0
(C,2,1,D,75):SHOOT=[-21.737]
Deciding optimal action for state: (4, 2, 1, 0, 100)
0.7200000000000001 ( 4 2 1 0 100 ) -10.0 0.0
0.18000000000000002 ( 4 2 1 1 100 ) -29.98 0.0
0.08000000000000002 ( 4 2 1 0 50 ) -5.004999999999999 0.0
0.020000000000000004 ( 4 2 1 1 50 ) -27.4825 0.0
(C,2,1,D,100):HIT=[-23.533]
(C,2,1,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 1, 1, 25)
0.25 ( 4 2 0 1 25 ) -27.4825 0.0
0.25 ( 4 2 0 1 0 ) 0.0 50.0
0.5 ( 4 2 0 0 50 ) -5.004999999999999 0.0
(C,2,1,R,25):SHOOT=[-26.856]
Deciding optimal action for state: (4, 2, 1, 1, 50)
0.425 ( 1 2 1 1 50 ) -10.0 0.0
0.075 ( 2 2 1 1 50 ) -24.985 0.0
0.5 ( 4 2 0 0 75 ) -10.0 0.0
(C,2,1,R,50):UP=[-41.093]
Deciding optimal action for state: (4, 2, 1, 1, 75)
0.425 ( 1 2 1 1 75 ) -10.0 0.0
0.075 ( 2 2 1 1 75 ) -29.98 0.0
0.5 ( 4 2 0 0 100 ) -10.0 0.0
(C,2,1,R,75):UP=[-41.467]
Deciding optimal action for state: (4, 2, 1, 1, 100)
0.425 ( 1 2 1 1 100 ) -10.0 0.0
0.075 ( 2 2 1 1 100 ) -29.98 0.0
0.5 ( 4 2 0 0 100 ) -10.0 0.0
(C,2,1,R,100):UP=[-41.467]
(C,2,2,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 2, 0, 25)
0.68 ( 2 2 2 0 25 ) 34.955000000000005 0.0
0.17 ( 2 2 2 1 25 ) -7.5024999999999995 0.0
0.12 ( 2 2 2 0 25 ) 34.955000000000005 0.0
0.03 ( 2 2 2 1 25 ) -7.5024999999999995 0.0
(C,2,2,D,25):RIGHT=[16.437]
Deciding optimal action for state: (4, 2, 2, 0, 50)
0.4 ( 4 2 1 0 50 ) -5.004999999999999 0.0
0.1 ( 4 2 1 1 50 ) -27.4825 0.0
0.4 ( 4 2 1 0 25 ) 14.975000000000001 0.0
0.1 ( 4 2 1 1 25 ) -17.4925 0.0
(C,2,2,D,50):SHOOT=[-10.509]
Deciding optimal action for state: (4, 2, 2, 0, 75)
0.4 ( 4 2 1 0 75 ) -10.0 0.0
0.1 ( 4 2 1 1 75 ) -29.98 0.0
0.4 ( 4 2 1 0 50 ) -5.004999999999999 0.0
0.1 ( 4 2 1 1 50 ) -27.4825 0.0
(C,2,2,D,75):SHOOT=[-21.737]
Deciding optimal action for state: (4, 2, 2, 0, 100)
0.7200000000000001 ( 4 2 2 0 100 ) -10.0 0.0
0.18000000000000002 ( 4 2 2 1 100 ) -29.98 0.0
0.08000000000000002 ( 4 2 2 0 50 ) -5.004999999999999 0.0
0.020000000000000004 ( 4 2 2 1 50 ) -27.4825 0.0
(C,2,2,D,100):HIT=[-23.533]
(C,2,2,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 2, 1, 25)
0.25 ( 4 2 1 1 25 ) -17.4925 0.0
0.25 ( 4 2 1 1 0 ) 0.0 50.0
0.5 ( 4 2 0 0 50 ) -5.004999999999999 0.0
(C,2,2,R,25):SHOOT=[-24.361]
Deciding optimal action for state: (4, 2, 2, 1, 50)
0.425 ( 1 2 2 1 50 ) -10.0 0.0
0.075 ( 2 2 2 1 50 ) -24.985 0.0
0.5 ( 4 2 0 0 75 ) -10.0 0.0
(C,2,2,R,50):UP=[-41.093]
Deciding optimal action for state: (4, 2, 2, 1, 75)
0.425 ( 1 2 2 1 75 ) -10.0 0.0
0.075 ( 2 2 2 1 75 ) -29.98 0.0
0.5 ( 4 2 0 0 100 ) -10.0 0.0
(C,2,2,R,75):UP=[-41.467]
Deciding optimal action for state: (4, 2, 2, 1, 100)
0.425 ( 1 2 2 1 100 ) -10.0 0.0
0.075 ( 2 2 2 1 100 ) -29.98 0.0
0.5 ( 4 2 0 0 100 ) -10.0 0.0
(C,2,2,R,100):UP=[-41.467]
(C,2,3,D,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 3, 0, 25)
0.68 ( 2 2 3 0 25 ) 34.955000000000005 0.0
0.17 ( 2 2 3 1 25 ) -7.5024999999999995 0.0
0.12 ( 2 2 3 0 25 ) 34.955000000000005 0.0
0.03 ( 2 2 3 1 25 ) -7.5024999999999995 0.0
(C,2,3,D,25):RIGHT=[16.437]
Deciding optimal action for state: (4, 2, 3, 0, 50)
0.4 ( 4 2 2 0 50 ) -5.004999999999999 0.0
0.1 ( 4 2 2 1 50 ) -27.4825 0.0
0.4 ( 4 2 2 0 25 ) 14.975000000000001 0.0
0.1 ( 4 2 2 1 25 ) -17.4925 0.0
(C,2,3,D,50):SHOOT=[-10.509]
Deciding optimal action for state: (4, 2, 3, 0, 75)
0.4 ( 4 2 2 0 75 ) -10.0 0.0
0.1 ( 4 2 2 1 75 ) -29.98 0.0
0.4 ( 4 2 2 0 50 ) -5.004999999999999 0.0
0.1 ( 4 2 2 1 50 ) -27.4825 0.0
(C,2,3,D,75):SHOOT=[-21.737]
Deciding optimal action for state: (4, 2, 3, 0, 100)
0.7200000000000001 ( 4 2 3 0 100 ) -10.0 0.0
0.18000000000000002 ( 4 2 3 1 100 ) -29.98 0.0
0.08000000000000002 ( 4 2 3 0 50 ) -5.004999999999999 0.0
0.020000000000000004 ( 4 2 3 1 50 ) -27.4825 0.0
(C,2,3,D,100):HIT=[-23.533]
(C,2,3,R,0):NONE=[0.000]
Deciding optimal action for state: (4, 2, 3, 1, 25)
0.25 ( 4 2 2 1 25 ) -17.4925 0.0
0.25 ( 4 2 2 1 0 ) 0.0 50.0
0.5 ( 4 2 0 0 50 ) -5.004999999999999 0.0
(C,2,3,R,25):SHOOT=[-24.361]
Deciding optimal action for state: (4, 2, 3, 1, 50)
0.425 ( 1 2 3 1 50 ) -10.0 0.0
0.075 ( 2 2 3 1 50 ) -24.985 0.0
0.5 ( 4 2 0 0 75 ) -10.0 0.0
(C,2,3,R,50):UP=[-41.093]
Deciding optimal action for state: (4, 2, 3, 1, 75)
0.425 ( 1 2 3 1 75 ) -10.0 0.0
0.075 ( 2 2 3 1 75 ) -29.98 0.0
0.5 ( 4 2 0 0 100 ) -10.0 0.0
(C,2,3,R,75):UP=[-41.467]
Deciding optimal action for state: (4, 2, 3, 1, 100)
0.425 ( 1 2 3 1 100 ) -10.0 0.0
0.075 ( 2 2 3 1 100 ) -29.98 0.0
0.5 ( 4 2 0 0 100 ) -10.0 0.0
(C,2,3,R,100):UP=[-41.467]
